# üß∞ Sprawozdanie z konfiguracji Ansible i zarzƒÖdzania artefaktem (Docker)

## üì¶ 1. Instalacja zarzƒÖdcy Ansible

### üåµ Utworzenie drugiej maszyny wirtualnej
Utworzono maszynƒô `ansible-target` z minimalnym zestawem oprogramowania, co ograniczy≈Ço zbƒôdne us≈Çugi i poprawi≈Ço wydajno≈õƒá.

### üåµ Taki sam system operacyjny jak maszyna g≈Ç√≥wna
Obie maszyny dzia≈ÇajƒÖ na systemie Ubuntu 24.04 LTS, co u≈Çatwia zarzƒÖdzanie i eliminuje problemy kompatybilno≈õci.

### üåµ Instalacja `tar` i `sshd`
Zainstalowano `tar` i `openssh-server`, aby umo≈ºliwiƒá obs≈Çugƒô archiw√≥w i dostƒôp przez SSH.

### üåµ Nadanie hostname `ansible-target`
Nazwa hosta zosta≈Ça ustawiona ju≈º podczas instalacji systemu ale jeszcze siƒô upewni≈Çem.

![Opis](ss/1.jpg)


### üåµ Wykonanie migawki maszyny
Zrobiono migawkƒô w VirtualBoxie, co pozwala wr√≥ciƒá do stanu poczƒÖtkowego w razie problem√≥w.

![Opis](ss/20.png)

### üåµ Instalacja Ansible na g≈Ç√≥wnej maszynie
Na `server` zainstalowano Ansible za pomocƒÖ APT:
```bash
sudo apt update && sudo apt install -y ansible
```
![Opis](ss/21.png)



### üåµ Wymiana kluczy SSH
Na `server` wygenerowano parƒô kluczy SSH i przes≈Çano je do `ansible@ansible-target`:
```bash
ssh-keygen
ssh-copy-id ansible@ansible-target
```
![Opis](ss/6.png)

---

## üóÇÔ∏è 2. Inwentaryzacja

### üåµ Ustawienie nazw host√≥w
Ustawiono `hostnamectl` na obu maszynach:
```bash
hostnamectl set-hostname server
hostnamectl set-hostname ansible-target
```
![Opis](ss/7.png)



### üåµ Dodanie wpis√≥w do /etc/hosts
Na maszynie `server` wpisano:
```
192.168.100.10 ansible-target
192.168.100.11 server
```
![Opis](ss/22.png)



### üåµ Weryfikacja ≈ÇƒÖczno≈õci
Sprawdzono po≈ÇƒÖczenie:
```bash
ping ansible-target
ping server
```
![Opis](ss/23.png)



### üåµ Stworzenie pliku inwentaryzacji
Plik `inventory.ini`:
```ini
[Orchestrators]
server ansible_host=server ansible_user=krzysztof ansible_port=2222

[Endpoints]
ansible-target ansible_host=ansible-target ansible_user=ansible
```
![Opis](ss/6,5.png)
![Opis](ss/8.png)


### üåµ Wys≈Çanie ping przez Ansible
```bash
ansible -i inventory.ini all -m ping
```
![Opis](ss/11.png)




### üåµ U≈ºyto dw√≥ch maszyn wirtualnych
Projekt zosta≈Ç przeprowadzony z wykorzystaniem dw√≥ch maszyn: `server` i `ansible-target`.

### üåµ Ponowna wymiana kluczy ssh-copy-id
Upewniono siƒô, ≈ºe u≈ºytkownik `ansible` ma poprawnie dodany klucz publiczny.

### üåµ Weryfikacja bezhas≈Çowego logowania
SSH dzia≈Ça≈Ço bez potrzeby wpisywania has≈Ça.

![Opis](ss/6.png)
---

## ‚öôÔ∏è 3. Zdalne wywo≈Çywanie procedur

### üåµ Pingowanie z playbooka
Utworzono `ping.yml`:
```yaml
- hosts: all
  gather_facts: false
  tasks:
    - name: Ping
      ansible.builtin.ping:
```
![Opis](ss/12.png)

![Opis](ss/13.png)


### üåµ Skopiowanie pliku inwentaryzacji na zdalnƒÖ maszynƒô
```yaml
- hosts: Endpoints
  gather_facts: false
  tasks:
    - name: Copy inventory
      copy:
        src: ../inventory.ini
        dest: /home/ansible/inventory.ini
```

![Opis](ss/14.png)

![Opis](ss/15.png)

### üåµ Por√≥wnanie wynik√≥w
Po skopiowaniu inventory na `ansible-target` uruchomiono test pingowy zdalnie ‚Äî wynik by≈Ç identyczny jak z `server`.

### üåµ Aktualizacja systemu
```yaml
- hosts: Endpoints
  become: true
  tasks:
    - name: Update APT
      apt:
        update_cache: yes

    - name: Upgrade packages
      apt:
        upgrade: dist
```

![Opis](ss/16.png)
![Opis](ss/17.png)

### üåµ Restart us≈Çug sshd i rngd
```yaml
- hosts: Endpoints
  become: true
  tasks:
    - name: Restart sshd
      service:
        name: ssh
        state: restarted

    - name: Restart rngd
      service:
        name: rngd
        state: restarted
      ignore_errors: true
```

![Opis](ss/18.png)
![Opis](ss/19.png)

### üåµ Test awarii (SSH down, interfejs down)
Wy≈ÇƒÖczono `sshd` i kartƒô sieciowƒÖ. Ansible zg≈Çosi≈Ç `UNREACHABLE` ‚Äî zgodnie z oczekiwaniami.

![Opis](ss/a1.png)
![Opis](ss/a2.png)

---

## üê≥ 4. ZarzƒÖdzanie artefaktem (kontener)

### üåµ Budowa i uruchomienie kontenera
W playbooku u≈ºyto obrazu `nginx:alpine`, kt√≥ry zosta≈Ç uruchomiony na porcie 8080.

### üåµ Pobranie z Docker Hub
Ansible pobra≈Ç oficjalny obraz `nginx:alpine` bez potrzeby budowania lokalnego obrazu.

### üåµ Instalacja Dockera przez Ansible
Rola `deploy_container` automatycznie instalowa≈Ça Dockera przez APT.

### üåµ Weryfikacja dzia≈Çania aplikacji
Sprawdzono dostƒôpno≈õƒá strony pod `http://localhost:8080` za pomocƒÖ modu≈Çu `uri`.

### üåµ Usuniƒôcie kontenera
Na koniec, kontener `hello-app` zosta≈Ç usuniƒôty z maszyny docelowej przez Ansible.

---

## ‚úÖ 5. Podsumowanie

- üñ•Ô∏è ≈örodowisko sk≈Çada siƒô z dw√≥ch maszyn wirtualnych
- üîê Zrealizowano logowanie bez has≈Ça
- üìÇ Plik inwentaryzacji z grupami: `Orchestrators` i `Endpoints`
- ‚öôÔ∏è Stworzono playbooki do pingu, kopiowania, aktualizacji, restartu us≈Çug
- üê≥ Wdro≈ºono i przetestowano kontener aplikacji z Docker Hub
- üì¶ Rolƒô stworzono przy u≈ºyciu `ansible-galaxy`

_____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________
_____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________
_____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________
_____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________
_____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________



# üìù Sprawozdanie: Pliki odpowiedzi dla wdro≈ºe≈Ñ nienadzorowanych

## üéØ Zagadnienie

Zadanie dotyczy≈Ço przygotowania automatycznego ≈∫r√≥d≈Ça instalacyjnego systemu Fedora 42 ‚Äî przydatnego w ≈õrodowiskach testowych, serwerowych lub IoT. Instalacja mia≈Ça odbywaƒá siƒô w pe≈Çni automatycznie, dziƒôki zastosowaniu pliku odpowiedzi Kickstart.

---

## üéØ Cel zadania

- Utworzenie pliku odpowiedzi `kickstart` do instalacji systemu.
- Zainstalowanie systemu Fedora 42 na maszynie wirtualnej w trybie nienadzorowanym.
- Upewnienie siƒô, ≈ºe system uruchamia siƒô w pe≈Çni skonfigurowany i gotowy do hostowania aplikacji.

---

## üì¶ ≈örodowisko i narzƒôdzia

- Oracle VirtualBox
- Obraz ISO Fedora 42 Everything Netinst
- Plik `anaconda-ks.cfg` z repozytorium GitHub
- Edytor nano, przeglƒÖdarka, TinyURL

---

## ü™ú Kroki realizacji

### 1. Uruchomienie instalatora z ISO

Na poczƒÖtku uruchomi≈Çem maszynƒô wirtualnƒÖ z obrazem instalacyjnym Fedora 42 w trybie ‚ÄúTest this media & install Fedora 42‚Äù.

üì∑ 
![Opis](ss/201.png)
![Opis](ss/202.png)



---

### 2. Problem z d≈Çugim linkiem i u≈ºycie TinyURL

Podczas konfiguracji GRUB okaza≈Ço siƒô, ≈ºe nie mogƒô wkleiƒá d≈Çugiego linku z GitHub Raw z plikiem Kickstart. W zwiƒÖzku z tym postanowi≈Çem u≈ºyƒá serwisu [TinyURL](https://tinyurl.com) do skr√≥cenia odno≈õnika.

```
#version=DEVEL
text
skipx
cdrom

keyboard --vckeymap=pl --xlayouts='pl'
lang pl_PL.UTF-8
timezone Europe/Warsaw --utc

rootpw --iscrypted --allow-ssh $y$j9T$u5Te1Uv/zc1G30Bm1z7ipamDc$3EBAqr78ouqUbIZt/CgcookAh0LiFJbyumYqU4WzW5
user --groups=wheel --name=user --password=haslohaslo --plaintext --iscrypted --gecos="user"

ignoredisk --only-use=sda
clearpart --all --initlabel
autopart

firstboot --enable

%packages
@^custom-environment
%end

reboot
```


![Opis](ss/203.png)
![Opis](ss/204.png)

Link do pliku Kickstart:
```
https://tinyurl.com/bdejyufr
```

---

### 3. Uruchomienie instalacji z pliku Kickstart

Instalator wykry≈Ç plik Kickstart i rozpoczƒÖ≈Ç instalacjƒô automatycznƒÖ. W logach widaƒá by≈Ço, ≈ºe u≈ºywany jest tryb tekstowy, co potwierdza wykorzystanie `inst.ks`.


![Opis](ss/206.png)
![Opis](ss/207.png)


---

### 4. Wej≈õcie do Anaconda GUI

Choƒá instalacja by≈Ça automatyczna, pojawi≈Ço siƒô okno z podsumowaniem konfiguracji. Musia≈Çem rƒôcznie potwierdziƒá konfiguracjƒô u≈ºytkownika i konta root.

![Anaconda GUI - podsumowanie instalacji](ss/209.png)

---

### 5. Postƒôp instalacji

Nastƒôpnie instalacja postƒôpowa≈Ça dalej, tworzƒÖc partycjƒô rozruchowƒÖ i systemowƒÖ.

![Anaconda GUI - podsumowanie instalacji](ss/212.png)

---




____________________________________________________________________________________________________________________________________________________________________________________________________________________
____________________________________________________________________________________________________________________________________________________________________________________________________________________
____________________________________________________________________________________________________________________________________________________________________________________________________________________
____________________________________________________________________________________________________________________________________________________________________________________________________________________
____________________________________________________________________________________________________________________________________________________________________________________________________________________


  

## Wdra≈ºanie na zarzƒÖdzalne kontenery: Kubernetes

  

### Instalacja klastra Kubernetes

Pierwszym zadaniem by≈Ço zainstalowanie implementacji stosu `k8s` na maszynie wirtualnej. W naszym przypadku jest to `minikube`, czyli lekkie, lokalne ≈õrodowisko do uruchamiania klastra Kubernetes na jednej maszynie.

  

Instalacja `minikube` w postaci paczki `RPM` dla architektury `x86-64`odby≈Ça siƒô poleceniami:



  

Instalator pobrany zosta≈Ç z oficjalnego, certyfikowanego ≈∫r√≥d≈Ça dystrybucji, co minimalizuje ryzyko u≈ºycia z≈Ço≈õliwego oprogramowania.

  

Dodatkowo zainstalowa≈Çem narzƒôdzie `conntrack`. Jest to narzƒôdzie u≈ºytkowe i biblioteka jƒÖdra Linuksa do ≈õledzenia stanu po≈ÇƒÖcze≈Ñ sieciowych, u≈ºywane przez Kubernetes do kontrolowania routingu i przekierowywania pakiet√≥w miƒôdzy `podami` i `service'ami`.

![ss](./Lab10/screenshots_lab10/ss1.png)

  

Nastƒôpnie zaopatrzy≈Çem siƒô w polecenie `kubectl` w wariancie `minikube` za pomocƒÖ aliasu:

``` bash

alias  kubectl="minikube kubectl --"

```

  

Po zainstalowaniu wymaganych zale≈ºno≈õci, uruchomi≈Çem Kubernetes:

![ss](./Lab10/screenshots_lab10/ss4.png)

  

Operacja zako≈Ñczy≈Ça siƒô sukcesem:

![ss](./Lab10/screenshots_lab10/ss5.png)

  

Rekomendowane zasoby dla `minicube` to co najmniej 2 rdzenie procesora, 2GB wolnej pamiƒôci oraz 20GB wolnej przestrzeni na dysku. Moja maszyna wirtualna spe≈Çnia≈Ça te wymagania, lecz w celu zwiƒôkszenia wydajno≈õci (mimo wystarczajƒÖcych zasob√≥w maszyna wirtualna dosyƒá wolno dzia≈Ça≈Ça) do≈Ço≈ºy≈Çem trochƒô pamiƒôci RAM, co rozwiƒÖza≈Ço problem.

  

Nastƒôpnie uruchomi≈Çem graficzny interfejs u≈ºytkownika dla klastra Kubernetes (Dashboard). Pozwala ono ≈Çatwo przeglƒÖdaƒá i zarzƒÖdzaƒá zasobami k8s.

  

Dashboard uruchomi≈Çem poleceniem:

![ss](./Lab10/screenshots_lab10/ss6.png)

  

Nastƒôpnie po automatycznym przekierowaniu portu w VS Code wy≈õwietli≈Çem go w oknie domy≈õlnej przeglƒÖdarki:

![ss](./Lab10/screenshots_lab10/ss7.png)

  

### Analiza posiadanego kontenera

Z racji, i≈º efektem mojego `pipeline`'u by≈Ç obraz zawierajƒÖcy oprogramowanie `Redis` opublikowany na DockerHubie, mog≈Çem go u≈ºyƒá podczas tych laboratori√≥w. Upewni≈Çem siƒô tylko, ≈ºe kontener pracuje po uruchomieniu (a nie natychmiast ko≈Ñczy pracƒô).

![ss](./Lab10/screenshots_lab10/ss8.png)

  

### Uruchamianie oprogramowania

Celem zadania by≈Ço uruchomienie kontenera z aplikacjƒÖ (w moim przypadku `Redisa` z projektu `pipeline`) na stosie k8s.

![ss](./Lab10/screenshots_lab10/ss9.png)

  

w wyniku tego polecenia utworzony zosta≈Ç `pod`, czyli podstawowa jednostka uruchomieniowa (najprostszy, najmniejszy element, kt√≥ry mo≈ºna wdro≈ºyƒá i zarzƒÖdzaƒá nim w klastrze)

  

Dzia≈Çanie poda mo≈ºna by≈Ço zauwa≈ºyƒá na powy≈ºszym zrzucie ekranu po wykonaniu polecenia:

``` bash

kubectl  get  pods

```

oraz poprzez Dashboard:

![ss](./Lab10/screenshots_lab10/ss10.png)

  

Nastƒôpnie przekierowa≈Çem port, aby m√≥c po≈ÇƒÖczyƒá siƒô z kontenerem:

![ss](./Lab10/screenshots_lab10/ss11.png)

  

W drugim terminalu spr√≥bowa≈Çem nawiƒÖzaƒá po≈ÇƒÖczenie - najprotszym sposobem, czyli poleceniem `ping` za pomocƒÖ `redis-cli`:

![ss](./Lab10/screenshots_lab10/ss12.png)

  

Uzyska≈Çem odpowied≈∫ `PONG`, co oznacza, ≈ºe pr√≥ba nawiƒÖzania po≈ÇƒÖczenia zako≈Ñczy≈Ça siƒô sukcesem.

  

### Przekucie wdro≈ºenia manualnego w plik wdro≈ºenia

Celem tego zadania by≈Ço zapisanie wdro≈ºenia wybranej aplikacji w pliku wdro≈ºenia (pliku YML).

  

Pracƒô rozpoczƒÖ≈Çem od utworzenia pliku wdro≈ºenia: [redis-deployment.yaml](./Lab10/redis-deployment.yaml)

```yaml

apiVersion: apps/v1

kind: Deployment

metadata:

name: redis-app

spec:

replicas: 4

selector:

matchLabels:

app: redis-app

template:

metadata:

labels:

app: redis-app

spec:

containers:

- name: redis-container

image: tomaszek03/redis-app

ports:

- containerPort: 6379

```

  

Nastƒôpnie przy pomocy pliku utworzy≈Çem nowy deployment:

![ss](./Lab10/screenshots_lab10/ss13.png)

  

Deployment zawiera 4 repliki. Wiele replik zwiƒôksza odporno≈õƒá aplikacji ‚Äî w przypadku awarii jednej z nich, aplikacja pozostaje dostƒôpna dziƒôki pozosta≈Çym. Dodatkowymi zaletami replik sƒÖ skalowalno≈õƒá oraz r√≥wnowa≈ºenie obciƒÖ≈ºenia (load balancing). W sytuacji wzmo≈ºonego ruchu lub wiƒôkszej liczby u≈ºytkownik√≥w mo≈ºna zwiƒôkszyƒá liczbƒô replik, aby rozproszyƒá obciƒÖ≈ºenie i zapewniƒá p≈Çynne dzia≈Çanie systemu. Ruch u≈ºytkownik√≥w jest kierowany do r√≥≈ºnych pod√≥w, co zmniejsza ryzyko przeciƒÖ≈ºenia pojedynczej instancji aplikacji.

  

Sprawdzi≈Çem stan wdro≈ºenia poni≈ºszym poleceniem:

![ss](./Lab10/screenshots_lab10/ss14.png)

  

Informacja `deployment redis-app successfully rolled out` oznacza, ≈ºe deployment Redis-a zako≈Ñczy≈Ç siƒô sukcesem.

  

Aby aplikacja dzia≈Ça≈Ça z zewnƒÖtrz, nale≈ºa≈Ço wyeksponowaƒá port:

![ss](./Lab10/screenshots_lab10/ss15.png)

  

U≈ºyte polecenie tworzy zas√≥b typu `Service` i eksponuje port `6379` kontenera. Ustawi≈Çem typ NodePort, co umo≈ºliwia dostƒôp do aplikacji spoza klastra Kubernetes.

  

Nastƒôpnie, tak jak poprzednio, przekierowa≈Çem port do serwisu:

![ss](./Lab10/screenshots_lab10/ss16.png)

  

Efekt poprawno≈õci dzia≈Çania ponownie zweryfikowa≈Çem wykonujƒÖc `ping` w oddzielnym terminalu:

![ss](./Lab10/screenshots_lab10/ss17.png)

  

Utworzony deployment mo≈ºna rownie≈º monitorowaƒá za pomocƒÖ Dashboardu:

![ss](./Lab10/screenshots_lab10/ss20.png)

  

* utworzone wdro≈ºenia

![ss](./Lab10/screenshots_lab10/ss18.png)

  

* utworzone pody:

![ss](./Lab10/screenshots_lab10/ss19.png)




____________________________________________________________________________________________________________________________________________________________________________________________________________________
____________________________________________________________________________________________________________________________________________________________________________________________________________________
____________________________________________________________________________________________________________________________________________________________________________________________________________________
____________________________________________________________________________________________________________________________________________________________________________________________________________________
____________________________________________________________________________________________________________________________________________________________________________________________________________________





# Zajƒôcia 10 ‚Äì Kubernetes (1)

## Wdra≈ºanie na zarzƒÖdzalne kontenery: Kubernetes

### Instalacja klastra Kubernetes

Na potrzeby zajƒôƒá zdecydowa≈Çem siƒô skorzystaƒá z `minikube`, czyli lekkiej implementacji Kubernetes do ≈õrodowisk lokalnych. Dziƒôki niej mogƒô przeprowadziƒá pe≈ÇnƒÖ konfiguracjƒô klastra, testy oraz wdro≈ºenia bez potrzeby korzystania z chmury.

#### Pobranie Minikube

Na poczƒÖtku pobra≈Çem najnowszƒÖ wersjƒô Minikube:
```
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
```

![Pobieranie Minikube](ss/z1.png)

#### Instalacja binarki

Nastƒôpnie zainstalowa≈Çem plik wykonywalny w katalogu `/usr/local/bin`:
```
sudo install minikube-linux-amd64 /usr/local/bin/minikube
```

![Instalacja binarki](ss/z2.png)

#### Uruchomienie klastra

Uruchomi≈Çem Minikube z wykorzystaniem sterownika Docker oraz zadeklarowaniem zasob√≥w:
```
minikube start --driver=docker --cpus=2 --memory=2048
```

![Start klastra](ss/z3.png)

### Weryfikacja dzia≈Çania klastra

Sprawdzi≈Çem namespace'y oraz role:
```
minikube kubectl -- get namespaces
minikube kubectl -- get clusterrolebindings
```

![Namespaces](ss/z5.png)
![Cluster Roles](ss/z4.png)

Dla pewno≈õci zajrza≈Çem do certyfikat√≥w:
```
minikube ssh
ls /var/lib/minikube/certs/
```

![Certyfikaty](ss/z6.png)

---

## Dashboard Kubernetes

Dashboard uruchomi≈Çem za pomocƒÖ:
```
minikube dashboard
```

![Dashboard start](ss/z7.png)

Z poziomu przeglƒÖdarki uzyska≈Çem dostƒôp do interfejsu:
![Dashboard GUI](ss/z10.png)

---

## Uruchamianie aplikacji ‚Äì pojedynczy Pod

Postanowi≈Çem przetestowaƒá wdro≈ºenie kontenera z aplikacjƒÖ nginx:
```
minikube kubectl -- run moja-aplikacja --image=nginx --port=80 --labels app=moja-aplikacja
```



Sprawdzi≈Çem jego status:
```
minikube kubectl -- get pods
```
![Pod gotowy](ss/z8.png)


Nastƒôpnie przekierowa≈Çem port:
```
minikube kubectl -- port-forward pod/moja-aplikacja 8080:80
```

Z przeglƒÖdarki na moim Windowsie odwiedzi≈Çem `http://localhost:8080`:
![Port-forward dzia≈Ça](ss/z13.png)
![Port-forward dzia≈Ça](ss/z14.png)

Komunikacja dzia≈Ça poprawnie, strona Nginxa siƒô za≈Çadowa≈Ça.

---

## Tworzenie pliku YAML dla Deploymentu

Przekszta≈Çci≈Çem powy≈ºsze wdro≈ºenie w pe≈Çnoprawny `Deployment` i `Service`:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 4
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  type: NodePort
  ports:
  - port: 80
    targetPort: 80
    nodePort: 30080
```

Plik zapisa≈Çem jako `nginx-deployment.yml`.

![Plik YAML](ss/z16.png)

Wdro≈ºenie wykona≈Çem komendƒÖ:
```
minikube kubectl -- apply -f nginx-deployment.yml
```

![Wdro≈ºenie](ss/z15.png)
![Wdro≈ºenie](ss/z16.png)
---

## Sprawdzanie rollout i dzia≈Çania aplikacji

Status rollout:
```
minikube kubectl -- rollout status deployment/nginx-deployment
```

![Rollout](ss/z17.png)

Sprawdzi≈Çem dzia≈ÇajƒÖce Pody:
```
minikube kubectl -- get pods
```

Sprawdzi≈Çem dostƒôpno≈õƒá serwisu:
```
minikube service nginx-service --url
```

![Pody i serwis](ss/z18.png)

Z przeglƒÖdarki odwiedzi≈Çem podany adres IP i port:
![Strona Nginx](ss/z14.png)

_____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________
_____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________
_____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________
_____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________
_____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________





  

# Zajƒôcia 11

  

## Wdra≈ºanie na zarzƒÖdzalne kontenery: Kubernetes (2)

  

---

  

## Zadania do wykonania

  

### Przygotowanie nowego obrazu

  

Na poczƒÖtku stworzy≈Çem nowy katalog `my-nginx`, w kt√≥rym przygotowa≈Çem w≈Çasny Dockerfile. Doda≈Çem do kontenera niestandardowƒÖ stronƒô HTML, aby mieƒá pe≈ÇnƒÖ kontrolƒô nad jego zawarto≈õciƒÖ.

  

```bash

mkdir  my-nginx && cd  my-nginx

nano  Dockerfile

mkdir  my-custom-content

echo  "Hello, this is my custom Nginx page!" > my-custom-content/index.html

```

  

Zbudowa≈Çem pierwszy obraz i opublikowa≈Çem go w Docker Hub:

  

```bash

docker  build  -t  iogougou/my-nginx:v1  .

docker  login

docker  push  iogougou/my-nginx:v1

```

  

![Strona Nginx](ss/x1.png)

![Strona Nginx](ss/x2.png)

![Strona Nginx](ss/x3.png)

  

W kolejnych krokach przygotowa≈Çem wersje `v2` i `v3`, gdzie `v3` zawiera≈Ça b≈ÇƒôdnƒÖ konfiguracjƒô:

  

```bash

docker  build  -t  iogougou/my-nginx:v2  .

docker  push  iogougou/my-nginx:v2

  

echo "invalid_config" > my-custom-content/nginx.conf

docker build -t iogougou/my-nginx:v3 .

docker push iogougou/my-nginx:v3

```

  

![Strona Nginx](ss/x4.png)

![Strona Nginx](ss/x5.png)

  

---

  

### Zmiany w deploymencie

  

Przygotowa≈Çem plik `nginx-deployment.yaml` i wykona≈Çem deployment:

  

```bash

kubectl apply -f nginx-deployment.yaml

```

  

![Strona Nginx](ss/x6.png)

![Strona Nginx](ss/x7.png)

  

Sprawdzi≈Çem historiƒô rollout√≥w i wykona≈Çem rollback:

  

```bash

kubectl rollout history deployment/my-nginx-deployment

kubectl rollout undo deployment/my-nginx-deployment

```

  

![Strona Nginx](ss/x8.png)

![Strona Nginx](ss/x9.png)

![Strona Nginx](ss/x10.png)

  

---

  

## Kontynuacja dzia≈Ça≈Ñ

  

### Czƒôste aktualizacje deploymentu

  

Wielokrotnie edytowa≈Çem plik `nginx-deployment.yaml`, zmieniajƒÖc liczbƒô replik, wersjƒô obrazu itd.

  

![Strona Nginx](ss/x11.png)

  

---

  

### Weryfikacja wdro≈ºenia

  

Stworzy≈Çem skrypt `verify-deployment.sh`:

  

```bash

chmod +x verify-deployment.sh

./verify-deployment.sh

```

  

![Strona Nginx](ss/x12.png)

  

---

  

### Lista aktywnych pod√≥w

  

```bash

kubectl get pods -n default

```

  

![Strona Nginx](ss/x13.png)

  

---

  

## Strategie wdro≈ºenia

  

### Przygotowanie plik√≥w

  

```bash

nano Recreate.yaml

nano RollingUpdate.yaml

nano Canary.yaml

```

  


  

---

  

### Strategia 1: Recreate

  

![Strona Nginx](ss/x14.1.png)

  

---

  

### Strategia 2: Rolling Update

  

![Strona Nginx](ss/x14.2.png)

  

---

  

### Strategia 3: Canary Deployment

  ![Strona Nginx](ss/x14.3.png)



Zastosowa≈Çem dwa deploymenty, aby wdro≈ºyƒá wersjƒô canary.

  

---

  

### Wynik zastosowania strategii

  

```bash

kubectl apply -f Recreate.yaml

kubectl apply -f RollingUpdate.yaml

kubectl apply -f Canary.yaml

kubectl get deployments -n default

```

  

![Strona Nginx](ss/x15.png)
