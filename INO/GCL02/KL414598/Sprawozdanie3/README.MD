# ğŸ§° Sprawozdanie z konfiguracji Ansible i zarzÄ…dzania artefaktem (Docker)

## ğŸ“¦ 1. Instalacja zarzÄ…dcy Ansible

### ğŸŒµ Utworzenie drugiej maszyny wirtualnej
Utworzono maszynÄ™ `ansible-target` z minimalnym zestawem oprogramowania, co ograniczyÅ‚o zbÄ™dne usÅ‚ugi i poprawiÅ‚o wydajnoÅ›Ä‡.

### ğŸŒµ Taki sam system operacyjny jak maszyna gÅ‚Ã³wna
Obie maszyny dziaÅ‚ajÄ… na systemie Ubuntu 24.04 LTS, co uÅ‚atwia zarzÄ…dzanie i eliminuje problemy kompatybilnoÅ›ci.

### ğŸŒµ Instalacja `tar` i `sshd`
Zainstalowano `tar` i `openssh-server`, aby umoÅ¼liwiÄ‡ obsÅ‚ugÄ™ archiwÃ³w i dostÄ™p przez SSH.

### ğŸŒµ Nadanie hostname `ansible-target`
Nazwa hosta zostaÅ‚a ustawiona juÅ¼ podczas instalacji systemu ale jeszcze siÄ™ upewniÅ‚em.

![Opis](ss/1.jpg)


### ğŸŒµ Wykonanie migawki maszyny
Zrobiono migawkÄ™ w VirtualBoxie, co pozwala wrÃ³ciÄ‡ do stanu poczÄ…tkowego w razie problemÃ³w.

![Opis](ss/20.png)

### ğŸŒµ Instalacja Ansible na gÅ‚Ã³wnej maszynie
Na `server` zainstalowano Ansible za pomocÄ… APT:
```bash
sudo apt update && sudo apt install -y ansible
```
![Opis](ss/21.png)



### ğŸŒµ Wymiana kluczy SSH
Na `server` wygenerowano parÄ™ kluczy SSH i przesÅ‚ano je do `ansible@ansible-target`:
```bash
ssh-keygen
ssh-copy-id ansible@ansible-target
```
![Opis](ss/6.png)

---

## ğŸ—‚ï¸ 2. Inwentaryzacja

### ğŸŒµ Ustawienie nazw hostÃ³w
Ustawiono `hostnamectl` na obu maszynach:
```bash
hostnamectl set-hostname server
hostnamectl set-hostname ansible-target
```
![Opis](ss/7.png)



### ğŸŒµ Dodanie wpisÃ³w do /etc/hosts
Na maszynie `server` wpisano:
```
192.168.100.10 ansible-target
192.168.100.11 server
```
![Opis](ss/22.png)



### ğŸŒµ Weryfikacja Å‚Ä…cznoÅ›ci
Sprawdzono poÅ‚Ä…czenie:
```bash
ping ansible-target
ping server
```
![Opis](ss/23.png)



### ğŸŒµ Stworzenie pliku inwentaryzacji
Plik `inventory.ini`:
```ini
[Orchestrators]
server ansible_host=server ansible_user=krzysztof ansible_port=2222

[Endpoints]
ansible-target ansible_host=ansible-target ansible_user=ansible
```
![Opis](ss/6,5.png)
![Opis](ss/8.png)


### ğŸŒµ WysÅ‚anie ping przez Ansible
```bash
ansible -i inventory.ini all -m ping
```
![Opis](ss/11.png)




### ğŸŒµ UÅ¼yto dwÃ³ch maszyn wirtualnych
Projekt zostaÅ‚ przeprowadzony z wykorzystaniem dwÃ³ch maszyn: `server` i `ansible-target`.

### ğŸŒµ Ponowna wymiana kluczy ssh-copy-id
Upewniono siÄ™, Å¼e uÅ¼ytkownik `ansible` ma poprawnie dodany klucz publiczny.

### ğŸŒµ Weryfikacja bezhasÅ‚owego logowania
SSH dziaÅ‚aÅ‚o bez potrzeby wpisywania hasÅ‚a.

![Opis](ss/6.png)
---

## âš™ï¸ 3. Zdalne wywoÅ‚ywanie procedur

### ğŸŒµ Pingowanie z playbooka
Utworzono `ping.yml`:
```yaml
- hosts: all
  gather_facts: false
  tasks:
    - name: Ping
      ansible.builtin.ping:
```
![Opis](ss/12.png)

![Opis](ss/13.png)


### ğŸŒµ Skopiowanie pliku inwentaryzacji na zdalnÄ… maszynÄ™
```yaml
- hosts: Endpoints
  gather_facts: false
  tasks:
    - name: Copy inventory
      copy:
        src: ../inventory.ini
        dest: /home/ansible/inventory.ini
```

![Opis](ss/14.png)

![Opis](ss/15.png)

### ğŸŒµ PorÃ³wnanie wynikÃ³w
Po skopiowaniu inventory na `ansible-target` uruchomiono test pingowy zdalnie â€” wynik byÅ‚ identyczny jak z `server`.

### ğŸŒµ Aktualizacja systemu
```yaml
- hosts: Endpoints
  become: true
  tasks:
    - name: Update APT
      apt:
        update_cache: yes

    - name: Upgrade packages
      apt:
        upgrade: dist
```

![Opis](ss/16.png)
![Opis](ss/17.png)

### ğŸŒµ Restart usÅ‚ug sshd i rngd
```yaml
- hosts: Endpoints
  become: true
  tasks:
    - name: Restart sshd
      service:
        name: ssh
        state: restarted

    - name: Restart rngd
      service:
        name: rngd
        state: restarted
      ignore_errors: true
```

![Opis](ss/18.png)
![Opis](ss/19.png)

### ğŸŒµ Test awarii (SSH down, interfejs down)
WyÅ‚Ä…czono `sshd` i kartÄ™ sieciowÄ…. Ansible zgÅ‚osiÅ‚ `UNREACHABLE` â€” zgodnie z oczekiwaniami.

---

## ğŸ³ 4. ZarzÄ…dzanie artefaktem (kontener)

### ğŸŒµ Budowa i uruchomienie kontenera
W playbooku uÅ¼yto obrazu `nginx:alpine`, ktÃ³ry zostaÅ‚ uruchomiony na porcie 8080.

### ğŸŒµ Pobranie z Docker Hub
Ansible pobraÅ‚ oficjalny obraz `nginx:alpine` bez potrzeby budowania lokalnego obrazu.

### ğŸŒµ Instalacja Dockera przez Ansible
Rola `deploy_container` automatycznie instalowaÅ‚a Dockera przez APT.

### ğŸŒµ Weryfikacja dziaÅ‚ania aplikacji
Sprawdzono dostÄ™pnoÅ›Ä‡ strony pod `http://localhost:8080` za pomocÄ… moduÅ‚u `uri`.

### ğŸŒµ UsuniÄ™cie kontenera
Na koniec, kontener `hello-app` zostaÅ‚ usuniÄ™ty z maszyny docelowej przez Ansible.

---

## âœ… 5. Podsumowanie

- ğŸ–¥ï¸ Åšrodowisko skÅ‚ada siÄ™ z dwÃ³ch maszyn wirtualnych
- ğŸ” Zrealizowano logowanie bez hasÅ‚a
- ğŸ“‚ Plik inwentaryzacji z grupami: `Orchestrators` i `Endpoints`
- âš™ï¸ Stworzono playbooki do pingu, kopiowania, aktualizacji, restartu usÅ‚ug
- ğŸ³ WdroÅ¼ono i przetestowano kontener aplikacji z Docker Hub
- ğŸ“¦ RolÄ™ stworzono przy uÅ¼yciu `ansible-galaxy`

_____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________
_____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________
_____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________
_____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________
_____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________



# ğŸ“ Sprawozdanie: Pliki odpowiedzi dla wdroÅ¼eÅ„ nienadzorowanych

## ğŸ¯ Zagadnienie

Zadanie dotyczyÅ‚o przygotowania automatycznego ÅºrÃ³dÅ‚a instalacyjnego systemu Fedora 42 â€” przydatnego w Å›rodowiskach testowych, serwerowych lub IoT. Instalacja miaÅ‚a odbywaÄ‡ siÄ™ w peÅ‚ni automatycznie, dziÄ™ki zastosowaniu pliku odpowiedzi Kickstart.

---

## ğŸ¯ Cel zadania

- Utworzenie pliku odpowiedzi `kickstart` do instalacji systemu.
- Zainstalowanie systemu Fedora 42 na maszynie wirtualnej w trybie nienadzorowanym.
- Upewnienie siÄ™, Å¼e system uruchamia siÄ™ w peÅ‚ni skonfigurowany i gotowy do hostowania aplikacji.

---

## ğŸ“¦ Åšrodowisko i narzÄ™dzia

- Oracle VirtualBox
- Obraz ISO Fedora 42 Everything Netinst
- Plik `anaconda-ks.cfg` z repozytorium GitHub
- Edytor nano, przeglÄ…darka, TinyURL

---

## ğŸªœ Kroki realizacji

### 1. Uruchomienie instalatora z ISO

Na poczÄ…tku uruchomiÅ‚em maszynÄ™ wirtualnÄ… z obrazem instalacyjnym Fedora 42 w trybie â€œTest this media & install Fedora 42â€.

ğŸ“· 
![Opis](ss/201.png)
![Opis](ss/202.png)



---

### 2. Problem z dÅ‚ugim linkiem i uÅ¼ycie TinyURL

Podczas konfiguracji GRUB okazaÅ‚o siÄ™, Å¼e nie mogÄ™ wkleiÄ‡ dÅ‚ugiego linku z GitHub Raw z plikiem Kickstart. W zwiÄ…zku z tym postanowiÅ‚em uÅ¼yÄ‡ serwisu [TinyURL](https://tinyurl.com) do skrÃ³cenia odnoÅ›nika.

```
#version=DEVEL
text
skipx
cdrom

keyboard --vckeymap=pl --xlayouts='pl'
lang pl_PL.UTF-8
timezone Europe/Warsaw --utc

rootpw --iscrypted --allow-ssh $y$j9T$u5Te1Uv/zc1G30Bm1z7ipamDc$3EBAqr78ouqUbIZt/CgcookAh0LiFJbyumYqU4WzW5
user --groups=wheel --name=user --password=haslohaslo --plaintext --iscrypted --gecos="user"

ignoredisk --only-use=sda
clearpart --all --initlabel
autopart

firstboot --enable

%packages
@^custom-environment
%end

reboot
```


![Opis](ss/203.png)
![Opis](ss/204.png)

Link do pliku Kickstart:
```
https://tinyurl.com/bdejyufr
```

---

### 3. Uruchomienie instalacji z pliku Kickstart

Instalator wykryÅ‚ plik Kickstart i rozpoczÄ…Å‚ instalacjÄ™ automatycznÄ…. W logach widaÄ‡ byÅ‚o, Å¼e uÅ¼ywany jest tryb tekstowy, co potwierdza wykorzystanie `inst.ks`.


![Opis](ss/206.png)
![Opis](ss/207.png)


---

### 4. WejÅ›cie do Anaconda GUI

ChoÄ‡ instalacja byÅ‚a automatyczna, pojawiÅ‚o siÄ™ okno z podsumowaniem konfiguracji. MusiaÅ‚em rÄ™cznie potwierdziÄ‡ konfiguracjÄ™ uÅ¼ytkownika i konta root.

![Anaconda GUI - podsumowanie instalacji](ss/209.png)

---

### 5. PostÄ™p instalacji

NastÄ™pnie instalacja postÄ™powaÅ‚a dalej, tworzÄ…c partycjÄ™ rozruchowÄ… i systemowÄ….

![Anaconda GUI - podsumowanie instalacji](ss/212.png)

---




____________________________________________________________________________________________________________________________________________________________________________________________________________________
____________________________________________________________________________________________________________________________________________________________________________________________________________________
____________________________________________________________________________________________________________________________________________________________________________________________________________________
____________________________________________________________________________________________________________________________________________________________________________________________________________________
____________________________________________________________________________________________________________________________________________________________________________________________________________________


  

## WdraÅ¼anie na zarzÄ…dzalne kontenery: Kubernetes

  

### Instalacja klastra Kubernetes

Pierwszym zadaniem byÅ‚o zainstalowanie implementacji stosu `k8s` na maszynie wirtualnej. W naszym przypadku jest to `minikube`, czyli lekkie, lokalne Å›rodowisko do uruchamiania klastra Kubernetes na jednej maszynie.

  

Instalacja `minikube` w postaci paczki `RPM` dla architektury `x86-64`odbyÅ‚a siÄ™ poleceniami:



  

Instalator pobrany zostaÅ‚ z oficjalnego, certyfikowanego ÅºrÃ³dÅ‚a dystrybucji, co minimalizuje ryzyko uÅ¼ycia zÅ‚oÅ›liwego oprogramowania.

  

Dodatkowo zainstalowaÅ‚em narzÄ™dzie `conntrack`. Jest to narzÄ™dzie uÅ¼ytkowe i biblioteka jÄ…dra Linuksa do Å›ledzenia stanu poÅ‚Ä…czeÅ„ sieciowych, uÅ¼ywane przez Kubernetes do kontrolowania routingu i przekierowywania pakietÃ³w miÄ™dzy `podami` i `service'ami`.

![ss](./Lab10/screenshots_lab10/ss1.png)

  

NastÄ™pnie zaopatrzyÅ‚em siÄ™ w polecenie `kubectl` w wariancie `minikube` za pomocÄ… aliasu:

``` bash

alias  kubectl="minikube kubectl --"

```

  

Po zainstalowaniu wymaganych zaleÅ¼noÅ›ci, uruchomiÅ‚em Kubernetes:

![ss](./Lab10/screenshots_lab10/ss4.png)

  

Operacja zakoÅ„czyÅ‚a siÄ™ sukcesem:

![ss](./Lab10/screenshots_lab10/ss5.png)

  

Rekomendowane zasoby dla `minicube` to co najmniej 2 rdzenie procesora, 2GB wolnej pamiÄ™ci oraz 20GB wolnej przestrzeni na dysku. Moja maszyna wirtualna speÅ‚niaÅ‚a te wymagania, lecz w celu zwiÄ™kszenia wydajnoÅ›ci (mimo wystarczajÄ…cych zasobÃ³w maszyna wirtualna dosyÄ‡ wolno dziaÅ‚aÅ‚a) doÅ‚oÅ¼yÅ‚em trochÄ™ pamiÄ™ci RAM, co rozwiÄ…zaÅ‚o problem.

  

NastÄ™pnie uruchomiÅ‚em graficzny interfejs uÅ¼ytkownika dla klastra Kubernetes (Dashboard). Pozwala ono Å‚atwo przeglÄ…daÄ‡ i zarzÄ…dzaÄ‡ zasobami k8s.

  

Dashboard uruchomiÅ‚em poleceniem:

![ss](./Lab10/screenshots_lab10/ss6.png)

  

NastÄ™pnie po automatycznym przekierowaniu portu w VS Code wyÅ›wietliÅ‚em go w oknie domyÅ›lnej przeglÄ…darki:

![ss](./Lab10/screenshots_lab10/ss7.png)

  

### Analiza posiadanego kontenera

Z racji, iÅ¼ efektem mojego `pipeline`'u byÅ‚ obraz zawierajÄ…cy oprogramowanie `Redis` opublikowany na DockerHubie, mogÅ‚em go uÅ¼yÄ‡ podczas tych laboratoriÃ³w. UpewniÅ‚em siÄ™ tylko, Å¼e kontener pracuje po uruchomieniu (a nie natychmiast koÅ„czy pracÄ™).

![ss](./Lab10/screenshots_lab10/ss8.png)

  

### Uruchamianie oprogramowania

Celem zadania byÅ‚o uruchomienie kontenera z aplikacjÄ… (w moim przypadku `Redisa` z projektu `pipeline`) na stosie k8s.

![ss](./Lab10/screenshots_lab10/ss9.png)

  

w wyniku tego polecenia utworzony zostaÅ‚ `pod`, czyli podstawowa jednostka uruchomieniowa (najprostszy, najmniejszy element, ktÃ³ry moÅ¼na wdroÅ¼yÄ‡ i zarzÄ…dzaÄ‡ nim w klastrze)

  

DziaÅ‚anie poda moÅ¼na byÅ‚o zauwaÅ¼yÄ‡ na powyÅ¼szym zrzucie ekranu po wykonaniu polecenia:

``` bash

kubectl  get  pods

```

oraz poprzez Dashboard:

![ss](./Lab10/screenshots_lab10/ss10.png)

  

NastÄ™pnie przekierowaÅ‚em port, aby mÃ³c poÅ‚Ä…czyÄ‡ siÄ™ z kontenerem:

![ss](./Lab10/screenshots_lab10/ss11.png)

  

W drugim terminalu sprÃ³bowaÅ‚em nawiÄ…zaÄ‡ poÅ‚Ä…czenie - najprotszym sposobem, czyli poleceniem `ping` za pomocÄ… `redis-cli`:

![ss](./Lab10/screenshots_lab10/ss12.png)

  

UzyskaÅ‚em odpowiedÅº `PONG`, co oznacza, Å¼e prÃ³ba nawiÄ…zania poÅ‚Ä…czenia zakoÅ„czyÅ‚a siÄ™ sukcesem.

  

### Przekucie wdroÅ¼enia manualnego w plik wdroÅ¼enia

Celem tego zadania byÅ‚o zapisanie wdroÅ¼enia wybranej aplikacji w pliku wdroÅ¼enia (pliku YML).

  

PracÄ™ rozpoczÄ…Å‚em od utworzenia pliku wdroÅ¼enia: [redis-deployment.yaml](./Lab10/redis-deployment.yaml)

```yaml

apiVersion: apps/v1

kind: Deployment

metadata:

name: redis-app

spec:

replicas: 4

selector:

matchLabels:

app: redis-app

template:

metadata:

labels:

app: redis-app

spec:

containers:

- name: redis-container

image: tomaszek03/redis-app

ports:

- containerPort: 6379

```

  

NastÄ™pnie przy pomocy pliku utworzyÅ‚em nowy deployment:

![ss](./Lab10/screenshots_lab10/ss13.png)

  

Deployment zawiera 4 repliki. Wiele replik zwiÄ™ksza odpornoÅ›Ä‡ aplikacji â€” w przypadku awarii jednej z nich, aplikacja pozostaje dostÄ™pna dziÄ™ki pozostaÅ‚ym. Dodatkowymi zaletami replik sÄ… skalowalnoÅ›Ä‡ oraz rÃ³wnowaÅ¼enie obciÄ…Å¼enia (load balancing). W sytuacji wzmoÅ¼onego ruchu lub wiÄ™kszej liczby uÅ¼ytkownikÃ³w moÅ¼na zwiÄ™kszyÄ‡ liczbÄ™ replik, aby rozproszyÄ‡ obciÄ…Å¼enie i zapewniÄ‡ pÅ‚ynne dziaÅ‚anie systemu. Ruch uÅ¼ytkownikÃ³w jest kierowany do rÃ³Å¼nych podÃ³w, co zmniejsza ryzyko przeciÄ…Å¼enia pojedynczej instancji aplikacji.

  

SprawdziÅ‚em stan wdroÅ¼enia poniÅ¼szym poleceniem:

![ss](./Lab10/screenshots_lab10/ss14.png)

  

Informacja `deployment redis-app successfully rolled out` oznacza, Å¼e deployment Redis-a zakoÅ„czyÅ‚ siÄ™ sukcesem.

  

Aby aplikacja dziaÅ‚aÅ‚a z zewnÄ…trz, naleÅ¼aÅ‚o wyeksponowaÄ‡ port:

![ss](./Lab10/screenshots_lab10/ss15.png)

  

UÅ¼yte polecenie tworzy zasÃ³b typu `Service` i eksponuje port `6379` kontenera. UstawiÅ‚em typ NodePort, co umoÅ¼liwia dostÄ™p do aplikacji spoza klastra Kubernetes.

  

NastÄ™pnie, tak jak poprzednio, przekierowaÅ‚em port do serwisu:

![ss](./Lab10/screenshots_lab10/ss16.png)

  

Efekt poprawnoÅ›ci dziaÅ‚ania ponownie zweryfikowaÅ‚em wykonujÄ…c `ping` w oddzielnym terminalu:

![ss](./Lab10/screenshots_lab10/ss17.png)

  

Utworzony deployment moÅ¼na rownieÅ¼ monitorowaÄ‡ za pomocÄ… Dashboardu:

![ss](./Lab10/screenshots_lab10/ss20.png)

  

* utworzone wdroÅ¼enia

![ss](./Lab10/screenshots_lab10/ss18.png)

  

* utworzone pody:

![ss](./Lab10/screenshots_lab10/ss19.png)




____________________________________________________________________________________________________________________________________________________________________________________________________________________
____________________________________________________________________________________________________________________________________________________________________________________________________________________
____________________________________________________________________________________________________________________________________________________________________________________________________________________
____________________________________________________________________________________________________________________________________________________________________________________________________________________
____________________________________________________________________________________________________________________________________________________________________________________________________________________





# ZajÄ™cia 10 â€“ Kubernetes (1)

## WdraÅ¼anie na zarzÄ…dzalne kontenery: Kubernetes

### Instalacja klastra Kubernetes

Na potrzeby zajÄ™Ä‡ zdecydowaÅ‚em siÄ™ skorzystaÄ‡ z `minikube`, czyli lekkiej implementacji Kubernetes do Å›rodowisk lokalnych. DziÄ™ki niej mogÄ™ przeprowadziÄ‡ peÅ‚nÄ… konfiguracjÄ™ klastra, testy oraz wdroÅ¼enia bez potrzeby korzystania z chmury.

#### Pobranie Minikube

Na poczÄ…tku pobraÅ‚em najnowszÄ… wersjÄ™ Minikube:
```
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
```

![Pobieranie Minikube](ss/z1.png)

#### Instalacja binarki

NastÄ™pnie zainstalowaÅ‚em plik wykonywalny w katalogu `/usr/local/bin`:
```
sudo install minikube-linux-amd64 /usr/local/bin/minikube
```

![Instalacja binarki](ss/z2.png)

#### Uruchomienie klastra

UruchomiÅ‚em Minikube z wykorzystaniem sterownika Docker oraz zadeklarowaniem zasobÃ³w:
```
minikube start --driver=docker --cpus=2 --memory=2048
```

![Start klastra](ss/z3.png)

### Weryfikacja dziaÅ‚ania klastra

SprawdziÅ‚em namespace'y oraz role:
```
minikube kubectl -- get namespaces
minikube kubectl -- get clusterrolebindings
```

![Namespaces](ss/z5.png)
![Cluster Roles](ss/z4.png)

Dla pewnoÅ›ci zajrzaÅ‚em do certyfikatÃ³w:
```
minikube ssh
ls /var/lib/minikube/certs/
```

![Certyfikaty](ss/z6.png)

---

## Dashboard Kubernetes

Dashboard uruchomiÅ‚em za pomocÄ…:
```
minikube dashboard
```

![Dashboard start](ss/z7.png)

Z poziomu przeglÄ…darki uzyskaÅ‚em dostÄ™p do interfejsu:
![Dashboard GUI](ss/z9.png)

---

## Uruchamianie aplikacji â€“ pojedynczy Pod

PostanowiÅ‚em przetestowaÄ‡ wdroÅ¼enie kontenera z aplikacjÄ… nginx:
```
minikube kubectl -- run moja-aplikacja --image=nginx --port=80 --labels app=moja-aplikacja
```

![Pod gotowy](ss/z10.png)

SprawdziÅ‚em jego status:
```
minikube kubectl -- get pods
```

NastÄ™pnie przekierowaÅ‚em port:
```
minikube kubectl -- port-forward pod/moja-aplikacja 8080:80
```

Z przeglÄ…darki na moim Windowsie odwiedziÅ‚em `http://localhost:8080`:
![Port-forward dziaÅ‚a](ss/z17.png)

Komunikacja dziaÅ‚a poprawnie, strona Nginxa siÄ™ zaÅ‚adowaÅ‚a.

---

## Tworzenie pliku YAML dla Deploymentu

PrzeksztaÅ‚ciÅ‚em powyÅ¼sze wdroÅ¼enie w peÅ‚noprawny `Deployment` i `Service`:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 4
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  type: NodePort
  ports:
  - port: 80
    targetPort: 80
    nodePort: 30080
```

Plik zapisaÅ‚em jako `nginx-deployment.yml`.

![Plik YAML](ss/z16.png)

WdroÅ¼enie wykonaÅ‚em komendÄ…:
```
minikube kubectl -- apply -f nginx-deployment.yml
```

![WdroÅ¼enie](ss/z15.png)

---

## Sprawdzanie rollout i dziaÅ‚ania aplikacji

Status rollout:
```
minikube kubectl -- rollout status deployment/nginx-deployment
```

![Rollout](ss/z14.png)

SprawdziÅ‚em dziaÅ‚ajÄ…ce Pody:
```
minikube kubectl -- get pods
```

SprawdziÅ‚em dostÄ™pnoÅ›Ä‡ serwisu:
```
minikube service nginx-service --url
```

![Pody i serwis](ss/z12.png)

Z przeglÄ…darki odwiedziÅ‚em podany adres IP i port:
![Strona Nginx](ss/z18.png)

---

## Wnioski

Podczas realizacji zadania przeszedÅ‚em przez caÅ‚y proces: od instalacji klastra Kubernetes w Å›rodowisku lokalnym, przez jego konfiguracjÄ™, aÅ¼ po przygotowanie i uruchomienie aplikacji w formie Poda oraz Deploymentu. SprawdziÅ‚em dziaÅ‚anie Dashboardu, przekierowanie portÃ³w oraz komunikacjÄ™ z aplikacjÄ…. WdroÅ¼enie Nginxa z czterema replikami oraz jego wystawienie przez `NodePort` pozwoliÅ‚o mi zrealizowaÄ‡ peÅ‚ny cykl wdroÅ¼eniowy zgodny z praktykami DevOps.
