# Sprawozdanie 3
## Meg Paskowski
## Grupa: 2
## Zajecia 8-11
### Automatyzacja i zdalne wykonywanie polece≈Ñ za pomocƒÖ Ansible (lab 8)

Bƒôdziemy potrzebowaƒá drugiej maszyny wirtualnej. Dla oszczƒôdno≈õci zasob√≥w, musi byƒá jak najmniejsza i jak najl≈ºejsza.

- `fedora-main` ‚Äì g≈Ç√≥wna maszyna (zarzƒÖdca / orchestrator)
- `ansible-target` ‚Äì maszyna docelowa (endpoint)

Utworzenie drugiej maszyny wirtualnej z systemem `Fedora`. Przy najmniejszym zbiorze zainstalowanego oprogramowania.

Zmaiana nazwy hosta.

![host-name](IMG/Host-name.png)

Ustawienia u≈ºytkownika.

![user](IMG/User.png)

Sprawdzenie, czy na maszynie jest `tar` i `sshd`.

![tar_ssh](IMG/tar_ssh.png)

Utworzenie mikawki maszyny -> pe≈Çne spakowanie maszyny wirtualnej (dysk, konfiguracja) do pliku .ova (standard Open Virtualization Format). Dziƒôki temu jeste≈õmy w stanie:
- Skopiowaƒá maszynƒô na inny komputer,
- Udostƒôpniƒá maszynƒô komu≈õ innemu,
- Zachowaƒá backup na przysz≈Ço≈õƒá.

Wybra≈Çam na pasku `Maszyna` ‚Üí `Narzƒôdzia` ‚Üí `Migawki` i nastƒôpnie `Zr√≥b`

![migawka1](IMG/migawka1.png)

![migawka2](IMG/migawka2.png)

Do wykonania eksportu maszyny `Plik` ‚Üí `Eksportuj urzƒÖdzenie wirtualne...`

![eksport](IMG/eksport.png)

Na maszynie g≈Ç√≥wnej podbra≈Çam `Ansible`.

```bash
sudo dnf install ansible -y

#Sprawdzenie
ansible --version

```

![ansible](IMG/ansible.png)

Nastƒôpnie, aby zmieniƒá klucze ssh tak, by logowanie `ssh ansible@ansible-target` nie wymaga≈Ço podania has≈Ça wykonalam nastƒôpujƒÖce kroki:

Skopiowa≈Çam kod publiczny z maszyny g≈Ç√≥wnej na `ansible-target`.

```bash
ssh-copy-id ansible@192.168.0.86
```

Edytowalam plik `sudo nano /etc/hosts` dodajƒÖc `192.168.0.86 ansible-target`, `192.168.0.7 fedora-main` i sprawdzi≈Çam po≈Çaczenie przez ssh.

```bash
ssh ansible@ansible-target
```

![ansible-ssh](IMG/ssh_ansible.png)

Na g≈Çownej maszynie bylo nale≈ºalo zmienic ustawienie `hostname` oraz dodac do pliku `/etc/hosts` ‚Üí `192.168.0.7 fedora-main` i `192.168.0.86 ansible-target`.

```bash
sudo hostnamectl set-hostname fedora-main

#Edycja pliku hosts
sudo nano /etc/hosts

```
Nale≈∫y rowniez skopiowaƒá klucz `ssh-copy-id mpaskowski@fedora-main` z maszyny g≈Çownej.


Na sam koniec zwerifikowa≈Çam `ping`.

Z maszyny `fedora-main`:

![ping-fedora-main](IMG/ping-fedora-main.png)

Z maszyny `ansible-target`:

![ping-ansible](IMG/ping-ansible.png)


Nastƒôpnie utworzylam plik inwentaryzacji.
Plik inventory w Ansible to prosty plik tekstowy, kt√≥ry zawiera informacje o hostach, do kt√≥rych Ansible ma siƒô po≈ÇƒÖczyƒá i zarzƒÖdzaƒá nimi. W pliku inventory dodajee maszyny, kt√≥re bƒôdƒÖ klasyfikowane w odpowiednich grupach, takich jak `Orchestrators` (maszyna g≈Ç√≥wna) i `Endpoints` (maszyny docelowe) u mnie pod nazwƒÖ `TargetMachines`.
Zaczelam od utworzenia katalogu `ansible_1` w ktorym bedzie `inventory.ini` (`nano inventory`).

Zawarto≈õƒá pliku `inventory.ini`:
```
[TargetMachines]
ansible-target ansible_ssh_user=ansible 

[Orchestrators]
fedora-main ansible_ssh_user=mpaskowski 
```

W pliku `/etc/ansible/ansible.cfg ` ustawi≈Çam ≈õcie≈ºkƒô dla pliku `inventory.ini`:

```bash
[defaults]
inventory = /home/mpaskowski/MDO2025_INO/INO/GCL02/MP417574/Sprawozdanie3/ansible_1/inventory.ini
```

Plik mo≈ºemy przetestowaƒá za pomocƒÖ komendy ping.

```bash
ansible -i inventory.ini -m ping TargetMachines

ansible -i inventory.ini -m ping Orchestrators
```

![ping](IMG/results_ping.png)


#### Zdalne wywo≈Çywanie procedur
Utworzy≈Çam katalog `` w kt√≥rm znajdowaƒá siƒô bƒôdƒÖ wszytkie playbook'i.

1. Utworzy≈Çam plik `ping_all_machines.yml` ‚Üí majƒÖct na celu wys≈Çanie ≈ºƒÖdania ping'u do wszystkich maszyn.
Zawarto≈õƒá pliku:

```yml
---
- name: Pingowanie wszystkich maszyn
  hosts: all        # Odwo≈Çanie do wszystkich maszyn z inventory
  tasks:
    - name: Wykonaj ping do wszystkich maszyn
      ansible.builtin.ping:

```

Aby uruchomiƒá:

```bash
nsible-playbook ping_all_machines.yml -i /home/mpaskowski/MDO2025_INO/INO/GCL02/MP417574/Sprawozdanie3/ansible_1/inventory.ini
```

![ping_all](IMG/ping_all.png)

2. Plik `copy_inventory.yml` ‚Üí kt√≥ry bƒôdzie kopiowaƒá plik inwentaryzacji na maszyne `Endpoints` (u mnie `TargetMachines`).

```yml
---
- name: Skopiowanie pliku inwentaryzacji na maszyny Endpoints
  hosts: TargetMachines   # Grupa maszyn, do ktrych kopiujemy pliki
  tasks:
    - name: Skopiowanie pliku inventory.ini na maszynƒô
      copy:
        src: /home/mpaskowski/MDO2025_INO/INO/GCL02/MP417574/Sprawozdanie3/ansible_1/inventory.ini  # ≈öcie≈ºka do pliku lokalnego
        dest: /home/ansible/ # ≈öcie≈ºka docelowa na maszynach z grupy Endpoints
        mode: '0644'  #Uprawnienia
```

Aby uruchomiƒá:

```bash
ansible-playbook -i /home/mpaskowski/MDO2025_INO/INO/GCL02/MP417574/Sprawozdanie3/ansible_1/inventory.ini copy_inventory.yml
```

![copy_1](IMG/copy_1.png)

Po ponownym uruchomieniu:

![copy_2](IMG/copy_2.png)

3. Plik `upadte.yml` ‚Üí dokonuje aktualizacji pakiet√≥w w systemie.
Zawarto≈õƒá pliku `upadte.yml`:

```yml
---
- name: Zaktualizuj pakiety w systemie
  hosts: all
  become: yes  # Wymaga podniesienia uprawnie≈Ñ do roota
  tasks:
    - name: Zaktualizuj wszystkie pakiety
      ansible.builtin.dnf:
        name: '*'
        state: latest

```

Aby uruchomiƒá:

```bash
ansible-playbook -i /home/mpaskowski/MDO2025_INO/INO/GCL02/MP417574/Sprawozdanie3/ansible_1/inventory.ini update.yml --ask-become-pass
```

![Update](IMG/update.png)

4. Plik `restart_services.yml` ‚Üí restartuje us≈Çugi `ssh` i `rngd`.

Zawarto≈õƒá pliku ` restart_services.yml`:

```yml
---
- name: Zrestartuj us≈Çugi sshd i rngd
  hosts: all
  become: yes 
  tasks:
    - name: Zrestartuj us≈Çugƒô sshd
      ansible.builtin.systemd:
        name: sshd
        state: restarted

    - name: Zrestartuj us≈Çugƒô rngd
      ansible.builtin.systemd:
        name: rngd
        state: restarted

```

Aby uruchomiƒá:

```bash
ansible-playbook -i /home/mpaskowski/MDO2025_INO/INO/GCL02/MP417574/Sprawozdanie3/ansible_1/inventory.ini restart_services.yml --ask-become-pass
```

![restart_services](IMG/restart_services.png)

Wszytkie te polecenia mogƒÖ zostaƒá uwgzlƒôdnione w jednym pliku ‚Üí `all_tasks.yml`.

```bash
ansible-playbook -i /home/mpaskowski/MDO2025_INO/INO/GCL02/MP417574/Sprawozdanie3/ansible_1/inventory.ini all_tasks.yml --ask-become-pass
```

![all_tasks](IMG/all_tasks.png)

5. Przeprowadzi≈Çam operacje wzglƒôdem maszyny z wy≈ÇƒÖczonym serwerem SSH, odpiƒôtƒÖ kartƒÖ sieciowƒÖ.

Wy≈ÇƒÖczenie serwera SSH i od≈ÇƒÖczenie karty sieciowej ma na celu zasymulowanie sytuacji, w kt√≥rej maszyna jest "nieosiƒÖgalna" zdalnie. Jest to u≈ºyteczne w kontek≈õcie testowania scenariuszy awarii lub konfiguracji systemu, kt√≥re wymagajƒÖ interakcji z maszynƒÖ, kt√≥ra nie jest dostƒôpna w normalny spos√≥b.

Kroki kt√≥re wyonaam przed uruchomieniem poprzednich zada≈Ñ (na maszynie docelowej - `asimble-target`):

```bash
#Odlaczenie serwera SSH
sudo systemctl stop sshd

#Wylaczenie karty sieciowej
sudo ifconfig eth0 down
```

Nastƒôpnie uruchomiam wcze≈õniej przeprowadzone zadania.

![test](IMG/test.png)

Podczas wykonywania playbooka Ansible napotkano problem z po≈ÇƒÖczeniem do maszyny ansible-target. PrzyczynƒÖ b≈Çƒôdu by≈Ço wy≈ÇƒÖczenie serwera SSH na tej maszynie oraz od≈ÇƒÖczenie interfejsu sieciowego, co uniemo≈ºliwi≈Ço nawiƒÖzanie po≈ÇƒÖczenia zdalnego. Aby rozwiƒÖzaƒá ten problem, przywr√≥cono serwis SSH do dzia≈Çania oraz ponownie aktywowano interfejs sieciowy, co pozwoli≈Ço na poprawne wykonanie kolejnych operacji.

#### ZarzƒÖdzanie stworzonym artefaktem
Moim artefaktem w projekcie z poprzednich zajƒôƒá by kontener.
W celu zautomatyzowania procesu u≈ºy≈Çam playbooka Ansible oraz struktury r√≥l, utworzonej za pomocƒÖ `ansible-galaxy`.
[role](https://docs.ansible.com/ansible/latest/playbook_guide/playbooks_reuse_roles.html)


Uworzy≈Çam nowƒÖ role `cjson-role`:

```bash
ansible-galaxy init cjson-role
```

![CJSON-role](IMG/cjson-role.png)

Po utworzeniu roli skopiowa≈Çam pliki `cjson.rpm` i `main.c` oraz edytowa≈Çam plik `main.yaml` tak, aby:
- przesy≈Ça≈Ç artefakty na `ansible-target`,
- instaluje Dockera oraz jego zale≈ºno≈õci,
- uruchamia kontener,
- instaluje biblioteki z pliku `.rpm`
- kompuluje program,
- uruchamia program i pobiera wynik.

Zawarto≈õƒá pliku `main.yaml`. Plik znajduje w folderze `cjson/tasks` w stworzonej roli `cjson-role`.

```yaml
---
# tasks file for cjson-role
- name: Create artifacts directory
  become: yes
  file:
    path: /home/ansible/cjson
    state: directory
    owner: ansible
    group: ansible
    mode: '0755'

- name: Copy artifacts to target
  copy:
    src: "{{ item }}"
    dest: /home/ansible/cjson/
    mode: '0644'
  loop:
    - files/cjson.rpm
    - files/main.c

- name: Install python3-requests
  ansible.builtin.dnf:
    name: python3-requests
    state: present

- name: Install Docker
  become: yes
  dnf:
    name: docker
    state: present
  
- name: Ensure Docker is started
  become: yes
  service:
    name: docker
    state: started
    enabled: true

- name: Add ansible to docker group
  user:
    name: ansible
    groups: docker
    append: true

- name: Start fedora container
  community.docker.docker_container:
    name: cjson
    image: fedora:41
    state: started
    command: sleep infinity
    volumes:
      - /home/ansible/cjson:/tmp:z

- name: Install gcc, cjson and tools
  community.docker.docker_container_exec:
    container: cjson
    command: dnf install -y gcc make /tmp/cjson.rpm

- name: Compile source file
  community.docker.docker_container_exec:
    container: cjson
    command: gcc -o /tmp/example /tmp/main.c -lcjson

- name: Run program
  community.docker.docker_container_exec:
    container: cjson
    command: bash -c "LD_LIBRARY_PATH=/usr/local/lib64 /tmp/example"
  register: result

- name: Print the result of the program
  debug:
    var: result.stdout
```

Uruchomienie roli przez playbook-cjson.yaml.

Zawarto≈õƒá pliku `playbook-cjson.yaml`:

```yaml
- name: Deploy CJSON
  hosts: ansible-target
  become: true
  roles:
    - cjson-role
```

Uruchomienie:

```bash
ansible-playbook /home/mpaskowski/MDO2025_INO/INO/GCL02/MP417574/Sprawozdanie3/ansible_playbooks/playbook-cjson.yaml -i /home/mpaskowski/MDO2025_INO/INO/GCL02/MP417574/Sprawozdanie3/ansible_1/inventory.ini --ask-become-pass
```

![wynik_1](IMG/result_1.png)


Inny spos√≥b - za pomocƒÖ playbooka Ansible.


Zbudowa≈Çam playbooka Ansible, kt√≥ry:
1. Buduje i uruchomia kontener sekcji `Deploy` z poprzednich zajƒôƒá.
2. Pobiera z `Docker Hub` aplikacjƒô w ramach kroku `Publish`
3. Weryfikuje ≈ÇƒÖczno≈õƒá z kontenerem
4. Zatrzymuje i usuwa kontener

Plik `project.yml`:
```yml
---
- name: Zbudowanie, uruchomienie, weryfikacja i usuniƒôcie kontenera
  hosts: all
  become: yes
  vars_files:
    - /home/mpaskowski/MDO2025_INO/INO/GCL02/MP417574/Sprawozdanie3/ansible_playbooks/vars.yml
  tasks:

    # Krok 1: Zalogowanie do Docker Hub
    - name: Zaloguj siƒô do Docker Hub
      docker_login:
        registry: "docker.io"
        username: "icharne2"
        password: "{{ docker_password }}"  # Przechowuj has≈Ço w zmiennej

    # Krok 2: Pobierz obraz z Docker Hub
    - name: Pobierz obraz z Docker Hub
      docker_image:
        name: "icharne2/cjson-deploy"
        source: pull

    # Krok 3: Zbudowanie i uruchomienie kontenera
    - name: Zbuduj kontener Docker z aplikacjƒÖ
      docker_container:
        name: "cjson-deploy"
        image: "icharne2/cjson-deploy"
        state: started
        restart_policy: no  # Zapobiegamy automatycznemu restartowi kontenera
        command: "sh -c './example && sleep 9999'"  # Uruchamiamy aplikacjƒô 'example' i zatrzymujemy kontener na kilka godzin

    # Krok 4: Weryfikacja, czy aplikacja dzia≈Ça
    - name: Sprawdzenie, czy aplikacja dzia≈Ça
      command: "docker exec cjson-deploy /bin/bash -c './example'"
      register: result
      failed_when: result.rc != 0  # Je≈õli aplikacja zako≈Ñczy siƒô b≈Çƒôdem, playbook siƒô nie powiedzie

    # Krok 5: Zatrzymanie kontenera
    - name: Zatrzymanie kontenera cjson-deploy
      docker_container:
        name: "cjson-deploy"
        state: stopped

    # Krok 6: Usuniƒôcie kontenera
    - name: Usuniƒôcie kontenera cjson-deploy
      docker_container:
        name: "cjson-deploy"
        state: absent
```

Uruchomienie:

```bash
ansible-playbook project.yml -i /home/mpaskowski/MDO2025_INO/INO/GCL02/MP417574/Sprawozdanie3/ansible_1/inventory.ini --ask-become-pass
```

![result](IMG/project_result.png)

Przedstawiono spos√≥b wykorzystania Ansible do zdalnego zarzƒÖdzania systemami operacyjnymi. Wykonane dzia≈Çania obejmowa≈Çy przygotowanie ≈õrodowiska, konfiguracjƒô dostƒôpu miƒôdzy maszynami, stworzenie pliku inwentaryzacji oraz realizacjƒô wybranych operacji administracyjnych za pomocƒÖ playbook√≥w.

Zadanie pokaza≈Ço, ≈ºe Ansible pozwala na centralne sterowanie wieloma systemami jednocze≈õnie, bez konieczno≈õci rƒôcznego logowania siƒô na ka≈ºdƒÖ maszynƒô. Dziƒôki temu mo≈ºliwe by≈Ço wykonanie operacji takich jak aktualizacja pakiet√≥w, kopiowanie plik√≥w czy zarzƒÖdzanie kontenerami. 

### Pliki odpowiedzi dla wdro≈ºe≈Ñ nienadzorowanych - Kickstart
Do wykonania zadania skorzysta≈Çam z pliku systemu Fedora 41 z poprzenic zajƒôƒá. Jako administrator skopiowa≈Çam plik pod scie≈ºkƒÖ `/root/anaconda-ks.cfg` do folderu `Sprawozdanie3`.

Nastƒôpnie zmodyfikowa≈Çam plik `anaconda-ks.cfg` dodajƒÖc informacjƒô o repozytoriach oraz zmieniajƒÖƒá nazwƒô u≈ºytkownika.

Dodane roepozytoria:

```conf
url --mirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=fedora-41&arch=x86_64
repo --name=update --mirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=updates-released-f41&arch=x86_64
```

Plik po dokonanych zmian zosta≈Ç przes≈Çany na Githuba. Nastƒôpnie skopiowa≈Çam link do pliku `Raw` w pliku na Githubie oraz skr√≥ciam link za pomocƒÖ [TinyURL](https://tinyurl.com/). Uzyskany link `https://tinyurl.com/3ehberxn`.

Podczas instalcji nowej maszyny, w menu statorym instalatora klikne≈Çam `e`, aby wej≈õƒá do trybu edycji polece≈Ñ GRUB. Edytowa≈Çam parametry instalacyjne.

![Instalacja](IMG/Lab9/instalacja.png)

Zapisuje zmiany `Crtl-X`.

Instalator uruchomiony w trybie graficznym:

![Instalacja2](IMG/Lab9/instalacja2.png)

Wszytkie informacje po poprawnym odczytaniu pliku powinny siƒô automatycznie za≈Çadowaƒá.
Po chwili instalator przeszed≈Ç dalej.

![Instalacja3](IMG/Lab9/instalacja3.png)

Po zako≈Ñczeniu nale≈ºa≈Ço ponownie urucomiƒá system oraz odpiƒÖƒá plik `iso`.

![Instalacja4](IMG/Lab9/instalacja4.png)

#### Rozszerzenie pliku odpowiedzi o dodatkowe opcje

W kolejnym kroku rozszerzy≈Çam plik `anaconda-ks.cfg`, dodajƒÖc:

- `reboot` ‚Äî aby system automatycznie uruchomi≈Ç siƒô ponownie po zako≈Ñczeniu instalacji,  
- `network --hostname=fedora.test` ‚Äî aby przypisaƒá maszynie nowƒÖ nazwƒô hosta,  

Zaktualizowany plik ponownie umie≈õci≈Çam na GitHubie. Przeprowadzi≈Çam proces instalacji jeszcze raz ‚Äî tym razem system samodzielnie wykona≈Ç restart po zako≈Ñczeniu.

Zmieniony plik:

```
# Generated by Anaconda 41.35
# Generated by pykickstart v3.58
#version=DEVEL

# Keyboard layouts
keyboard --vckeymap=us --xlayouts='us'

# System language
lang en_US.UTF-8

# Network information
network --bootproto=dhcp --device=enp0s3 --hostname=fedora.test --ipv6=auto --activate

# Repository Added
url --mirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=fedora-41&arch=x86_64
repo --name=update --mirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=updates-released-f41&arch=x86_64

%packages
@^server-product-environment

%end

# Run the Setup Agent on first boot
firstboot --enable

# Generated using Blivet version 3.11.0
ignoredisk --only-use=sda

#Automatic partitioning
autopart

# Partition clearing information
clearpart --none --initlabel

# System timezone
timezone Europe/Warsaw --utc

# Root password
rootpw --iscrypted --allow-ssh $y$j9T$f42zTKc8FiEAXyECki5fEwka$TuNUxPrCp2i9fdDQZY3W7TDOMG9aGTdtVCupZdfmsJ4
user --groups=wheel --name=kickstart --password=$y$j9T$eKgdXsLTmrSEuSPjMoYgokYv$EzoYQqbicne4onhOqzWeVa420g3u.59tCdlzzVbTSb8 --iscrypted --gecos="kickstart"

reboot
```

Wynik sprawdzenie nazwy hosta po ponownej instalacji.

![Hostname](IMG/Lab9/hostname.png)

#### Instalacja biblioteki cjson w wykorzystaniem pliku odpowiedzi

Do wykonania zadania przebudowa≈Çam pliki s≈Çu≈ºace do sorprzenia pliku `rpm` z poprzednich zajƒôƒá.
W celu udustƒôpnienia biblioteki w formie repozytorium YUM, pobra≈Çam serwer `Apache` i narzƒôdzie `createrepo `.

```bash
sudo dnf install -y httpd createrepo policycoreutils-python-utils
```

Uruchom≈Çam i w≈ÇƒÖcz≈Çam HTTPD oraz otworzy≈Çam port 80 

```bash
sudo systemctl enable --now httpd

sudo firewall-cmd --permanent --add-service=http
sudo firewall-cmd --reload
```

Nastƒôpnie utworzy≈Çam katalog `/var/www/html/cjson`.

```bash
sudo mkdir -p /var/www/html/cjson
sudo cp cjson.rpm /var/www/html/cjson/
```

Nada≈Çam poprawny kontekst SELinux i od≈õwie≈ºy≈Çam oraz wygenerowalam metadane RPM

```bash
sudo semanage fcontext -a -t httpd_sys_content_t "/var/www/html/cjson(/.*)?"
sudo restorecon -Rv /var/www/html/cjson

sudo createrepo /var/www/html/cjson
```

Sprawdzenie poprawno≈õci dzialania:

```bash
curl -I http://192.168.0.7/cjson/repodata/repomd.xml
curl -s  http://192.168.0.7/cjson/ | grep '\.rpm'
```
Sprawdzenie z przeglƒÖdarki, czy dzia≈Ça:

![Resut-www](IMG/Lab9/repo_cjson.png)

Instalacja weryfikujƒÖca poprawne pobranie artefaktu z utworzonego repozytorium `cjson`. 

![Resut-www](IMG/Lab9/repo_cjson_inst1.png)

Edycja pliku `anaconda-ks.cfg`:

```
# Generated by Anaconda 41.35
# Generated by pykickstart v3.58
#version=DEVEL

# Keyboard layouts
keyboard --vckeymap=us --xlayouts='us'

# System language
lang en_US.UTF-8

# Network information
network --bootproto=dhcp --device=enp0s3 --hostname=fedora.test --ipv6=auto --activate

# Repository Added
url --mirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=fedora-41&arch=x86_64
repo --name=update --mirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=updates-released-f41&arch=x86_64
repo --name=cjson --baseurl=http://192.168.0.7/cjson/

%packages
@core
cjson
gcc
glibc
curl
%end

# Run the Setup Agent on first boot
firstboot --enable

# Generated using Blivet version 3.11.0
ignoredisk --only-use=sda

#Automatic partitioning
autopart

# Partition clearing information
clearpart --none --initlabel

# System timezone
timezone Europe/Warsaw --utc

# Root password
rootpw --iscrypted --allow-ssh $y$j9T$f42zTKc8FiEAXyECki5fEwka$TuNUxPrCp2i9fdDQZY3W7TDOMG9aGTdtVCupZdfmsJ4
user --groups=wheel --name=kickstart --password=$y$j9T$eKgdXsLTmrSEuSPjMoYgokYv$EzoYQqbicne4onhOqzWeVa420g3u.59tCdlzzVbTSb8 --iscrypted --gecos="kickstart"

# Display result
  %post --interpreter /bin/bash
  echo "Confirming installation..."
  ls /usr/include/cjson
  ls /usr/lib/libcjson*
  %end

reboot
```

Zmiany wprowadzone do pliku:
- Potrzebne pakiety `%packages` miƒôdzy innymi pakiet `cJSON`,
- Repozytorium `cjson`,
- W sekcji `%post` zweryfikowano obecno≈õƒá artefaktu.

Sprawdzenie, czy wszytko przebieg≈Ço pomy≈õ≈Çnie:

![Resut](IMG/Lab9/add_repo_cjson.png)

W celu zautomatyzowania procesu mo≈ºna utworzyƒá bootowalny `.iso`.
Aby tego dokonaƒá nale≈ºy zmodyfikowaƒá obraz instalatora systemu.

Na sam poczatek utworzylam katalog wspoldzielony miedzy Fedora a systemem Windows.

Rozpakowanie `.iso`:

```bash
sudo dnf install -y xorriso

mkdir ~/iso-raw  
cd ~/iso-raw  
xorriso -osirrox on -indev /≈õcie≈ºka/do/Fedora-41.iso -extract / .  
```

Edycja pliku `boot/grub2/grub.cfg`:

```
  set default="0"
  
  function load_video {
    insmod all_video
  }
  
  load_video
  set gfxpayload=keep
  insmod gzio
  insmod part_gpt
  insmod ext2
  insmod chain
  
  set timeout=0
  ### END /etc/grub.d/00_header ###
  
  search --no-floppy --set=root -l 'Fedora-E-dvd-x86_64-41'
  
  ### BEGIN /etc/grub.d/10_linux ###
  menuentry 'Install Fedora 41' --class fedora --class gnu-linux --class gnu --class os {
  	linux /images/pxeboot/vmlinuz inst.stage2=hd:LABEL=Fedora-E-dvd-x86_64-41 inst.ks=https://tinyurl.com/3ehberxn quiet
  	initrd /images/pxeboot/initrd.img
  }
```

Zawarto≈õƒá pliku: 
- dodanie plik konfiguracji poprzez adres `inst.ks=https://tinyurl.com/3ehberxn`,
- Domy≈õlna opcja - `set default="0"`,
- Aby instalacja rozpocze≈Ça siƒô od razu `set timeout=0`,
- Zmiana etykiety na `Fedora-KickStart`

Uruchomienie w katalogu `~/iso-raw`:

```bah
cd ~/iso-raw
xorriso -as mkisofs \
  -o /media/sf_ShareISO/Fedora-Kickstart.iso \
  -J -R -V "Fedora-Kickstart" \
  .
```

![Instal_Resut](IMG/Lab9/instal.png)

![Instal_Resut_2](IMG/Lab9/fedoraiso.png)

Skryp Powershell Script na systemie Windows - utworzenie maszyny z nowo utworzonego obrazu.

```bash
  $vmName     = "Fedora-instalation"
  $isoPath    = "C:\Users\Meg Paskowski\Desktop\ShareISO\Fedora-Kickstart.iso"
  $diskFolder = "C:\Users\Meg Paskowski\VirtualBox VMs\$vmName"
  $diskPath   = "$diskFolder\$vmName.vdi"
  $VBoxManage = "C:\Program Files\Oracle\VirtualBox\VBoxManage.exe"
  $memory     = 3048
  $cpus       = 2
  
  & $VBoxManage createvm --name $vmName --ostype Fedora_64 --register
  
  & $VBoxManage modifyvm $vmName --memory $memory --cpus $cpus --boot1 dvd --firmware efi
  
  New-Item -ItemType Directory -Path $diskFolder -Force | Out-Null
  & $VBoxManage createhd --filename "$diskPath" --size 2048
  
  & $VBoxManage storagectl $vmName --name "SATA Controller" --add sata --controller IntelAhci
  & $VBoxManage storageattach $vmName --storagectl "SATA Controller" --port 0 --device 0 --type hdd --medium "$diskPath"
  
  & $VBoxManage storagectl $vmName --name "IDE Controller" --add ide
  & $VBoxManage storageattach $vmName --storagectl "IDE Controller" --port 0 --device 0 --type dvddrive --medium "$isoPath"
  
  & $VBoxManage startvm $vmName --type gui
```

Odpalenie skrypu:

![Skrypt](IMG/Lab9/skrypt.png)

Skrypt utworzy≈Ç maszyne o podanej nazwie, dysk, kontrolery, podtsawowe ustawienie CPU, pamiƒôci i uruchomi≈Ç maszynƒô.

Automatyczna instalacja:

![Instal_final](IMG/Lab9/Instalacja_100.png)

### Wdra≈ºanie na zarzƒÖdzalne kontenery
#### Instalacja Kubernetes 

Sprzƒôt i wirtualizacja:

- **CPU / RAM**: minimalnie 2 CPU i 2 GiB RAM (zalecane 4 CPU i 4 GiB),
- **Wirtualizacja**: w≈ÇƒÖczona w BIOS/UEFI (Intel VT-x lub AMD-V),
- **Hypervisor**: zainstalowany VirtualBox, KVM2 (libvirt) lub Docker.

Dziƒôki temu maszyna (fizyczna lub wirtualna) bƒôdzie w stanie uruchomiƒá klaster Kubernetes lokalnie, bez konieczno≈õci dostƒôpu do zewnƒôtrznego ≈õrodowiska chmurowego.

Instalacja Minikube:

```bash
# Pobranie RPM
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-latest.x86_64.rpm
# Instalacja za pomocƒÖ RPM
sudo rpm -Uvh minikube-latest.x86_64.rpm
#Uruchomienie klastra z 2 CPU i 2 GiB RAM (driver: Docker)
minikube start --driver=docker --cpus=2 --memory=2048
```

- `minikube start`: inicjuje lokalny klaster Kubernetes w ramach wskazanego drivera (tutaj Docker).
- Parametry `--cpus` i `--memory` gwarantujƒÖ wystarczajƒÖce zasoby.

![Minikube](IMG/Lab10/1.png)

Poziom bezpiecze≈Ñstwa instalacji: `minikube kubectl -- get clusterrolebindings`. Pokazuje, jakie uprawnienia klastra (role) sƒÖ przypisane do kt√≥rych u≈ºytkownik√≥w czy serwis√≥w.

![clusterrolebindings](IMG/Lab10/2.png)

Domy≈õlne przestrzenie nazwy do separacji zasob√≥w w Kubernetes (np. default, kube-system, kube-public), kt√≥re pozwalajƒÖ na logicznƒÖ separacjƒô zasob√≥w `minikube kubectl -- get namespaces`.

![Namespaces](IMG/Lab10/3.png)

Wszystkie komponenty klastra komunikujƒÖ siƒô po bezpiecznym kanale TLS. Mo≈ºna to zweryfikowaƒá przez zalogowanie siƒô do maszyny Minikube: (`minikube ssh` -> `ls /var/lib/minikube/certs/`). W katalogu certs znajdujƒÖ siƒô certyfikaty API‚Äêservera, Kubelet, itp.

![SSH](IMG/Lab10/4.png)

Zainstalowa≈Çam pakietu `kubectl` i stworzy≈Çam alias `minikubectl`.

```bash
sudo dnf install -y kubectl

#Utworzenie aliasu
echo "alias minikubectl='minikube kubectl'" >> ~/.bashrc

#Za≈Çadowanie zmian
source ~/.bashrc
```

Dziƒôki temu zamiast pisaƒá za ka≈ºdym razem minikube kubectl mo≈ºemy uzywaƒá prostszego minikubectl.

Uruchomi≈Çam Kubernetesa

```bash
#Otwiera webowy interfejs do monitorowania klastra.
minikube start

minikubctl --get nodes
```

![start](IMG/Lab10/5.png)

![nodes](IMG/Lab10/6.png)

Uruchomi≈Çam Dashboard oraz skopiowa≈Çam usyskany adres IP oraz wykona≈Çam przekierowanie na port 8087.

```bash
minikube dashboard

ssh -L 8087:/127.0.0.1:43031 mpaskowski@192.168.0.7
```

W przeglƒÖdarce na Windows: `http://127.0.0.1:43031/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/`.

![nodes](IMG/Lab10/7.png)

![nodes](IMG/Lab10/8.png)

Rezultat:
![nodes](IMG/Lab10/9.png)

Z uwagi na korzystanie z biblioteki `cJSON` na poprzednic zajeciach, skorzysta≈Çam teraz z prostej aplikacji `app.py` w jezyku python.

Kod aplikacji:

```py
from flask import Flask
app = Flask(__name__)

@app.route("/")
def hello():
    return "<h1>Hello, Kubernetes! üå±</h1>"

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)
```

Flask nas≈Çuchuje na wszystkich interfejsach (0.0.0.0) na porcie 5000.

Dockerfile dla `app.py`:

```Dockerfile
# Gotowy obraz Pythona
FROM python:3.11-slim

# Katalog roboczy
WORKDIR /app

# Pliki z hosta do obrazu
COPY requirements.txt .
COPY app.py .

# Zale≈ºno≈õci
RUN pip install --no-cache-dir -r requirements.txt

# Port
EXPOSE 5000

# Domy≈õlna komenda
CMD ["python", "app.py"]
```
Powy≈ºszy kod pliku `Dockerfile` buduje lekki obraz na bazie `python:3.11-slim`, kopiuje kod i instaluje zale≈ºno≈õci.

Nastƒôpnie budowa≈Çam lokalnie obraz i za≈Çadowa≈Çam do Minikube.

```bash
docker build -t flask-hello .
minikube image load flask-hello
```

- `minikube image load` importuje obraz bezpo≈õrednio do lokalnego rejestru Minikube.

![app](IMG/Lab10/10.png)

Uruchomi≈Çam kontener w Kubernecie jako Pod:

```bash
minikube kubectl -- run flask-hello-pod \
  --image=flask-hello \
  --port=5000 \
  --image-pull-policy=Never
```

![img](IMG/Lab10/11.png)

Zwerifikowa≈Çam stan Poda: `minikube kubectl -- get pods`.

![pod](IMG/Lab10/12.png)

Nastƒôpnie wystawi≈Çam Pod jako Serice na porcie 5000:

```bash
minikube kubectl -- expose pod flask-hello-pod \
  --name=flask-hello-svc \
  --port=5000 \
  --target-port=5000 \
  --type=ClusterIP

  #Sprawdzenie
  minikube kubectl -- get svc
```

![pod2](IMG/Lab10/13.png)

Wynik w przegladarce:

![pod3](IMG/Lab10/14.png)

Przekierowa≈Çam port na 5080 VM:

```bash
minikube kubectl -- port-forward pod/flask-hello-pod 5080:5000
```

![podrt2](IMG/Lab10/15.png)

Na ho≈õcie uruchomi≈Çam tunel SSH:

```bash
ssh -N -L 5084:127.0.0.1:5080 mpaskowski@192.168.0.7
```

Sprawdzenie poprawnosci dzia≈Çania:

![web1](IMG/Lab10/16.png)

Weryfikacja dostƒôpu do aplikacji:

![web2](IMG/Lab10/17.png)

Nastƒôpnie przesz≈Çam do kroku Deploy i utworzy≈Çam plik `flask-deploy.yaml` z czterema r√≥wnoleg≈Çymi instancjami aplikacji (`replicas: 4`).

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flask-hello-deployment
  labels:
    app: flask-hello
spec:
  replicas: 4
  selector:
    matchLabels:
      app: flask-hello
  template:
    metadata:
      labels:
        app: flask-hello
    spec:
      containers:
      - name: flask-hello
        image: flask-hello
        ports:
        - containerPort: 5000
        imagePullPolicy: Never
```

I zastosowa≈Çam wdrƒÖ≈ºenia `minikube kubectl -- apply -f flask-deploy.yaml`.
Sprawdzilam rollout `minikube kubectl -- rollout status deployment/flask-hello-deployment`. Deployment przechodzi przez kolejne etapy a≈º do `Available`.

![deployment](IMG/Lab10/18.png)

Wystawi≈Çam Deploy jako Service:

```bash
minikube kubectl -- expose deployment flask-hello-deployment \
  --type=ClusterIP \
  --port=5000 \
  --name=flask-hello-svc-deploy
```

![deploy2](IMG/Lab10/19.png)

Uruchomienie i przekierowanie portu:

```bash
minikube kubectl -- port-forward service/flask-hello-svc-deploy 5081:5000
```

![deploy3](IMG/Lab10/20.png)

Sprawdzenie:
```bash
curl http://127.0.0.1:5081/
```
![test](IMG/Lab10/21.png)

Tunel SSH do hosta `ssh -N -L 5085:127.0.0.1:5081 mpaskowski@192.168.0.7`.

Wynik:

![result_deploy1](IMG/Lab10/22.png)

Weryfikacja w Dashboard:

![result_deploy2](IMG/Lab10/23.png)

### Wdra≈ºanie na zarzƒÖdzalne kontenery: Kubernetes (2)


Budowanie obrazu docker: `docker build -t icharne2/flask-hello:v1 .`

![app.py](IMG/Lab11/lab11_1.png)

Przeslanie na Docker Hub `docker push icharne2/flask-hello:v1`

![app.py2](IMG/Lab11/lab11_2.png)

Podobne kroki wykonuje z wersja 2 i zlƒÖ wersjƒÖ programu (sztucznie uszkodzona, generuje CrashLoopBackOff).

Zawarto≈õƒá `app.py`:

1. Dla wersji drugiej -poprawnej:

```py
from flask import Flask
app = Flask(__name__)

@app.route("/")
def hello():
    return "<h1>Hello, Kubernetes version 2! üöÄ</h1>"

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)

```

2. Dla wersji zlej - bad:

```py
# app.py ‚Äì wersja "bad" pod CrashLoopBackOff
from flask import Flask
import sys

raise RuntimeError("Intentional startup error for testing rollback")

app = Flask(__name__)

@app.route("/")
def hello():
    return "<h1>To nie powinno siƒô uruchomiƒá!</h1>"

if __name__ == "__main__":
    # Nigdy tu nie dojdziemy, bo wyjƒÖtek odpalili≈õmy wy≈ºej
    app.run(host="0.0.0.0", port=5000)

```

Potwierdzenie umieszczenia na `Docker Hub`:

![Docker_Hub](IMG/Lab11/lab11_3.png)

Plik `YAML` wdrƒÖ≈ºeniowy (`flask-deploy.yaml`).

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flask-hello-deployment
spec:
  replicas: 4
  selector:
    matchLabels:
      app: flask-hello
  template:
    metadata:
      labels:
        app: flask-hello
    spec:
      containers:
      - name: flask-hello
        image: icharne2/flask-hello:v1
        ports:
        - containerPort: 5000
        imagePullPolicy: Always
```

Domy≈õlna strategia `RollingUpdate` ma parametry:

```yaml
maxUnavailable: 1
maxSurge:       1
```

Co oznacza, ≈ºe `Kubernetes` -> Tworzy jednƒÖ nowƒÖ replikƒô czeka a≈º bƒôdzie Ready dopiero wtedy usuwa jednƒÖ starƒÖ. 

Wdrazenie:

```bash
#WdrƒÖ≈ºenie
minikube kubectl -- apply -f flask-deploy.yaml
minikube kubectl -- rollout status deployment/flask-hello-deployment
#Sprawdzenie pod
minikube kubectl -- get pods -l app=flask-hello
```

Sprawdzenie w `Dashboard`:
![Dashboard2](IMG/Lab11/lab11_4.png)

Wstawianie Deployment jako Service:

```bash
minikube kubectl -- expose deployment flask-hello-deployment \
  --name=flask-hello-svc --type=ClusterIP --port=5000 --target-port=5000
```

Przekierowanie portu:

```bash
minikube kubectl -- port-forward service/flask-hello-svc 5080:5000
```


Skalowanie replik - pozwala na dynamiczne dostosowanie liczby instancji pod obciƒÖ≈ºenie. Wykonane komendy:

```bash
# do 8 replik
minikube kubectl -- scale deployment flask-hello-deployment --replicas=8
minikube kubectl -- get pods -l app=flask-hello

#Sprawdzenie
minikube kubectl -- rollout status deployment/flask-hello-deployment
deployment "flask-hello-deployment" successfully rolled out
```

![8replik](IMG/Lab11/lab11_8replik.png)
![8replik](IMG/Lab11/lab11_8replik2.png)

```bash
# do 1 repliki
minikube kubectl -- scale deployment flask-hello-deployment --replicas=1
minikube kubectl -- get pods -l app=flask-hello

#Sprawdzenie
minikube kubectl -- rollout status deployment/flask-hello-deployment
deployment "flask-hello-deployment" successfully rolled out
```

![1replika](IMG/Lab11/lab11_1replika.png)
![1replika](IMG/Lab11/lab11_1replika2.png)

```bash
# do 0 replik
minikube kubectl -- scale deployment flask-hello-deployment --replicas=0
minikube kubectl -- get pods -l app=flask-hello

#Sprawdzenie
minikube kubectl -- get pods -l app=flask-hello
```

![0replik](IMG/Lab11/lab11_0replik.png)
![0replik](IMG/Lab11/lab11_0replik2.png)

```bash
# z powrotem do 4 replik
minikube kubectl -- scale deployment flask-hello-deployment --replicas=4
minikube kubectl -- get pods -l app=flask-hello
minikube kubectl -- rollout status deployment/flask-hello-deployment

#Sprawdzenie
minikube kubectl -- get pods -l app=flask-hello
```

![4repliki_2](IMG/Lab11/lab11_4repliki2.png)
![4repliki_2](IMG/Lab11/lab11_4repliki2_2.png)

Aktualizacja obrazu (Rolling Update):
- zastƒôpuje stopniowo stare repliki nowymi, zapewniajƒÖc ciƒÖg≈Ço≈õƒá dzia≈Çania.
- W przypadku wersji bad, nowe Pody nie przechodzƒÖ do stanu Ready ‚Üí stara wersja pozostaje aktywna.

Wykonane komendy:
1. Prze≈ÇƒÖczenie na wersje 2:

```bash
minikube kubectl -- set image deployment/flask-hello-deployment \
  flask-hello=icharne2/flask-hello:v2
minikube kubectl -- rollout status deployment/flask-hello-deployment
```

![ver2](IMG/Lab11/lab11_v2.png)
![ver2](IMG/Lab11/lab11_v2_2.png)

2. Cofniƒôcie do wersji 1:

```bash
minikube kubectl -- set image deployment/flask-hello-deployment \
  flask-hello=icharne2/flask-hello:v1
minikube kubectl -- rollout status deployment/flask-hello-deployment
```

![ver1](IMG/Lab11/lab11_v1.png)
![ver1](IMG/Lab11/lab11_v1_2.png)

3. wersja z bledem:

```bash
minikube kubectl -- set image deployment/flask-hello-deployment \
  flask-hello=icharne2/flask-hello:bad
```

![bad](IMG/Lab11/lab11_bad.png)

Strategia RollingUpdate dla obrazu `bad`:

W przypadku obrazu `bad` nowe Pody natychmiast padajƒÖ (`CrashLoopBackOff`), wiƒôc nigdy nie osiƒÖgnƒÖ stanu Ready. Kubernetes nie usuwa kolejnych starych Pod√≥w. W efekcie w Dashboardzie jest widoczne kilka Pod√≥w z `:v1` w Running oraz kilka nowych z `:bad` w `Error/` `CrashLoopBackOff` ‚Äî dop√≥ki nowe repliki nie udowodniƒÖ, ≈ºe potrafiƒÖ siƒô uruchomiƒá, stare repliki pozostajƒÖ w≈ÇƒÖczone.

Historia i Rollback:

```bash
#Wy≈õwietlenie historii rewizji
minikube kubectl -- rollout history deployment/flask-hello-deployment

#szczeg√≥≈Çy konkretnej rewizji
minikube kubectl -- rollout history deployment/flask-hello-deployment --revision=18
```

![history](IMG/Lab11/history.png)

Przywracanie do konkretnej rewizji:

```bash
minikube kubectl -- rollout undo deployment/flask-hello-deployment --to-revision=6
minikube kubectl -- rollout status deployment/flask-hello-deployment
minikube kubectl -- get pods -l app=flask-hello
```

![revision](IMG/Lab11/revision.png)

Aby ustawic opis w kolumnie `CHANGE-CAUSE` nale≈ºy dodaƒá `-- record` w komendzie `minikube kubectl -- set image deployment/flask-hello-deployment \ flask-hello=icharne2/flask-hello:v2 --record` 

Wtedy `rollout history` bƒôdzie pokazywa≈Ço, jakƒÖ komendƒÖ dokonywane by≈Çy zmiany.

#### Kontrola wdrƒÖ≈ºeniowa

Skrypt `wait-rollout.sh`, kt√≥ry w ciƒÖgu 60 s sprawdzi, czy Deployment osiƒÖgnƒÖ≈Ç stan Available.

```sh
#!/usr/bin/env bash

# wait-rollout.sh
# Skrypt czeka do 60 sekund (domy≈õlnie) na to, a≈º Deployment osiƒÖgnie stan Available.

DEPLOY="${1:-flask-hello-deployment}"
TIMEOUT="${2:-60}"

end=$((SECONDS + TIMEOUT))
echo "Czekam na dostƒôpno≈õƒá Deploymentu '$DEPLOY' (max $TIMEOUT s)..."

while [ $SECONDS -lt $end ]; do
  status=$(kubectl get deployment "$DEPLOY" \
    -o jsonpath='{.status.conditions[?(@.type=="Available")].status}')
  if [[ "$status" == "True" ]]; then
    echo "Deployment '$DEPLOY' jest dostƒôpny."
    exit 0
  fi
  sleep 2
done

echo "Timeout! Deployment '$DEPLOY' nie sta≈Ç siƒô dostƒôpny w ciƒÖgu $TIMEOUT s."
exit 1
```

Nada≈Çam prawa do wykonywalno≈õci `chmod +x wait-rollout.sh` oraz uruchomi≈Çam skrypt `./wait-rollout.sh flask-hello-deployment 60`.

![skrypt](IMG/Lab11/lab_11_skrypt.png)


#### Strategie wdro≈ºenia

Wersje wdro≈ºen w pliku `yaml`:
1. `Recreate`

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flask-hello-deployment
spec:
  replicas: 4

  strategy:
    type: Recreate

  selector:
    matchLabels:
      app: flask-hello
  template:
    metadata:
      labels:
        app: flask-hello
    spec:
      containers:
      - name: flask-hello
        image: icharne2/flask-hello:v1
        ports:
        - containerPort: 5000
        imagePullPolicy: Always
```

Nastƒôpnie wykona≈Çam:

```bash
# 1. Stworzenie Deployment z Recreate
minikube kubectl -- apply -f flask-deploy-recreate.yaml

# 2. Sprawdzenie rollout
minikube kubectl -- rollout status deployment/flask-hello-deployment-recreate

# 3. Implementacja obrazu "bad"
minikube kubectl -- set image deployment/flask-hello-deployment-recreate \
  flask-hello=icharne2/flask-hello:bad

# 4. Obserwacja Pod√≥w
minikube kubectl -- get pods -l app=flask-hello
```

`Recreate`: przy tej strategii starych P√≥d√≥w nie zastƒôpuje siƒô stopniowo ‚Äî wszystkie sƒÖ najpierw usuwane, a dopiero potem tworzone nowe repliki. Podczas prze≈ÇƒÖczenia na obraz `bad` wszystkie 4 Pody zniknƒô≈Çy, a dopiero potem wpad≈Çy w `CrashLoopBackOff`. Nie by≈Ço zachowania czƒô≈õci starej wersji, co mo≈ºe prowadziƒá do ca≈Çkowitej niedostƒôpno≈õci aplikacji w czasie wdro≈ºenia.

![Recreate](IMG/Lab11/Recreate.png)

2. `Rolling Update` (z parametrami maxUnavailable > 1, maxSurge > 20%)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flask-hello-deployment-rolling
spec:
  replicas: 4
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 2        # do 2 dodatkowych Pod√≥w poza 4
      maxUnavailable: 2  # do 2 starych jednocze≈õnie mo≈ºe byƒá niedostƒôpnych
  selector:
    matchLabels:
      app: flask-hello
  template:
    metadata:
      labels:
        app: flask-hello
    spec:
      containers:
      - name: flask-hello
        image: icharne2/flask-hello:v1
        ports:
        - containerPort: 5000
        imagePullPolicy: Always
```

Nastƒôpnie wykona≈Çam:

```bash
minikube kubectl -- apply -f flask-deploy-rolling.yaml
minikube kubectl -- rollout status deployment/flask-hello-deployment-rolling

# Aktualizacja na v2
minikube kubectl -- set image deployment/flask-hello-deployment-rolling \
  flask-hello=icharne2/flask-hello:v2
minikube kubectl -- rollout status deployment/flask-hello-deployment-rolling

# Obserwacja: 
minikube kubectl -- get pods -l app=flask-hello-deployment-rolling
```

`RollingUpdate` (maxSurge=2, maxUnavailable=2): pozwala na szybsze przechodzenie miƒôdzy wersjami, tworzƒÖc do 2 nowych repliki ponad wymagane 4 i dopuszczajƒÖc do 2 niegotowych starych. To balansuje szybko≈õƒá aktualizacji z minimalnym ryzykiem utraty dostƒôpno≈õci.

![RollingUpdate](IMG/Lab11/RollingUpdate.png)

3. `Canary Deployment workload`

Stworzy≈Çam dwa pliki `yaml`:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flask-hello-deployment-main
spec:
  replicas: 3
  selector:
    matchLabels:
      app: flask-hello
      role: main
  template:
    metadata:
      labels:
        app: flask-hello
        role: main
    spec:
      containers:
      - name: flask-hello
        image: icharne2/flask-hello:v1
        ports:
        - containerPort: 5000
```

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flask-hello-deployment-canary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: flask-hello
      role: canary
  template:
    metadata:
      labels:
        app: flask-hello
        role: canary
    spec:
      containers:
      - name: flask-hello
        image: icharne2/flask-hello:v2
        ports:
        - containerPort: 5000
```

Nastƒôpnie wykona≈Çam:

1. Dla `main`:

```bash
#WdrƒÖ≈ºenie
minikube kubectl -- apply -f flask-deploy-main.yaml

#Weryfikacja
minikube kubectl -- get deployments
minikube kubectl -- get pods -l app=flask-hello,role=main

#Expose g≈Ç√≥wny Deployment jako Service
minikube kubectl -- expose deployment flask-hello-deployment-main \
  --name=flask-hello-svc-main \
  --port=5000 \
  --target-port=5000

#Weryfikacja
minikube kubectl -- get svc flask-hello-svc-main
```

2. dla `Canary `:

```bash
#WdrƒÖ≈ºenie
minikube kubectl -- apply -f flask-deploy-canary.yaml

#Weryfikacja
minikube kubectl -- get deployments
minikube kubectl -- get pods -l app=flask-hello,role=canary

#Expose g≈Ç√≥wny Deployment jako Service
minikube kubectl -- expose deployment flask-hello-deployment-canary \
  --name=flask-hello-svc-canary \
  --port=5000 \
  --target-port=5000

#Weryfikacja
minikube kubectl -- get svc flask-hello-svc-canary
```

![Weryfikacja](IMG/Lab11/weryfikacja_punktc.png)

Oraz wykona≈Çam przekierowanie portu:

```bash
# ‚Äì g≈Ç√≥wny:
minikube kubectl -- port-forward service/flask-hello-svc-main 5081:5000

# ‚Äì canary:
minikube kubectl -- port-forward service/flask-hello-svc-canary 8090:5000
```

![Weryfikacja](IMG/Lab11/v1.png)

![Weryfikacja](IMG/Lab11/v2.png)

Utworzy≈Çam dwa oddzielne `Deploymenty` z etykietami `role=main` (3 repliki v1) i `role=canary` (1 replika v2), oraz odpowiednie us≈Çugi (`flask-hello-svc-main` i `flask-hello-svc-canary`). `Canary` dzia≈Ça≈Ç na osobnym porcie `8090`, dziƒôki port-forwardingowi mogli≈õmy r√≥wnolegle testowaƒá nowƒÖ wersjƒô (v2) na jednej replice, bez ≈ºadnego wp≈Çywu na stabilny ruch produkcyjny na porcie `5081` (v1) `main`.

Etykieta `role` pozwoli≈Ça precyzyjnie wyodrƒôbniƒá dwa typy Deployment√≥w, a dziƒôki osobnym Service‚Äôom mog≈Çam skierowaƒá ruch do g≈Ç√≥wnego i canary niezale≈ºnie. To rozwiƒÖzanie zapewnia bezpieczne testy nowej wersji, a w razie wykrycia problem√≥w ‚Äì szybki rollback bez przerywania dzia≈Çania produkcji.


Podsumowanie:
| Strategia         | Zasada dzia≈Çania                                                                                                                                      | Ryzyko przerwy w dzia≈Çaniu                          |
| ----------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------- |
| **Recreate**      | Usuniƒôcie wszystkich starych Pod√≥w przed tworzeniem nowych.                                                                                           | Wysokie ‚Äì aplikacja nie dostƒôpna podczas wdro≈ºenia. |
| **RollingUpdate** | Domy≈õlnie `maxUnavailable:1`, `maxSurge:1` ‚Äì stopniowa wymiana pojedynczych Pod√≥w. Mo≈ºna dostosowaƒá `maxUnavailable` i `maxSurge` na szybszƒÖ wymianƒô. | Niskie ‚Äì ciƒÖg≈Ço≈õƒá zachowana.                        |
| **Canary**        | R√≥wnoleg≈Çe utworzenie ma≈Çej grupy nowych Pod√≥w (np. 1 replika) obok stabilnej g≈Ç√≥wnej floty (np. 3 repliki). Umo≈ºliwia testy na czƒô≈õci ruchu.         | Minimalne ‚Äì nowe wersje testowane na ma≈Çej pr√≥bce.  |


#### Wykorzystanie AI do wykonania zada≈Ñ
