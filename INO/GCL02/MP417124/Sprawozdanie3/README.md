# Sprawozdanie (Zadania 8-10)


## Zadanie 9: Pliki odpowiedzi dla wdroźeń nienadzorowanych

1. **Instalacja systemu Fedora i wyciągniecie pliku odpowiedzi:**

Zaczełam od instalacji nienadzorowanego systemu Fedora stosując instalator siedziowy (netinst). Następnie pobrałam plik odpowiedzi z ścieżki: `/root/anaconda-ks.cfg`. Plik wyglądał następująco:

``` bash
# Generated by Anaconda 41.35
# Generated by pykickstart v3.58
#version=DEVEL

# Keyboard layouts
keyboard --vckeymap=pl --xlayouts='pl'
# System language
lang pl_PL.UTF-8

# Network information
network  --bootproto=dhcp --device=enp0s8 --ipv6=auto --activate
network  --hostname=devops

%packages
@^server-product-environment

%end

# Run the Setup Agent on first boot
firstboot --enable

# Generated using Blivet version 3.11.0
ignoredisk --only-use=sda
# Partition clearing information
clearpart --none --initlabel
# Disk partitioning information
part /boot/efi --fstype="efi" --ondisk=sda --size=600 --fsoptions="umask=0077,shortname=winnt"
part /boot --fstype="ext4" --ondisk=sda --size=1024
part pv.48 --fstype="lvmpv" --ondisk=sda --size=13700
volgroup fedora_devops --pesize=4096 pv.48
logvol / --fstype="ext4" --grow --maxsize=71680 --size=1024 --name=root --vgname=fedora_devops

# System timezone
timezone Europe/Warsaw --utc

#Root password
rootpw --lock
user --groups=wheel --name=mpalewicz --password=$y$j9T$WRrPj8ruqRhT/Jdus.DBcaJL$pWYQ/nbxtIYHk/3kZA0ToBjINDoFvZ9DzSa0LxpTFJ5 --iscrypted --gecos="Małgorz"
```

Następnie przeszłam do dodania linijek kodu odpowiedzialnych za określenie źródła pakietów instalacyjnych używanych podczas instalacji systemu oraz linijke odpowiedzialną za dodanie dodatkowych repozytorium o nazwie `update` do instalatora.

```bash
url --mirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=fedora-41&arch=aarch64
```
Wskazuje że instalator ma pobrać pakiety z jednego losowo wybranych mirrorów (serwerów lustrzanych Fedory), odpowiedniego dla `Fedory 41` i architektury `aarch64`.

```bash
repo --name=update --mirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=updates-released-f41&arch=aarch64
```
Repozytorium to zawiera wydane aktualizacje do `Fedory 41`, dzięki temu instaltor od razu korzysta z najnowszych dostępnych pakietów, zamiast tylko tych, które byłu dostępne w momencie wydania obrazu ISO.

Dodatkowo została dodana opcja na końcu pliku `reboot` w celu zadbania o automatyczne ponowne uruchomienia na końcu instalacji.
 

2. **Automatyczna instalacja Fedory z użyciem pliku Kickstart:** 

Następnie wykonałam instalację systemu Fedora przy użyciu pliku Kickstart na nowo utworzonej maszynie wirtualnej. 

W tym celu, podczas pierwszego uruchomienia instalatora Fedory, wybrałam opcję `Install Fedora`, a następnie nacisnęłam klawisz `e`, aby edytować polecenie rozruchowe przed startem systemu.

W edytorze GRUB dopisałam do linii komendy adres URL prowadzący do pliku Kickstart, który wcześniej pobrałam z repozytorium GitHub, korzystając z opcji `Raw`, aby uzyskać bezpośredni link do pliku. Dopisany adres:

```bash
inst.ks=https://raw.githubusercontent.com/InzynieriaOprogramowaniaAGH/MDO2025_INO/refs/heads/MP417124/INO/GCL02/MP417124/Sprawozdanie3/anaconda-ks.cfg
```

Po wprowadzeniu zmian zatwierdziłam je, naciskając kombinację klawiszy `Ctrl` + `X`, co spowodowało powrót do ekranu startowego i rozpoczęcie instalacji z wykorzystaniem wskazanego pliku Kickstart. Dzięki temu instalacja przebiegła w sposób automatyczny, zgodnie z zawartą w pliku konfiguracją.

![](https://github.com/InzynieriaOprogramowaniaAGH/MDO2025_INO/blob/MP417124/INO/GCL02/MP417124/Sprawozdanie3/Screenshots/Screenshot%202025-05-06%20at%207.40.16%E2%80%AFPM.png)


## Zadanie 10: Wdrażanie na zarządzalne kontenery: Kubernetes (1)

1. **Instalacja i uruchomienie Minikube:**

Proces rozpoczęłam od pobrania i zainstalowania narzędzia Minikube, które pozwala na uruchomienie lokalnego klastra Kubernetes na moim systemie Fedora działającym na architekturze aarch64. Najpierw pobrałam odpowiedni instalator za pomocą polecenia:

``` 
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-latest.aarch64.rpm
```
Następnie zainstalowałam Minikube przy użyciu menedżera pakietów RPM:
``` 
sudo rpm -Uvh minikube-latest.aarch64.rpm
```
Dzięki temu Minikube zostało poprawnie zainstalowane w moim systemie:
![7.22.34](https://github.com/InzynieriaOprogramowaniaAGH/MDO2025_INO/blob/MP417124/INO/GCL02/MP417124/Sprawozdanie3/Screenshots/Screenshot%202025-05-20%20at%207.22.34%E2%80%AFPM.png)

2. **Konfiguracja środowiska i uruchomienie klastra:**

Po instalacji uruchomiłam usługę Docker, która jest wykorzystywana jako sterownik kontenerowy przez Minikube:
```
sudo systemctl start docker
```
Następnie rozpoczęłam pracę z Minikube, uruchamiając lokalny klaster Kubernetes:
```
minikube start
```
![7.24.25](https://github.com/InzynieriaOprogramowaniaAGH/MDO2025_INO/blob/MP417124/INO/GCL02/MP417124/Sprawozdanie3/Screenshots/Screenshot%202025-05-20%20at%207.24.25%E2%80%AFPM.png)
Uruchomienie klastra przebiegło pomyślnie, co potwierdziło działanie podstawowych komponentów Kubernetes.

Aby ułatwić korzystanie z narzędzia kubectl w kontekście Minikube, dodałam alias do pliku `~/.bashrc`:

```
echo 'alias kubectl="minikube kubectl --"' >> ~/.bashrc
source ~/.bashrc
```
![7.25.16](https://github.com/InzynieriaOprogramowaniaAGH/MDO2025_INO/blob/MP417124/INO/GCL02/MP417124/Sprawozdanie3/Screenshots/Screenshot%202025-05-20%20at%207.25.16%E2%80%AFPM.png)

Dzięki temu wszystkie polecenia kubectl automatycznie korzystają z kontekstu klastra Minikube, co upraszcza zarządzanie zasobami Kubernetes. 


3. **Weryfikacja działania klastra:**

Sprawdziłam działanie podstawowych komponentów i status podów systemowych komendą:

```
kubectl get po -A
```
Widoczne były wszystkie kluczowe komponenty systemowe, działające bez problemów, co potwierdziło, że klaster jest gotowy do wdrożeń. Włączyłam również przydatny dodatek metrics-server w Minikube, który pozwala na zbieranie i monitorowanie metryk zasobów klastra:

```
minikube addons enable metrics-server
```

![7.25.51](https://github.com/InzynieriaOprogramowaniaAGH/MDO2025_INO/blob/MP417124/INO/GCL02/MP417124/Sprawozdanie3/Screenshots/Screenshot%202025-05-20%20at%207.25.51%E2%80%AFPM.png)

4. **Uruchomienie interfejsu Kubernetes Dashboard:**

Aby ułatwić zarządzanie klastrem oraz podgląd stanu wdrożonych aplikacji, uruchomiłam graficzny dashboard Minikube:

```
minikube dashboard &
```

![7.26.16](https://github.com/InzynieriaOprogramowaniaAGH/MDO2025_INO/blob/MP417124/INO/GCL02/MP417124/Sprawozdanie3/Screenshots/Screenshot%202025-05-20%20at%207.26.16%E2%80%AFPM.png)
Dzięki uruchomieniu go w tle nie blokowałam terminala i mogłam jednocześnie wykonywać dalsze polecenia. Dashboard otworzył się w przeglądarce pod lokalnym adresem, umożliwiając wygodne zarządzanie zasobami Kubernetes.

link:  `http://127.0.0.1:32955/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/#/workloads?namespace=default`.


5. **Uruchomienie przykładowego kontenera jako Pod:**

Kolejnym krokiem było uruchomienie prostej aplikacji – serwera nginx – jako pojedynczego poda w klastrze:

```
minikube kubectl -- run mojpod --image=nginx --port=80 --labels app=mojpod
```
![7.36.02](https://github.com/InzynieriaOprogramowaniaAGH/MDO2025_INO/blob/MP417124/INO/GCL02/MP417124/Sprawozdanie3/Screenshots/Screenshot%202025-05-20%20at%207.36.02%E2%80%AFPM.png)

Sprawdziłam, czy pod działa poprawnie, za pomocą:
```
kubectl get pods
```
oraz w zakładce `Workloads` > `Pods` w Dashboardzie. Wszystko wskazywało na prawidłowe działanie.
![7.36.46](https://github.com/InzynieriaOprogramowaniaAGH/MDO2025_INO/blob/MP417124/INO/GCL02/MP417124/Sprawozdanie3/Screenshots/Screenshot%202025-05-20%20at%207.36.46%E2%80%AFPM.png)
![7.32.25](https://github.com/InzynieriaOprogramowaniaAGH/MDO2025_INO/blob/MP417124/INO/GCL02/MP417124/Sprawozdanie3/Screenshots/Screenshot%202025-05-20%20at%207.32.25%E2%80%AFPM.png)
![7.38.08](https://github.com/InzynieriaOprogramowaniaAGH/MDO2025_INO/blob/MP417124/INO/GCL02/MP417124/Sprawozdanie3/Screenshots/Screenshot%202025-05-20%20at%207.38.08%E2%80%AFPM.png)

6. **Uzyskanie dostępu do aplikacji:**

Aby uzyskać dostęp do serwera nginx działającego w podzie, wyprowadziłam port lokalny na port kontenera:
```
kubectl port-forward pod/mojpod 8080:80
```
![7.39.45](https://github.com/InzynieriaOprogramowaniaAGH/MDO2025_INO/blob/MP417124/INO/GCL02/MP417124/Sprawozdanie3/Screenshots/Screenshot%202025-05-20%20at%207.39.45%E2%80%AFPM.png)

Po wpisaniu w przeglądarce adresu: `http://localhost:8080` pojawiła się strona powitalna nginx, co potwierdziło, że połączenie z pod-em działa poprawnie.

![7.42.17](https://github.com/InzynieriaOprogramowaniaAGH/MDO2025_INO/blob/MP417124/INO/GCL02/MP417124/Sprawozdanie3/Screenshots/Screenshot%202025-05-20%20at%207.42.17%E2%80%AFPM.png)
![7.42.44](https://github.com/InzynieriaOprogramowaniaAGH/MDO2025_INO/blob/MP417124/INO/GCL02/MP417124/Sprawozdanie3/Screenshots/Screenshot%202025-05-20%20at%207.42.44%E2%80%AFPM.png)

7. **Tworzenie i wdrożenie Deploymentu z pliku YAML:**

Aby przejść od pojedynczego poda do bardziej produkcyjnego wdrożenia z replikami, przygotowałam plik `nginx-deployment.yaml` z definicją Deploymentu:


```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: moj-nginx
spec:
  replicas: 4
  selector:
    matchLabels:
      app: moj-nginx
  template:
    metadata:
      labels:
        app: moj-nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
```

Deployment ten definiuje cztery repliki aplikacji nginx, zapewniając tym samym wysoką dostępność i skalowalność. Wdrożyłam go komendą `kubectl apply -f nginx-deployment.yaml` i sprawdziłam status wdrożenia `kubectl rollout status deployment/moj-nginx`. Wszystkie repliki uruchomiły się poprawnie i były dostępne.

![8.08.45](https://github.com/InzynieriaOprogramowaniaAGH/MDO2025_INO/blob/MP417124/INO/GCL02/MP417124/Sprawozdanie3/Screenshots/Screenshot%202025-05-20%20at%208.08.45%E2%80%AFPM.png)
![8.12.04](https://github.com/InzynieriaOprogramowaniaAGH/MDO2025_INO/blob/MP417124/INO/GCL02/MP417124/Sprawozdanie3/Screenshots/Screenshot%202025-05-20%20at%208.12.04%E2%80%AFPM.png)

8. **Eksponowanie Deploymentu jako usługi:**

Aby umożliwić dostęp do aplikacji spoza klastra, wyeksponowałam Deployment jako serwis typu NodePort:

```
kubectl expose deployment moj-nginx --type=NodePort --port=80
```


Następnie sprawdziłam, jaki port został przypisany na węźle klastra poprzez `kubectl get svc moj-nginx` i na podstawie przydzielonego portu `NodePort` (z zakresu 30000-32767) wyprowadziłam port lokalny na port serwisu `kubectl port-forward svc/moj-nginx 8080:80`. Po otwarciu w przeglądarce adresu `http://localhost:8080` mogłam korzystać z aplikacji działającej w Kubernetes.

![8.15.42](https://github.com/InzynieriaOprogramowaniaAGH/MDO2025_INO/blob/MP417124/INO/GCL02/MP417124/Sprawozdanie3/Screenshots/Screenshot%202025-05-20%20at%208.15.42%E2%80%AFPM.png)
![8.18.42](https://github.com/InzynieriaOprogramowaniaAGH/MDO2025_INO/blob/MP417124/INO/GCL02/MP417124/Sprawozdanie3/Screenshots/Screenshot%202025-05-20%20at%208.18.42%E2%80%AFPM.png)
![8.19.24](https://github.com/InzynieriaOprogramowaniaAGH/MDO2025_INO/blob/MP417124/INO/GCL02/MP417124/Sprawozdanie3/Screenshots/Screenshot%202025-05-20%20at%208.19.24%E2%80%AFPM.png)
![8.23.44](https://github.com/InzynieriaOprogramowaniaAGH/MDO2025_INO/blob/MP417124/INO/GCL02/MP417124/Sprawozdanie3/Screenshots/Screenshot%202025-05-20%20at%208.23.44%E2%80%AFPM.png)

## Zadanie 11: Wdrażanie na zarządzalne kontenery: Kubernetes (2)


1. **Przygotowanie nowego obrazu:**


Stworzyłam dwa obrazy Docker, które opublikowałam na Docker Hub pod nazwą `malgorzatapalewicz/moje-nginx`. Pierwszy obraz, oznaczony jako `v1`, bazował na oficjalnym obrazie `nginx:latest`. Do tego obrazu dodałam jedynie mój własny plik `index.html`, który skopiowałam do katalogu `/usr/share/nginx/html/index.html`, aby Nginx serwował moją stronę. Dockerfile dla tej wersji był prosty i wyglądał tak:

```
FROM nginx:latest
COPY index.html /usr/share/nginx/html/index.html
```

Obraz ten zbudowałam komendą `docker build -t malgorzatapalewicz/moje-nginx:v1 .` i opublikowałam go poleceniem `docker push malgorzatapalewicz/moje-nginx:v1`. Ta wersja działała prawidłowo i uruchamiała się bez problemów.

![7.06.24](https://github.com/InzynieriaOprogramowaniaAGH/MDO2025_INO/blob/MP417124/INO/GCL02/MP417124/Sprawozdanie3/Screenshots/Screenshot%202025-05-29%20at%207.06.24%E2%80%AFPM.png)

![7.16.11](https://github.com/InzynieriaOprogramowaniaAGH/MDO2025_INO/blob/MP417124/INO/GCL02/MP417124/Sprawozdanie3/Screenshots/Screenshot%202025-05-29%20at%207.16.11%E2%80%AFPM.png)

Następnie przygotowałam drugi obraz, oznaczony jako `v2`. Bazą również był oficjalny obraz `nginx:latest`, a do niego skopiowałam ten sam plik `index.html`. Jednak tym razem dodałam do Dockerfile polecenie: `CMD ["sh", "-c", "exit 1"]`. To polecenie powoduje, że po uruchomieniu kontenera natychmiast wykonywane jest wyjście z kodem błędu 1. Oznacza to, że gdy kontener się uruchamia, zamiast startować proces `Nginx`, wykonywane jest polecenie `exit 1`, które powoduje natychmiastowe zakończenie działania kontenera z błędem. W efekcie kontener przestaje działać zaraz po starcie i nie świadczy żadnych usług. Pełny Dockerfile dla wersji `v2` wyglądał tak:

```
FROM nginx:latest
COPY index.html /usr/share/nginx/html/index.html
CMD ["sh", "-c", "exit 1"]
```

Obraz zbudowałam i wypchnęłam do Docker Hub (do którego wcześniej się zalogowałam poprzez usługę `docker login` i otrzymałam informacje o prawidłowym połączeniu z moim urządzeniem) analogicznie jak w przypadku wersji `v1`.

![8.51.33](https://github.com/InzynieriaOprogramowaniaAGH/MDO2025_INO/blob/MP417124/INO/GCL02/MP417124/Sprawozdanie3/Screenshots/Screenshot%202025-05-31%20at%208.51.33%E2%80%AFPM.png)

![7.07.20](https://github.com/InzynieriaOprogramowaniaAGH/MDO2025_INO/blob/MP417124/INO/GCL02/MP417124/Sprawozdanie3/Screenshots/Screenshot%202025-05-29%20at%207.07.20%E2%80%AFPM.png)
![7.08.02](https://github.com/InzynieriaOprogramowaniaAGH/MDO2025_INO/blob/MP417124/INO/GCL02/MP417124/Sprawozdanie3/Screenshots/Screenshot%202025-05-29%20at%207.08.02%E2%80%AFPM.png)


2. **Kontrola wdrożenia:**

Wykonałam skalowanie wdrożenia poprzez modyfikację pola replicas w pliku YAML. Zwiększyłam liczbę replik do 8, następnie zmniejszyłam do 1, potem do 0, a finalnie przywróciłam 4 repliki.

8 replik:
![5.25.11](https://github.com/InzynieriaOprogramowaniaAGH/MDO2025_INO/blob/MP417124/INO/GCL02/MP417124/Sprawozdanie3/Screenshots/Screenshot%202025-05-31%20at%205.25.11%E2%80%AFPM.png)
1 replika:
![5.28.03](https://github.com/InzynieriaOprogramowaniaAGH/MDO2025_INO/blob/MP417124/INO/GCL02/MP417124/Sprawozdanie3/Screenshots/Screenshot%202025-05-31%20at%205.28.03%E2%80%AFPM.png)
4 repliki:
![5.28.58](https://github.com/InzynieriaOprogramowaniaAGH/MDO2025_INO/blob/MP417124/INO/GCL02/MP417124/Sprawozdanie3/Screenshots/Screenshot%202025-05-31%20at%205.28.58%E2%80%AFPM.png)


Zaktualizowałam również obraz kontenera na wersję `v2`, która zawierała błędną konfigurację (brak pliku `index.html`). Po wdrożeniu pojawił się błąd 404. Po wdrożeniu obrazu `v2` Kubernetes uruchamia kontenery, ale proces w nich się natychmiast kończy z błędem. Kubernetes próbuje automatycznie restartować pod, ale ponieważ problem jest powtarzalny (kontener kończy się zawsze tym samym błędem), pod szybko przechodzi w stan `CrashLoopBackOff`. Oznacza to, że Kubernetes wielokrotnie próbuje uruchomić pod, ale ten ciągle się wyłącza, co skutkuje niestabilnym i niedostępnym środowiskiem.

![8.58.58](https://github.com/InzynieriaOprogramowaniaAGH/MDO2025_INO/blob/MP417124/INO/GCL02/MP417124/Sprawozdanie3/Screenshots/Screenshot%202025-05-31%20at%208.58.58%E2%80%AFPM.png)


Kiedy wdrożyłam wadliwy obraz, zauważyłam, że pody nie uruchamiają się poprawnie. Widziałam, że wersja `v1` była stabilna, a która `v2` wprowadziła problem. Następnie wykonałam rollback do poprzedniej, działającej wersji komendą:

```
kubectl rollout undo deployment/moj-nginx
```
![9.01.21](https://github.com/InzynieriaOprogramowaniaAGH/MDO2025_INO/blob/MP417124/INO/GCL02/MP417124/Sprawozdanie3/Screenshots/Screenshot%202025-05-31%20at%209.01.21%E2%80%AFPM.png)

![5.30.41](https://github.com/InzynieriaOprogramowaniaAGH/MDO2025_INO/blob/MP417124/INO/GCL02/MP417124/Sprawozdanie3/Screenshots/Screenshot%202025-05-31%20at%205.30.41%E2%80%AFPM.png)


Następnie sprawdziłam historie zmian poprzez komendę `kubectl rollout history deployment/moj-nginx`:
![9.03.07](https://github.com/InzynieriaOprogramowaniaAGH/MDO2025_INO/blob/MP417124/INO/GCL02/MP417124/Sprawozdanie3/Screenshots/Screenshot%202025-05-31%20at%209.03.07%E2%80%AFPM.png)


  
3. **Strategie wdrożenia:**
Po przywróceniu poprawnej wersji aplikacji za pomocą polecenia `kubectl rollout undo`, przeszłam do napisania skryptu weryfikującego, czy wdrożenie zakończyło się poprawnie w ciągu 60 sekund.

Najpierw utworzyłam plik skryptu o nazwie `check_deployment.sh`, nadałam mu prawa do wykonania za pomocą komendy `chmod +x check_deployment.sh`, a następnie uruchomiłam go poleceniem `./check_deployment.sh`.

Kod: 
```
#!/bin/bash

DEPLOYMENT_NAME="moj-nginx"
NAMESPACE="default"  
TIMEOUT=60
INTERVAL=5
ELAPSED=0

echo "Sprawdzam status rollout deploymentu $DEPLOYMENT_NAME co $INTERVAL sekund..."

while [ $ELAPSED -lt $TIMEOUT ]; do
    STATUS=$(kubectl rollout status deployment/$DEPLOYMENT_NAME -n $NAMESPACE --timeout=1s 2>&1)

    if echo "$STATUS" | grep -q "successfully rolled out"; then
        echo "Deployment zakończył się sukcesem."
        exit 0
    fi

    echo "Trwa wdrożenie... ($ELAPSED sekund)"
    sleep $INTERVAL
    ELAPSED=$((ELAPSED + INTERVAL))
done

echo "Timeout! Deployment nie zakończył się w ciągu $TIMEOUT sekund."
exit 1
```
Kod został zastosowany to na wersji `v2`, więc prawidłowo wykrył, że deployment nie zakończył się:
![9.12.35](https://github.com/InzynieriaOprogramowaniaAGH/MDO2025_INO/blob/MP417124/INO/GCL02/MP417124/Sprawozdanie3/Screenshots/Screenshot%202025-05-31%20at%209.12.35%E2%80%AFPM.png)

Następnie przeszłam do testowania i porównania trzech podstawowych strategii wdrożeń dostępnych w Kubernetesie: `Recreate`, `RollingUpdate` oraz `Canary Deployment`.

### Strategia: Recreate

Podczas wdrażania strategii `Recreate`, wszystkie istniejące pody są najpierw usuwane, zanim nowe zostaną utworzone. Skutkuje to krótką przerwą w dostępności aplikacji.

Kod deploymentu:
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: moj-nginx-recreate
spec:
  replicas: 4
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: moj-nginx-recreate
  template:
    metadata:
      labels:
        app: moj-nginx-recreate
    spec:
      containers:
      - name: nginx
        image: malgorzatapalewicz/moje-nginx:v1
        ports:
        - containerPort: 80
```
Zastosowałam ten deployment w Kubernetesie poleceniem `kubectl apply -f moj-nginx-recreate.yaml`, a następnie sprawdziłam działające pody.

![9.24.40](https://github.com/InzynieriaOprogramowaniaAGH/MDO2025_INO/blob/MP417124/INO/GCL02/MP417124/Sprawozdanie3/Screenshots/Screenshot%202025-05-31%20at%209.24.40%E2%80%AFPM.png)

*Wnioski:*

- Strategia **Recreate** oznacza, że stare pody są najpierw całkowicie usuwane, zanim zostaną uruchomione nowe.
- W praktyce oznacza to, że w trakcie rollout’u może wystąpić krótka przerwa w dostępności usługi — bo nie ma jednocześnie działających starych i nowych podów.
- Z obserwacji wynika, że wszystkie repliki zostały utworzone poprawnie i działają (`READY=1/1`, `STATUS=Running`), co oznacza, że deployment zakończył się sukcesem.
- Ponieważ replik jest 4, i wszystkie są jednocześnie uruchomione po `rollout`, oznacza to, że stare pody zostały usunięte zanim nowe zaczęły działać (typowe dla `Recreate`).


### Strategia: Rolling Update

W kolejnym kroku przetestowałam strategię `RollingUpdate`, z dodatkowymi parametrami `maxUnavailable` i `maxSurge`.

Kod deploymentu:
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: moj-nginx-rolling
spec:
  replicas: 4
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 2        
      maxSurge: 20%            
  selector:
    matchLabels:
      app: moj-nginx-rolling
  template:
    metadata:
      labels:
        app: moj-nginx-rolling
    spec:
      containers:
      - name: nginx
        image: malgorzatapalewicz/moje-nginx:v1
        ports:
        - containerPort: 80
```

Po uruchomieniu deploymentu poleceniem `kubectl apply -f moj-nginx-rolling.yaml`, sprawdziłam aktualny stan podów oraz przebieg rolloutu.

![9.33.43](https://github.com/InzynieriaOprogramowaniaAGH/MDO2025_INO/blob/MP417124/INO/GCL02/MP417124/Sprawozdanie3/Screenshots/Screenshot%202025-05-31%20at%209.33.43%E2%80%AFPM.png)

*Wnioski:*

Podczas testowania strategii RollingUpdate, zauważyłam, że wszystkie pody zostały zaktualizowane bardzo szybko — wyglądało to, jakby zaktualizowały się prawie jednocześnie. Początkowo oczekiwałam bardziej widocznej, stopniowej wymiany podów.

Po przeanalizowaniu parametrów użytych w konfiguracji (maxUnavailable: 2, maxSurge: 20%) zrozumiałam, że Kubernetes mógł usunąć do dwóch starych podów naraz oraz uruchomić jeden nowy pod dodatkowo. W przypadku 4 replik oznacza to, że w krótkim czasie mogły zostać wymienione aż 3 pody, co tłumaczy szybkość operacji.

- Kubernetes wymienia pody stopniowo, nie wszystkie naraz. W moim przypadku dopuszczalne jest, by do 2 podów było niedostępnych jednocześnie (`maxUnavailable: 2`), a także, by powstał dodatkowy pod ponad docelową liczbę replik (`maxSurge: 20%` → 1 pod).
- Dzięki parametrom update jest płynny i serwis pozostaje dostępny (nigdy nie powinno być sytuacji, gdzie wszystkie pody są niedostępne).
- W `Recreate` najpierw są wyłączane wszystkie stare pody, potem tworzone nowe — tu serwis może być niedostępny na czas przełączania. `Rolling Update` minimalizuje takie ryzyko.

Nie odnotowałam przerw w dostępności ani problemów ze stabilnością podów. Strategia ta jest odpowiednia do produkcyjnych wdrożeń, gdzie kluczowa jest ciągła dostępność usługi.

