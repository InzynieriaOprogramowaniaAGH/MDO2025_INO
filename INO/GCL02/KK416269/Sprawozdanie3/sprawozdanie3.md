# **Sprawozdanie 3** - Metodyki DevOps
_________________________________________________________________________________________________________________________________________________________
## **LAB 8 - Automatyzacja i zdalne wykonywanie poleceÅ„ za pomocÄ… Ansible** 

Celem Ä‡wiczeÅ„ byÅ‚o przygotowanie pliku odpowiedzi i wykorzystanie go do przeprowadzenia nienadzorowanej instalacji systemu Fedora. Podczas zajÄ™Ä‡ skonfigurowaÅ‚am instalator tak, aby system po uruchomieniu automatycznie zawieraÅ‚ wymagane repozytoria, zaleÅ¼noÅ›ci oraz uruchamiaÅ‚ moje oprogramowanie. DziÄ™ki temu udaÅ‚o siÄ™ w peÅ‚ni zautomatyzowaÄ‡ proces instalacji i wdroÅ¼enia Å›rodowiska testowego.

### Instalacja zarzÄ…dcy Ansible
- [x] **UtwÃ³rz drugÄ… maszynÄ™ wirtualnÄ… o **jak najmniejszym** zbiorze zainstalowanego oprogramowania**
  - **Zastosuj ten sam system operacyjny, co "gÅ‚Ã³wna" maszyna (najlepiej teÅ¼ w tej samej wersji)**
  - **Zapewnij obecnoÅ›Ä‡ programu `tar` i serwera OpenSSH (`sshd`)**

UÅ¼yÅ‚am poniÅ¼szych poleceÅ„ Å¼eby siÄ™ upeniÄ‡ Å¼e tar i sshd bÄ™dÄ… na maszynie:
```bash
sudo dnf install -y tar openssh-server
sudo systemctl enable sshd --now
```

  - **Nadaj maszynie *hostname* `ansible-target` (najlepiej jeszcze podczas instalacji)**

UstawiÅ‚am hostname poleceniem `sudo hostnamectl set-hostname ansible-target`. NastÄ™pnie zrestartowaÅ‚am system w celu wprowadzenia zmian dziÄ™ki `sudo reboot`

  - **UtwÃ³rz w systemie uÅ¼ytkownika `ansible` (najlepiej jeszcze podczas instalacji)**

StworzyÅ‚am uÅ¼ytkownika i stworzyÅ‚am do niego hasÅ‚o
``` bash
sudo useradd ansible
sudo passwd ansible
```
NadaÅ‚am mu teÅ¼ uprawnienia komendÄ… `sudo usermod -aG wheel ansible`.

  - **ZrÃ³b migawkÄ™ maszyny (i/lub przeprowadÅº jej eksport)**
W Hyper-V w odpowiednim miejscu w ustawieniach kliknÄ™Å‚am stworzeni migawki (punktu kontrolnego) maszyny. 
![image](https://github.com/user-attachments/assets/0a1ff97f-9f44-4d7d-a75e-d1f65f96a325)

ZaÅ› aby wykonaÄ‡ jej eksport musiaÅ‚abym tak jak poniÅ¼ej na wyÅ‚Ä…czonej maszynie kilkajÄ…c prawym przyciskiem na maszynÄ™ wybrac miejsce docelwoe jej eksportowania i mogÅ‚abym siÄ™ wÃ³wczas cieszyÄ‡ zapisem mojej maszyny:

![image](https://github.com/user-attachments/assets/f42364e0-512b-49f5-a73b-abf3c1d259a0)

- [x] **Na gÅ‚Ã³wnej maszynie wirtualnej, zainstaluj oprogramowanie Ansible**

Na maszynie "Fedora" pobraÅ‚am ansible `sudo dnf install ansible -y`

- [x] **WymieÅ„ klucze SSH miÄ™dzy uÅ¼ytkownikiem w gÅ‚Ã³wnej maszynie wirtualnej, a uÅ¼ytkownikiem `ansible` z nowej tak, by logowanie `ssh ansible@ansible-target` nie wymagaÅ‚o podania hasÅ‚a**

SkorzystaÅ‚am z polecenia  `ssh-copy-id ansible@ansible-target`, niestety coÅ› poszÅ‚o nie tak i poÅ‚czÄ…enie wymagaÅ‚o podania hasÅ‚a:

![image](https://github.com/user-attachments/assets/ad564db3-eca1-4644-a6a4-f7e7f3dd8bcb)

Jak widaÄ‡, miaÅ‚am maÅ‚y porblem z wymianÄ… kluczy. WystarczyÅ‚o jednak ruchomiÄ‡ proces `ssh-agent (eval "$(ssh-agent -s)")`, a nastÄ™pnie dodaÅ‚am do niego mÃ³j klucz prywatny (`ssh-add ~/.ssh/id_ed25519`). Agent SSH przechowuje klucz w pamiÄ™ci i automatycznie go udostÄ™pnia przy kaÅ¼dej prÃ³bie poÅ‚Ä…czenia, dziÄ™ki czemu polecenie
`ssh ansible@ansible-target` loguje siÄ™ od razu â€“ bez ponownego pytania o passphrase. Teraz wymiana kluczy miÄ™dzy maszynami przebiegÅ‚a poprawnie:

![image](https://github.com/user-attachments/assets/f5798d6d-f7dc-4047-9d3b-f4ff8a1cfc2a)

SprawdziÅ‚am szybko poÅ‚Ä…czenie, w tym celu w pliku */etc/ansible/hosts* dodaÅ‚am linijkÄ™ 
```bash
[targets]
ansible-target
```
Rezultaty:
![image](https://github.com/user-attachments/assets/1347efd4-0cb9-4ee0-af5d-777b62c4ce67)

### Inwentaryzacja
- [x] **Dokonaj inwentaryzacji systemÃ³w**
  - **Ustal przewidywalne nazwy komputerÃ³w (maszyn wirtualnych) stosujÄ…c `hostnamectl`, Unikaj `localhost`.**

Na maszynie gÅ‚Ã³wnej uruchomiÅ‚am `hostnamectl set-hostname ansible-orchestrator`, a dla maszyny docelowej pozostaÅ‚a nazwa *ansible-target*

![image](https://github.com/user-attachments/assets/b9fa36e8-5019-4a50-8452-477715e198a6)

  - **WprowadÅº nazwy DNS dla maszyn wirtualnych, stosujÄ…c `systemd-resolved` lub `resolv.conf` i `/etc/hosts`**

W pliku /etc/hosts dodaÅ‚am wpisy przypisujÄ…ce statyczne adresy IP do nazw maszyn wirtualnych. DziÄ™ki temu moÅ¼liwe jest odwoÅ‚ywanie siÄ™ do komputerÃ³w za pomocÄ… nazw (np. ansible-orchestrator, ansible-target), zamiast zapamiÄ™tywania adresÃ³w IP. 

![image](https://github.com/user-attachments/assets/9fca6e63-a822-42ab-b534-49a9fad4f84d)

  - **Zweryfikuj Å‚Ä…cznoÅ›Ä‡**

W tym celu poprostu wykonaÅ‚am *ping*:
![image](https://github.com/user-attachments/assets/1c0a4691-841c-48ca-a94c-f74276b3144d)


  - **StwÃ³rz plik inwentaryzacji]**
  - **UmieÅ›Ä‡ w nim sekcje `Orchestrators` oraz `Endpoints`. UmieÅ›Ä‡ nazwy maszyn wirtualnych w odpowiednich sekcjach**

UtowrzyÅ‚am plik hosts.ini (INI to format pliku konfiguracyjnego - nazwa od "initialization")
hosts.ini:
```
[Orchestrators]
ansible-orchestrator ansible_user=kaoina

[Endpoints]
ansible-target ansible_user=ansible

```

  - **WyÅ›lij Å¼Ä…danie `ping` do wszystkich maszyn**
Aby to zrobiÄ‡ wykonaÅ‚am polecenie `ansible all -i hosts.ini -m ping`. Polecenie to wykorzystuje Ansible do wysÅ‚ania Å¼Ä…dania ping do wszystkich hostÃ³w (all) zdefiniowanych w pliku hosts.ini. DziÄ™ki temu mogÅ‚am upewniÄ‡ siÄ™, Å¼e maszyny sÄ… dostÄ™pne przez SSH, poprawnie rozpoznajÄ… siÄ™ po nazwach
![image](https://github.com/user-attachments/assets/ad665af3-a1dc-49a5-8f1f-faf7a26dbf70)

- [x] **Zapewnij Å‚Ä…cznoÅ›Ä‡ miÄ™dzy maszynami**
  - **UÅ¼yj co najmniej dwÃ³ch maszyn wirtualnych**
  - **Dokonaj wymiany kluczy miÄ™dzy maszynÄ…-dyrygentem, a koÅ„cÃ³wkami**

Aby zapewniÄ‡ bezhasÅ‚owÄ… Å‚Ä…cznoÅ›Ä‡ SSH miÄ™dzy maszynÄ…-dyrygentem (ansible-orchestrator) a maszynami docelowymi (ansible-target oraz samym orchestrator), wygenerowaÅ‚am parÄ™ kluczy SSH (jeÅ›li jeszcze nie istniaÅ‚a), a nastÄ™pnie przesÅ‚aÅ‚am klucz publiczny na maszyny docelowe przy uÅ¼yciu polecenia ssh-copy-id. OperacjÄ™ wykonaÅ‚am dla obu maszyn:
```bash
ssh-copy-id kaoina@ansible-orchestrator 
ssh-copy-id ansible@ansible-target
```
  - **Upewnij siÄ™, Å¼e Å‚Ä…cznoÅ›Ä‡ SSH miÄ™dzy maszynami jest moÅ¼liwa i nie potrzebuje haseÅ‚**
Aby upewniÄ‡ siÄ™, Å¼e Å‚Ä…cznoÅ›Ä‡ SSH miÄ™dzy maszynami nie wymaga podawania hasÅ‚a, uruchomiÅ‚am agenta SSH poleceniem `eval "$(ssh-agent -s)"`. NastÄ™pnie dodaÅ‚am klucz prywatny do agenta za pomocÄ… `ssh-add ~/.ssh/id_ed25519`. Po podaniu hasÅ‚a do klucza klucz zostaÅ‚ zaÅ‚adowany i mogÅ‚am poÅ‚Ä…czyÄ‡ siÄ™ z maszynÄ… ansible-target poleceniem ssh ansible@ansible-target bez potrzeby podawania hasÅ‚a do uÅ¼ytkownika.

![image](https://github.com/user-attachments/assets/8096a149-55b3-4263-bd8a-c6cd1e97d930)

### Zdalne wywoÅ‚ywanie procedur
Za pomocÄ… playbooka Ansible:
  - [x] **WyÅ›lij Å¼Ä…danie `ping` do wszystkich maszyn**
  - [x] **Skopiuj plik inwentaryzacji na maszyny/Ä™ `Endpoints`**
  - [x] **PonÃ³w operacjÄ™, porÃ³wnaj rÃ³Å¼nice w wyjÅ›ciu**

ZawartoÅ›Ä‡ playbooka:
```bash
- name: Ping and copy inventory file
  hosts: all
  tasks:
    - name: Ping each host
      ansible.builtin.ping:

    - name: Copy hosts.ini to endpoints
      copy:
        src: ./hosts.ini
        dest: /tmp/hosts.ini
      when: "'ansible-target' in inventory_hostname"
```
WywoÅ‚anie playbooka komendÄ…: ` ansible-playbook -i hosts.ini ping_and_copy.yml `
NastÄ™pnie poÅ‚Ä…czyÅ‚am siÄ™ zdalnie z ansible-target, aby upewniÄ‡ siÄ™, Å¼e plik zostaÅ‚ poprawnie skopiowany do */tmp/hosts.ini*:
![image](https://github.com/user-attachments/assets/bf58eee1-abd2-4d70-b84d-8a888ca375c3)

Pierwsze uruchomienie:
![image](https://github.com/user-attachments/assets/78e005c4-331e-40ab-81c1-4e0fbff07084)

Kolejne uruchomienie:
![image](https://github.com/user-attachments/assets/b162ff46-49c5-4507-a687-2d97534202e5)

Przy kolejnym uruchomieniu:
-	ping dalej dziaÅ‚a
-	copy pokazuje ok zamiast changed
Przy ponownym uruchomieniu playbooka, zadanie ping nadal potwierdza Å‚Ä…cznoÅ›Ä‡ ze wszystkimi maszynami, natomiast zadanie kopiowania pliku zwraca ok zamiast changed, poniewaÅ¼ zawartoÅ›Ä‡ pliku hosts.ini na maszynie docelowej nie ulegÅ‚a zmianie. To pokazuje, Å¼e Ansible nie wykonuje zmian, jeÅ›li nie sÄ… potrzebne.

  - [x] **Zaktualizuj pakiety w systemie**

Aby zaktualizowaÄ‡ wszystkie pakiety na maszynach docelowych, przygotowaÅ‚am prosty playbook Ansible o nazwie update.yml. Playbook ten wykorzystuje moduÅ‚ dnf do bezpiecznej aktualizacji wszystkich pakietÃ³w do najnowszych wersji:
```bash
- name: Update all packages on Endpoints
  hosts: Endpoints
  become: true
  tasks:
    - name: Update all packages (safe way)
      ansible.builtin.dnf:
        name: "*"
        state: latest
        update_cache: yes
```
Playbook uruchomiÅ‚am komendÄ… ` ansible-playbook -i hosts.ini update.yml --ask-become-pass`. Ze wzglÄ™du na to, Å¼e aktualizacja pakietÃ³w wymaga uprawnieÅ„ administratora, musiaÅ‚am dodaÄ‡ opcjÄ™ --ask-become-pass, aby Ansible zapytaÅ‚o o hasÅ‚o do sudo.

![image](https://github.com/user-attachments/assets/c7748060-6cd8-4f54-a0dd-4580c4b13371)

  - [x] **Zrestartuj usÅ‚ugi `sshd` i `rngd`**

Aby zrealizowaÄ‡ zadanie polegajÄ…ce na restarcie usÅ‚ug sshd i rngd na maszynach koÅ„cowych, przygotowaÅ‚am playbook restart.yml, ktÃ³ry wyglÄ…daÅ‚ nastÄ™pujÄ…co:
plik *restart.yml*:
```bash
- name: Restart sshd and rngd services on Endpoints
  hosts: Endpoints
  become: true
  tasks:
    - name: Restart sshd
      ansible.builtin.service:
        name: sshd
        state: restarted

    - name: Restart rngd
      ansible.builtin.service:
        name: rngd
        state: restarted
```

PoczÄ…tkowo uruchomienie playbooka zakoÅ„czyÅ‚o siÄ™ bÅ‚Ä™dem
![image](https://github.com/user-attachments/assets/3a923204-5d2d-44da-89ef-58e68c370200)

UsÅ‚uga rngd nie byÅ‚a dostÄ™pna na systemie ("Could not find the requested service rngd"). SprawdziÅ‚am jej status (`systemctl status rngd`), a nastÄ™pnie zainstalowaÅ‚am brakujÄ…cy pakiet `sudo dnf install rng-tools`

![image](https://github.com/user-attachments/assets/db742689-6452-4ea0-ba85-779cfedbdc6d)

Ponowne wykonanie playbooka zakoÅ„czyÅ‚o siÄ™ peÅ‚nym sukcesem. DziÄ™ki temu oba demony (sshd i rngd) zostaÅ‚y zrestartowane automatycznie z poziomu Ansible.
![image](https://github.com/user-attachments/assets/e8c31f7c-ebab-4335-9741-af94d5f00faf)

  - [x] **PrzeprowadÅº operacje wzglÄ™dem maszyny z wyÅ‚Ä…czonym serwerem SSH, odpiÄ™tÄ… kartÄ… sieciowÄ…**
Po odÅ‚Ä…czeniu ssh na maszynie ansible (` sudo systemctl stop sshd`) otrzymaliÅ›my poniÅ¼szy wynik:

![image](https://github.com/user-attachments/assets/4a59f249-c842-4fe4-ac15-ccb7cc9533c0)

### ZarzÄ…dzanie stworzonym artefaktem
Za pomocÄ… playbooka Ansible:

- [x] **JeÅ¼eli artefaktem z Twojego *pipeline'u* byÅ‚ kontener:
  - **Zbuduj i uruchom kontener sekcji `Deploy` z poprzednich zajÄ™Ä‡**
  - **Pobierz z Docker Hub aplikacjÄ™ "opublikowanÄ…" w ramach kroku `Publish`**
  - **Na maszynie docelowej, **Dockera zainstaluj Ansiblem!**
  - **Zweryfikuj Å‚Ä…cznoÅ›Ä‡ z kontenerem**
  - **Zatrzymaj i usuÅ„ kontener**

W ramach zadania stworzyÅ‚am playbook do zarzÄ…dzania kontenerem Docker, ktÃ³ry jest efektem dziaÅ‚ania pipeline'u. CaÅ‚oÅ›Ä‡ zostaÅ‚a zaimplementowana w postaci roli Ansible. UtworzyÅ‚am strukturÄ™ roli przy pomocy komendy ` ansible-galaxy init docker_artifact`

![image](https://github.com/user-attachments/assets/c56cb3b6-159c-4aa0-b4a9-86e537405131)

W pliku docker_artifact/tasks/main.yml umieÅ›ciÅ‚am zadania odpowiedzialne za:
  * instalacjÄ™ i uruchomienie Dockera
  * pobranie kontenera kaoina666/redis_runtime:2 z Docker Hub
  * jego uruchomienie z odpowiednim przekierowaniem portu
  * sprawdzenie dostÄ™pnoÅ›ci usÅ‚ugi Redis
  * a nastÄ™pnie jego zatrzymanie i usuniÄ™cie

```bash
- name: Install Docker
  ansible.builtin.dnf:
    name: docker
    state: present
  become: true

- name: Enable and start Docker
  ansible.builtin.service:
    name: docker
    state: started
    enabled: true
  become: true

- name: Pull Redis image from Docker Hub
  community.docker.docker_image:
    name: kaoina666/redis_runtime
    tag: "2"
    source: pull

- name: Run Redis container
  community.docker.docker_container:
    name: redis_test
    image: kaoina666/redis_runtime:2
    state: started
    restart_policy: always
    published_ports:
      - "6380:6379"

- name: Wait for Redis to respond on port 6380
  ansible.builtin.wait_for:
    host: "127.0.0.1"
    port: 6380
    timeout: 15

- name: Stop Redis container
  community.docker.docker_container:
    name: redis_test
    state: stopped

- name: Remove Redis container
  community.docker.docker_container:
    name: redis_test
    state: absent
```
Aby dziaÅ‚aÅ‚o, muszÄ… byÄ‡ zainstalowane wszystkie potrzebne kolekcje:
![image](https://github.com/user-attachments/assets/2dfacd4a-2637-48e1-99d3-68bab6b054b4)

Plik requirements.yml:
```bash
collections:
  - name: community.docker
```
CaÅ‚oÅ›Ä‡ uruchomiÅ‚am przez playbook run_artifact.yml komendÄ… ` ansible-playbook -i hosts.ini run_artifact.yml --ask-become-pass`

![image](https://github.com/user-attachments/assets/d2c868f8-b548-40d9-b078-ca5252966640)

Moje run artifact:
```
- name: Manage Redis container artifact
  hosts: Endpoints
  become: true

  roles:
    - docker_artifact
```
Po uruchomieniu playbook wykonaÅ‚ wszystkie zadania poprawnie: Redis zostaÅ‚ pobrany i uruchomiony, a nastÄ™pnie zatrzymany i usuniÄ™ty.
_______________________________________________________________________
## **LAB 9 Pliki odpowiedzi dla wdroÅ¼eÅ„ nienadzorowanych (Kickstart)** 

Tematem laboratorium byÅ‚o przygotowanie automatycznego ÅºrÃ³dÅ‚a instalacji systemu operacyjnego Fedora z wykorzystaniem pliku odpowiedzi Kickstart. Celem byÅ‚o przeprowadzenie nienadzorowanej instalacji systemu oraz konfiguracja Å›rodowiska tak, aby po uruchomieniu system automatycznie uruchamiaÅ‚ nasze oprogramowanie. W ramach zadania naleÅ¼aÅ‚o takÅ¼e zadbaÄ‡ o odpowiednie repozytoria, zaleÅ¼noÅ›ci oraz sposÃ³b pobrania i instalacji aplikacji.

### Zadania do wykonania
PrzeprowadÅº instalacjÄ™ nienadzorowanÄ… systemu Fedora z pliku odpowiedzi z naszego repozytorium
- [x] **Zainstaluj system Fedora**
  * zastosuj instalator sieciowy (*Everything Netinst*) lub
  * zastosuj instalator wariantu *Server* z wbudowanymi pakietami, przyjmujÄ…cy plik odpowiedzi (dobra opcja dla osÃ³b z ograniczeniami transferu internetowego)

- [x] **Pobierz plik odpowiedzi `/root/anaconda-ks.cfg`**

Za pomocÄ… polecenia cat wyciÄ…gnÄ™Å‚am zawartoÅ›Ä‡ pliku *anaconda-ks.cfg* i skopiowaÅ‚am go

![image](https://github.com/user-attachments/assets/cb34927f-e1a9-4e76-8548-76ab9bfd1b82)


- [x] **Zapoznaj siÄ™ z dokumentacjÄ… pliku odpowiedzi i zmodyfikuj swÃ³j plik:**
  - **Plik odpowiedzi moÅ¼e nie zawieraÄ‡ wzmianek na temat potrzebnych repozytoriÃ³w. JeÅ¼eli Twoja pÅ‚yta instalacyjna nie zawiera pakietÃ³w, dodaj wzmiankÄ™ o repozytoriach skÄ…d je pobraÄ‡. Na przykÅ‚ad, dla systemu Fedora 38:
      * `url --mirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=fedora-38&arch=x86_64`
      * `repo --name=update --mirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=updates-released-f38&arch=x86_64`
  - **Plik odpowiedzi moÅ¼e zakÅ‚adaÄ‡ pusty dysk. Zapewnij, Å¼e zawsze bÄ™dzie formatowaÄ‡ caÅ‚oÅ›Ä‡, stosujÄ…c `clearpart --all`**
  - **Ustaw *hostname* inny niÅ¼ domyÅ›lny `localhost`**
  
Na podstawie dokumentacji Kickstarta oraz wymagaÅ„ zadania, wprowadziÅ‚am nastÄ™pujÄ…ce zmiany w pliku anaconda-ks.cfg:

  * Dodanie repozytoriÃ³w internetowych â€“ umoÅ¼liwia pobranie pakietÃ³w:
```bash
url --mirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=fedora-41&arch=x86_64
repo --name=update --mirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=updates-released-f41&arch=x86_64
```
 
  * Ustawienie niestandardowej nazwy hosta
```bash
network --hostname=NowyHostKaoiny
```
(Uwaga: poczÄ…tkowo uÅ¼yÅ‚am Nowy_host_Kaoiny, ale znak _ powodowaÅ‚ bÅ‚Ä…d â€“ zostaÅ‚ usuniÄ™ty.)

  * Dodanie wyczyszczenia wszystkich partycji â€“ zapewnia, Å¼e instalator nadpisze caÅ‚y dysk i nie zostanÄ… Å¼adne dane
``` bash
clearpart --all --initlabel
```

  * Dodanie automatycznego restartu systemu po instalacji:
`reboot`


```bash
# Generated by Anaconda 41.35
# Generated by pykickstart v3.58
#version=DEVEL

# Keyboard layouts
keyboard --vckeymap=pl --xlayouts='pl'
# System language
lang pl_PL.UTF-8

# Repozytoria i ÅºrÃ³dÅ‚o instalacji
url --mirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=fedora-41&arch=x86_64
repo --name=update --mirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=updates-released-f41&arch=x86_64

# Network information
network  --bootproto=dhcp --device=eth0 --ipv6=auto --activate
network  --hostname=Nowy_host_Kaoiny

%packages
@^server-product-environment

%end

# Run the Setup Agent on first boot
firstboot --enable

# Generated using Blivet version 3.11.0
ignoredisk --only-use=sda
# System bootloader configuration
bootloader --location=mbr --boot-drive=sda
# Partition clearing information
clearpart --all --initlabel
# Disk partitioning information
part pv.159 --fstype="lvmpv" --ondisk=sda --size=8500
part /boot --fstype="ext4" --ondisk=sda --size=1024
part /boot/efi --fstype="efi" --ondisk=sda --size=600 --fsoptions="umask=0077,shortname=winnt"
volgroup fedora_kaoinafedora --pesize=4096 pv.159
logvol / --fstype="ext4" --grow --maxsize=71680 --size=1024 --name=root --vgname=fedora_kaoinafedora

# System timezone
timezone Europe/Warsaw --utc

# Root password
rootpw --lock
user --groups=wheel --name=kaoina --password=$y$j9T$Za.bEcfmtiXOZcJwU4ZjeVc3$J1zgPRZWQk29WsCQCxdmuuGHUw3fkISdkP1RJfpqYQ6 --iscrypted --gecos="Kaoina"

reboot
```

- [x] **UÅ¼yj pliku odpowiedzi do przeprowadzenia instalacji nienadzorowanej**
  - **Uruchom nowÄ… maszynÄ™ wirtualnÄ… z pÅ‚yty ISO i wskaÅ¼ instalatorowi przygotowany plik odpowiedzi stosownÄ… dyrektywÄ…**

W celu uruchomienia nienadzorowanej instalacji, przygotowany plik Kickstarta umieÅ›ciÅ‚am w repozytorium GitHub, a nastÄ™pnie, podczas startu maszyny wirtualnej z obrazu ISO Fedory, nacisnÄ™Å‚am klawisz e w menu i dopisaÅ‚am do linii startowej jÄ…dra parametr `inst.ks=<link_do_pliku_ks>`, gdzie <link_do_pliku_ks> wskazywaÅ‚ bezpoÅ›redni link do wersji Raw pliku Kickstarta w repozytorium.

![image](https://github.com/user-attachments/assets/6171688c-1edb-4a48-bc6b-57f841c2b86f)

SkÄ…d link do pliku odpowiedzi(Raw File):

![image](https://github.com/user-attachments/assets/7d857342-bfb3-42a5-aec8-13d8ef6776a7)

BÅ‚ad z powodu znaku "_":C

![image](https://github.com/user-attachments/assets/195cdb53-f057-4808-8f05-42d6e47cffd6)


Instalacja rozpoczÄ™Å‚a siÄ™ automatycznie i zakoÅ„czyÅ‚a sukcesem, bez koniecznoÅ›ci dalszej ingerencji z mojej strony. Maszyna zostaÅ‚a poprawnie skonfigurowana, zainstalowana i zrestartowana â€“ zgodnie z zawartoÅ›ciÄ… przygotowanego pliku .ks.

![image](https://github.com/user-attachments/assets/f92c53fd-6fca-4fcc-9367-5ff64409687e)
![image](https://github.com/user-attachments/assets/33e1a1be-e6e3-4c9b-9618-d72f555779b4)


---
- [x] **Rozszerz plik odpowiedzi o repozytoria i oprogramowanie potrzebne do uruchomienia programu, zbudowanego w ramach projektu - naszego *pipeline'u*.**
  - **W przypadku kontenera, jest to po prostu Docker.
      - **UtwÃ³rz w sekcji `%post` mechanizm umoÅ¼liwiajÄ…cy pobranie i uruchomienie kontenera
      - **JeÅ¼eli efektem pracy pipeline'u nie byÅ‚ kontener, a aplikacja samodzielna - zainstaluj jÄ…
    - **PamiÄ™taj, Å¼e **Docker zadziaÅ‚a dopiero na uruchomionym systemie!** - nie da siÄ™ wdaÄ‡ w interakcjÄ™ z Dockerem z poziomu instalatora systemu: polecenia `docker run` nie powiodÄ… siÄ™ na tym etapie. Nie zadziaÅ‚a teÅ¼ `systemctl start` (ale `systemctl enable` juÅ¼ tak)

W ramach poprzednich laboratoriÃ³w korzystaÅ‚am z kontenera Dockera zawierajÄ…cego Redis. W zwiÄ…zku z tym, aby system po instalacji automatycznie pobieraÅ‚ i uruchamiaÅ‚ ten kontener, wykonaÅ‚am nastÄ™pujÄ…ce modyfikacje w pliku *anaconda*:

  * Dodanie repozytorium Dockera â€“ umoÅ¼liwia instalacjÄ™ Dockera
```bash
repo --name=docker-ce --baseurl=https://download.docker.com/linux/fedora/41/x86_64/stable
```
  * Dodanie pakietÃ³w Dockera w sekcji %packages â€“ wymagane komponenty do uruchamiania kontenerÃ³w
```bash
docker-ce
docker-ce-cli
containerd.io
```
  * Konfiguracja w sekcji %post

    * WÅ‚Ä…czono usÅ‚ugÄ™ Docker przy starcie:
      
    ```bash
    systemctl enable docker
    ```
    
    * Utworzono katalog */opt/scripts* i plik *start-redis.sh*, ktÃ³ry uruchamia kontener Redis:
    
    ```bash
    cat > /opt/scripts/start-redis.sh << 'EOF'
    #!/bin/bash
    sleep 10
    docker run -d --name redis --restart=always -p 6379:6379 kaoina666/redis_runtime:2
    EOF

    chmod +x /opt/scripts/start-redis.sh
    ```
    
    * Utworzono usÅ‚ugÄ™ *systemd start-redis-container.service*, ktÃ³ra uruchamia skrypt:
  
    ```bash
    cat > /etc/systemd/system/start-redis-container.service << 'EOF'
    [Unit]
    Description=Start Redis container from Docker Hub
    After=network-online.target docker.service
    Wants=network-online.target

    [Service]
    Type=oneshot
    ExecStart=/opt/scripts/start-redis.sh
    RemainAfterExit=yes

    [Install]
    WantedBy=multi-user.target
    EOF

    systemctl enable start-redis-container.service
    ```



Zmodyfikowany plik:
```bash
# Generated by Anaconda 41.35
# Generated by pykickstart v3.58
#version=DEVEL

# Keyboard layouts
keyboard --vckeymap=pl --xlayouts='pl'
# System language
lang pl_PL.UTF-8

# Repozytoria i ÅºrÃ³dÅ‚o instalacji
url --mirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=fedora-41&arch=x86_64
repo --name=update --mirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=updates-released-f41&arch=x86_64
repo --name=docker-ce --baseurl=https://download.docker.com/linux/fedora/41/x86_64/stable

# Network information
network  --bootproto=dhcp --device=eth0 --ipv6=auto --activate
network  --hostname=NowyHostKaoiny

%packages
@^server-product-environment
docker-ce
docker-ce-cli
containerd.io
%end

%post --log=/root/ks-post.log
# Docker do uruchamiania przy starcie
systemctl enable docker

# Pobranie i przygotowanie Redisa (wÅ‚asny kontener z Docker Huba)
mkdir -p /opt/scripts

cat > /opt/scripts/start-redis.sh << 'EOF'
#!/bin/bash
# Start dockera
sleep 10
# Uruchom kontener redis z dockerhub
docker run -d --name redis --restart=always -p 6379:6379 kaoina666/redis_runtime:2
EOF

chmod +x /opt/scripts/start-redis.sh

# Jednostka systemd do uruchamiania kontenera przy starcie
cat > /etc/systemd/system/start-redis-container.service << 'EOF'
[Unit]
Description=Start Redis container from Docker Hub
After=network-online.target docker.service
Wants=network-online.target

[Service]
Type=oneshot
ExecStart=/opt/scripts/start-redis.sh
RemainAfterExit=yes

[Install]
WantedBy=multi-user.target
EOF

# Jednostka systemd
systemctl enable start-redis-container.service
%end

# Run the Setup Agent on first boot
firstboot --enable

# Generated using Blivet version 3.11.0
ignoredisk --only-use=sda
# System bootloader configuration
bootloader --location=mbr --boot-drive=sda
# Partition clearing information
clearpart --all --initlabel
# Disk partitioning information
part pv.159 --fstype="lvmpv" --ondisk=sda --size=8500
part /boot --fstype="ext4" --ondisk=sda --size=1024
part /boot/efi --fstype="efi" --ondisk=sda --size=600 --fsoptions="umask=0077,shortname=winnt"
volgroup fedora_kaoinafedora --pesize=4096 pv.159
logvol / --fstype="ext4" --grow --maxsize=71680 --size=1024 --name=root --vgname=fedora_kaoinafedora

# System timezone
timezone Europe/Warsaw --utc

# Root password
rootpw --lock
user --groups=wheel --name=kaoina --password=$y$j9T$Za.bEcfmtiXOZcJwU4ZjeVc3$J1zgPRZWQk29WsCQCxdmuuGHUw3fkISdkP1RJfpqYQ6 --iscrypted --gecos="Kaoina"

reboot
```

Uruchamianie i konfiguracja maszyny:

![image](https://github.com/user-attachments/assets/0f83c469-8c14-4d7e-8474-c79c4c8adf37)


- [x] **Zadbaj o automatyczne ponowne uruchomienie na koÅ„cu instalacji**

Za automatyczny restart systemu po zakoÅ„czeniu instalacji odpowiada ostatnia linijka w pliku: `reboot`

- [x] **Zapewnij, by od razu po pierwszym uruchomieniu systemu, oprogramowanie zostaÅ‚o uruchomione (w dowolny sposÃ³b)**

1. Czy Docker dziaÅ‚a?
`systemctl status docker`
Docker.service ma status active (running)
![image](https://github.com/user-attachments/assets/f7d7b1ec-a3eb-4768-af4e-ab4bae495930)

2. Czy kontener Redis dziaÅ‚a?
`docker ps`
Widoczny kontener Redis z STATUS: Up 
![image](https://github.com/user-attachments/assets/c6712ff9-bff4-46a5-b805-66f7ee5ae659)

3. Czy Redis nasÅ‚uchuje?
`ss -lntp | grep 6379`
Widoczny proces nasÅ‚uchujÄ…cy na porcie 6379.
![image](https://github.com/user-attachments/assets/944a4a2c-6e54-496a-b969-e96712b87f9b)

4. Czy usÅ‚uga systemd dziaÅ‚a?
`systemctl status start-redis-container.service`
UsÅ‚uga start-redis-container.service dziaÅ‚aÅ‚a i zakoÅ„czyÅ‚a siÄ™ sukcesem:
![image](https://github.com/user-attachments/assets/f8fc7bc9-adc4-4801-a4ae-1941ea5247d0)

_________________________________________________________________
## **LAB 10 WdraÅ¼anie na zarzÄ…dzalne kontenery: Kubernetes (1)**

Celem tych laboratoriÃ³w byÅ‚o zapoznanie siÄ™ z podstawami dziaÅ‚ania Kubernetesa oraz nauczenie siÄ™, jak uruchamiaÄ‡ i zarzÄ…dzaÄ‡ aplikacjami kontenerowymi w lokalnym klastrze za pomocÄ… Minikube. W ramach Ä‡wiczeÅ„ zainstalowaÅ‚am Minikube, uruchomiÅ‚am Dashboard, wdroÅ¼yÅ‚am wÅ‚asny kontener oraz przygotowaÅ‚am plik YAML opisujÄ…cy deployment z replikacjÄ…. 
     
### Instalacja klastra Kubernetes
- [x] **Zaopatrz siÄ™ w implementacjÄ™ stosu k8s: minikube**

ZainstalowaÅ‚am Minikube, korzystajÄ…c z wersji RPM package, zgodnie z instrukcjÄ… na stronie

![image](https://github.com/user-attachments/assets/450d2659-a3fa-4d25-b80c-fb995d861c5f)

- [x] **PrzeprowadÅº instalacjÄ™, wykaÅ¼ poziom bezpieczeÅ„stwa instalacji**

Aby zainstalowaÄ‡ Minikube, wykonaÅ‚am nastÄ™pujÄ…ce komendy:

```bash
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-latest.x86_64.rpm
sudo rpm -Uvh minikube-latest.x86_64.rpm
```

![image](https://github.com/user-attachments/assets/226e2b91-e9ec-48f2-980a-fca29f9c259a)

Po instalacji, aby sprawdziÄ‡ stan klastra, uÅ¼yÅ‚am komend:
``` bash
minikube kubectl -- get pods -A -o wide
```
Ta komenda pozwala uzyskaÄ‡ szczegÃ³Å‚owe informacje o podach systemowych. Na podstawie wynikÃ³w mogÅ‚am zweryfikowaÄ‡ poprawnoÅ›Ä‡ dziaÅ‚ania aplikacji na klastrze.
![image](https://github.com/user-attachments/assets/3d61c98b-e8b6-4223-abba-4c1b46f0214b)

Ponadto, aby uzyskaÄ‡ dodatkowe informacje o poszczegÃ³lnych podach, skorzystaÅ‚am z komendy:
```bash
minikube kubectl -- describe pod <nazwa> -n kube-system
```
Wynik tej komendy pozwoliÅ‚ mi upewniÄ‡ siÄ™, Å¼e wszystkie pody dziaÅ‚ajÄ… zgodnie z oczekiwaniami.
![image](https://github.com/user-attachments/assets/29ae13b2-ba75-4b09-8b63-41606c6d5f95)


- [x] **Zaopatrz siÄ™ w polecenie `kubectl` w wariancie minikube, moÅ¼e byÄ‡ alias `minikubctl`, jeÅ¼eli masz juÅ¼ "prawdziwy" `kubectl`**

Zgodnie z instruckjÄ… na stornie 
![image](https://github.com/user-attachments/assets/175523b4-6e8c-404b-b43a-ebdb1dc95872)

UruchomiÅ‚am dwie poniÅ¼sze komendy aby uÅ¼ywaÄ‡ narzÄ™dzia kubectl
```bash
minikube kubectl -- get po -A
alias kubectl="minikube kubectl --"
```

![image](https://github.com/user-attachments/assets/b99989cb-7ccf-42ca-8ce9-e581879ef064)

- [x] **Uruchom Kubernetes, pokaÅ¼ dziaÅ‚ajÄ…cy kontener/worker**

Aby uruchomiÄ‡ klaster Kubernetes, uÅ¼yÅ‚am komendy
```minikube start```

![image](https://github.com/user-attachments/assets/627e5c21-b6ed-4b1c-8b8d-1401f539dc91)

Po uruchomieniu klastra Minikube sprawdziÅ‚am jego stan komendÄ… `minikube kubectl -- get nodes` 

![image](https://github.com/user-attachments/assets/5b60c2a4-8442-410c-ad8b-7181b952a1a1)

Wynik pokazuje, Å¼e wÄ™zeÅ‚ o nazwie minikube ma status Ready, peÅ‚ni rolÄ™ control-plane i dziaÅ‚a od kilku minut, co potwierdza poprawne uruchomienie klastra Kubernetes. NastÄ™pnie sprawdziÅ‚am dziaÅ‚ajÄ…ce pody systemowe w przestrzeni nazw kube-system, korzystajÄ…c z polecenia `minikube kubectl -- get pods -A`.

![image](https://github.com/user-attachments/assets/841c83ab-fa24-4ce5-ac4c-aed92b0cd0e5)

Wszystkie pody sÄ… w stanie Running, co oznacza, Å¼e komponenty systemowe Kubernetesa dziaÅ‚ajÄ… poprawnie.

- [x] **Zmityguj problemy wynikajÄ…ce z wymagaÅ„ sprzÄ™towych lub odnieÅ› siÄ™ do nich (wzglÄ™dem dokumentacji)**

Podczas uruchamiania klastra Minikube napotkaÅ‚am na problem zwiÄ…zany z pamiÄ™ciÄ… RAM. Minikube domyÅ›lnie przydziela okreÅ›lonÄ… iloÅ›Ä‡ pamiÄ™ci wirtualnej maszynie, co moÅ¼e byÄ‡ niewystarczajÄ…ce w przypadku komputerÃ³w o ograniczonych zasobach. Aby rozwiÄ…zaÄ‡ ten problem, dostosowaÅ‚am iloÅ›Ä‡ przydzielonej pamiÄ™ci, wykorzystujÄ…c opcjÄ™ --memory podczas uruchamiania klastra. Na przykÅ‚ad `minikube start --memory 4096`

- [x] **Uruchom Dashboard, otwÃ³rz w przeglÄ…darce, przedstaw Å‚Ä…cznoÅ›Ä‡**

UruchomiÅ‚am Dashbord poleceniem `minikube dashboard`, co umoÅ¼liwiÅ‚o mi teÅ¼ zarzÄ…dzanie prze zinterface webowy.
![image](https://github.com/user-attachments/assets/0ccd28ea-92bd-4615-a372-ba7cef1437e9)

- [x] **Zapoznaj siÄ™ z koncepcjami funkcji wyprowadzanych przez Kubernetesa (*pod*, *deployment* itp)**

  * Pod to najmniejsza jednostka w Kubernetes, ktÃ³ra moÅ¼e zawieraÄ‡ jeden lub wiÄ™cej kontenerÃ³w, dziaÅ‚ajÄ…cych w tym samym Å›rodowisku.
  * Deployment umoÅ¼liwia zarzÄ…dzanie aplikacjami w Kubernetes, zapewniajÄ…c skalowanie i aktualizowanie kontenerÃ³w w sposÃ³b automatyczny.
  * Service pozwala na tworzenie staÅ‚ych punktÃ³w dostÄ™pu do aplikacji dziaÅ‚ajÄ…cych w klastrze, umoÅ¼liwiajÄ…c komunikacjÄ™ z kontenerami.

 
### Analiza posiadanego kontenera
- [x] **Zdefiniuj krok "Deploy" swojego projektu jako "Deploy to cloud":**
  - **Deploy zbudowanej aplikacji powinien siÄ™ odbywaÄ‡ "na kontener"**
  - **Przygotuj obraz Docker ze swojÄ… aplikacjÄ… - sprawdÅº, Å¼e TwÃ³j kontener Deploy na pewno **pracuje**, a nie natychmiast koÅ„czy pracÄ™! ğŸ˜**
  - **WykaÅ¼, Å¼e wybrana aplikacja pracuje jako kontener

Ja miaÅ‚am juÅ¼ gotowy ten etap dzieki wczeÅ›niejszym laboratoriom. Kontener z aplikacjÄ… Redis miaÅ‚am juÅ¼ zdeployowany na DockerHubie (przygotowany obraz Docker kaoina666/redis_runtime:2).

### Uruchamianie oprogramowania
- [x] **Uruchom kontener ze swojÄ…/wybranÄ… aplikacjÄ… na stosie k8s**
- [x] **Kontener uruchomiony w minikubie zostanie automatycznie "ubrany" w *pod*.**
- [x] **```minikube kubectl run -- <nazwa-jednopodowego-wdroÅ¼enia> --image=<obraz-docker> --port=<wyprowadzany port> --labels app=<nazwa-jednopodowego-wdroÅ¼enia>```**

Uruchomienie za pomocÄ… `kubectl run redis-deployment --image=kaoina666/redis_runtime:2 --port=6379 --labels app=redis-deployment`. Kubectl run sÅ‚uÅ¼y do uruchamiania pojedynczego poda z okreÅ›lonym obrazem Docker oraz konfiguracjÄ… portÃ³w i etykiet. 

![image](https://github.com/user-attachments/assets/e2c0048a-29a1-46ec-9f38-ab0f6533e41d)

- [x] **Przedstaw Å¼e *pod* dziaÅ‚a (via Dashboard oraz `kubectl`)**

dziÄ™ki `kubectl get pods` moÅ¼emy w terminalu zoabczyÄ‡, Å¼e *redis-deployment* jest aktywnym podem
![image](https://github.com/user-attachments/assets/f2271ddc-af73-4cd9-86a6-e1c6b7aa4843)

rÃ³wnieÅ¼ w webowej aplikacji widoczny bÄ™dzie nasz pod

![image](https://github.com/user-attachments/assets/4a631cd5-21d4-4a1c-bfc6-89e930523f74)

  
- [x] **WyprowadÅº port celem dotarcia do eksponowanej funkcjonalnoÅ›ci**
- [x] **```kubectl port-forward pod/<nazwa-wdroÅ¼enia> <LO_PORT>:<PODMAIN_CNTNR_PORT> ```**

`kubectl port-forward pod/redis-deployment 6379:6379`
Komenda ta przekierowuje port 6379 z kontenera Redis (uruchomionego w podzie o nazwie redis-deployment) na lokalny port 6379 na maszynie. Przekierowanie portÃ³w pozwala na dostÄ™p do usÅ‚ugi Redis, dziaÅ‚ajÄ…cej wewnÄ…trz klastra Kubernetes, poprzez lokalny port na komputerze uÅ¼ytkownika.

Po wykonaniu komendy, terminal zwrÃ³ciÅ‚ komunikat: 
![image](https://github.com/user-attachments/assets/48f9516a-2db5-4338-b6c3-f24a0c38b08c)

- [x] **Przedstaw komunikacjÄ™ z eskponowanÄ… funkcjonalnoÅ›ciÄ…**
W celu weryfikacji poprawnoÅ›ci dziaÅ‚ania, uÅ¼yto klienta Redis, Å‚Ä…czÄ…c siÄ™ za pomocÄ… nastÄ™pujÄ…cej komendy: `redis-cli -h 127.0.0.1 -p 6379`.

![image](https://github.com/user-attachments/assets/25fb98d4-8e6e-4ca9-83e5-3f4247a5212e)

Port forwarding dziaÅ‚a poprawnie, a usÅ‚uga Redis jest dostÄ™pna lokalnie na porcie 6379.


### Przekucie wdroÅ¼enia manualnego w plik wdroÅ¼enia (wprowadzenie)
- [x] **Zapisz wdroÅ¼enie jako plik YML**
- [x] **PrzeprowadÅº prÃ³bne wdroÅ¼enie przykÅ‚adowego *deploymentu***
  - **Wykonaj ```kubectl apply``` na pliku**
  - **Upewnij siÄ™, Å¼e posiadasz wdroÅ¼enie zapisane jako plik**
  - **WzbogaÄ‡ swÃ³j *deployment* o 4 repliki**
  - **Rozpocznij wdroÅ¼enie za pomocÄ… ```kubectl apply```**
  - **Zbadaj stan za pomocÄ… ```kubectl rollout status```**
- [x] **Wyeksponuj wdroÅ¼enie jako serwis**
- [x] **Przekieruj port do serwisu (tak, jak powyÅ¼ej)**

W ramach tego zadania stworzyÅ‚am plik redis.yaml, ktÃ³ry umoÅ¼liwia wdroÅ¼enie aplikacji Redis w Kubernetes. Celem tego wdroÅ¼enia byÅ‚o utworzenie instancji Redis, ktÃ³rÄ… moÅ¼na bÄ™dzie wystawiÄ‡ na zewnÄ…trz klastra. Aby uruchomiÄ‡ to wdroÅ¼enie, uÅ¼yÅ‚am komendy kubectl apply -f redis.yaml.

Oto zawartoÅ›Ä‡ pliku redis.yaml:

```bash
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-deployment
spec:
  replicas: 4
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - name: redis
        image: kaoina666/redis_runtime:2
        ports:
        - containerPort: 6379
---
apiVersion: v1
kind: Service
metadata:
  name: redis-service
spec:
  selector:
    app: redis
  type: NodePort
  ports:
    - port: 6379
      targetPort: 6379
      nodePort: 30079
```

Po uruchomieniu tego pliku utworzyÅ‚y siÄ™ cztery pody, co mogÅ‚am zweryfikowaÄ‡, sprawdzajÄ…c status wdroÅ¼enia (`kubectl get pods`)
![image](https://github.com/user-attachments/assets/3ce1a97d-ae98-4f94-b245-5d97509009c0)

Aby upewniÄ‡ siÄ™, Å¼e wdroÅ¼enie poszÅ‚o pomyÅ›lnie, uÅ¼yÅ‚am komendy `kubectl rollout status deployment/redis-deployment`. Zgodnie z oczekiwaniami, proces zakoÅ„czyÅ‚ siÄ™ sukcesem, a wszystkie repliki zostaÅ‚y uruchomione:
![image](https://github.com/user-attachments/assets/3950cd09-ecf5-4d30-9214-5e992918f858)

Na koÅ„cu, zgodnie z planem, przekierowaÅ‚am port do serwisu, uÅ¼ywajÄ…c komendy `kubectl port-forward svc/redis-service 6379:6379`. Aby sprawdziÄ‡, czy wszystko dziaÅ‚a, poÅ‚Ä…czyÅ‚am siÄ™ z Redisem, uÅ¼ywajÄ…c *redis-cli* komendÄ… `redis-cli -h localhost -p 6379`. Rezultat:

![image](https://github.com/user-attachments/assets/7fb51887-a4fa-433c-847b-fe4febdb1bb9)

_________________________________________________________________
## **LAB 11 WdraÅ¼anie na zarzÄ…dzalne kontenery: Kubernetes (2)**

Celem tych laboratoriÃ³w byÅ‚o przygotowanie i zarzÄ…dzanie wersjami obrazÃ³w kontenerÃ³w w Dockerze oraz wdraÅ¼anie w Å›rodowisku Kubernetes. W trakcie zajÄ™Ä‡ skoncentrowaÅ‚am siÄ™ na aktualizowaniu, skalowaniu oraz przywracaniu poprzednich wersji aplikacji, a takÅ¼e na monitorowaniu wdroÅ¼eÅ„ przy uÅ¼yciu rÃ³Å¼nych strategii, takich jak Recreate, Rolling Update czy Canary Deployment. Dodatkowo, zapoznaÅ‚am siÄ™ z automatyzacjÄ… procesÃ³w za pomocÄ… skryptÃ³w.

### Przygotowanie nowego obrazu
- [x] **Zarejestruj nowÄ… wersjÄ™ swojego obrazu `Deploy`**
- [x] **Upewnij siÄ™, Å¼e dostÄ™pne sÄ… dwie co najmniej wersje obrazu z wybranym programem**
- [x] **BÄ™dzie to wymagaÄ‡**
  - **przejÅ›cia przez *pipeline* dwukrotnie**

Aby uzyskaÄ‡ nowÄ… wersjÄ™ mojego obrazu skorzystaÅ‚am z mojego gotowego pipeline w Jenkinsie. OdpaliÅ‚am go po raz drugi i uzyskaÅ‚am dziÄ™ki temu na DockerHubie oczekiwany efekt:

![image](https://github.com/user-attachments/assets/76a4c835-a79e-4615-b84e-e2cc4d60331f)

### Zmiany w deploymencie
- [x] **Aktualizuj plik YAML z wdroÅ¼eniem i przeprowadzaj je ponownie po zastosowaniu nastÄ™pujÄ…cych zmian:**
  - **zwiÄ™kszenie replik np. do 8**
  - **zmniejszenie liczby replik do 1**
  - **zmniejszenie liczby replik do 0**
  - **ponowne przeskalowanie w gÃ³rÄ™ do 4 replik (co najmniej)**
  - **Zastosowanie nowej wersji obrazu**
  - **Zastosowanie starszej wersji obrazu**
  - **Zastosowanie "wadliwego" obrazu**
- [x] **Przywracaj poprzednie wersje wdroÅ¼eÅ„ za pomocÄ… poleceÅ„**
  - **```kubectl rollout history```**
  - **```kubectl rollout undo```**
     
Aby wykonaÄ‡ tÄ… czeÅ›Ä‡ Ä‡wiczenia korzystaÅ‚am ze stworoznego przeze mnie pliku *redis-deploy.yaml*:

```bash
# redis-deploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - name: redis-container
        image: kaoina666/redis_runtime:3 #zmiana z 2
        ports:
        - containerPort: 6379
```

W zarzÄ…dzaniu zmianami pomagaÅ‚y komendy: 
`kubectl rollout undo deployment redis-app` - cofniÄ™cie do poprzedniej wersji	
`kubectl rollout history deployment redis-app` - pokazuje historiÄ™ rewizji przed rollbackiem, do Å›ledzenia zmian

![image](https://github.com/user-attachments/assets/3042b1a6-7d24-4faf-ad43-87c4692813d9)

Jednym z zadaÅ„ byÅ‚a kilkukrotna zmiana iloÅ›ci replik naszej apliakcji. Aby zwiÄ™kszyÄ‡ bÄ…dÅº zmniejszyÄ‡ iloÅ›Ä‡ replik naleÅ¼aÅ‚o zmieniaÄ‡ parametr ` replicas: x` - w miejsce x wstawiaÅ‚am odpowiedniÄ… iloÅ›Ä‡ replik jakÄ… naleÅ¼aÅ‚o uzyskaÄ‡, a nastÄ™pnie aplikowaÅ‚am zmiany przez `kubectl apply -f redis-deploy.yaml`. IloÅ›Ä‡ podÃ³w redis-app (ignorujmy te z 10 dni temu) sprawdzaÅ‚am znanym dobrze `kubectl get pods`.

8 podÃ³w:
![image](https://github.com/user-attachments/assets/335fcbe6-0235-4b24-97cc-987596eaa8c1)

1 pod:
![image](https://github.com/user-attachments/assets/b4ed9eb1-5930-46b1-8311-677bee673a2f)

0 podÃ³w: 
![image](https://github.com/user-attachments/assets/58263ac8-a2ad-42ea-9540-38a9a2bd11f6)

4 pody:
![image](https://github.com/user-attachments/assets/9a2d24d6-2b1d-405c-871c-77bf2159d0d0)

Kolejnym z zadaÅ„ byÅ‚a zmiana wersji naszego obrazu, wystarczyÅ‚o podmieniÄ‡ tag z `2` na `3`. Efekty byÅ‚y dobrze zauwaÅ¼alne np w dashbordzie:

![image](https://github.com/user-attachments/assets/647f5fb4-d896-4cca-b971-847a8b97d35f)


Aby cofnÄ…Ä‡ spowrotem do wersji tagu `2` cofnÄ™Å‚am siÄ™ poleceniem `kubectl rollout undo deployment redis-app`, i wszystko jakby cofnÄ™Å‚o siÄ™ w czasie, co odnotowaÅ‚am w dashbordzie jak poniÅ¼ej:
![image](https://github.com/user-attachments/assets/49b8f376-60ab-4230-ae18-1216fcc10d39)
![image](https://github.com/user-attachments/assets/82d014d4-aae5-4a92-902d-914b5e218e8c)

Kolejnym zadaniem byÅ‚a prÃ³ba zastosowania wadliwego obrazu. w tym celu w pliku wdroÅ¼eniowym zamiast poprawnego tagu wybraÅ‚am taki ktÃ³ry nie istnieje (w moim przypadku `:bad`).

![image](https://github.com/user-attachments/assets/424baf2b-763c-4bb9-84f9-575874681cac)

Jak widaÄ‡ odrazu po zmianie barw, wystÄ…piÅ‚y bÅ‚Ä™dy. Kubernetes sprÃ³bowaÅ‚ wdroÅ¼yÄ‡ nowÄ… wersjÄ™, ale rollout siÄ™ nie powiÃ³dÅ‚. W takich przypadkach wÅ‚aÅ›nie znowu przydaje siÄ™ â€˜kubectl rollout undo deployment redis-appâ€™, dziÄ™ki czemu wszsytko spowrotem dziaÅ‚a:

![image](https://github.com/user-attachments/assets/9e9d443c-aa92-449d-8639-d69c73abcf27)
![image](https://github.com/user-attachments/assets/0d63cd3e-c1b3-42ba-9413-b4f72d4555a7)



### Kontrola wdroÅ¼enia
- [x] **Zidentyfikuj historiÄ™ wdroÅ¼enia i zapisane w niej problemy, skoreluj je z wykonywanymi czynnoÅ›ciami**

Poleceniem `kubectl describe deployment redis-app` mogÅ‚am przejrzeÄ‡ historiÄ™ wdroÅ¼enia.

![image](https://github.com/user-attachments/assets/909680fa-d735-49b1-ad54-9abefe9fa886)

WidaÄ‡, Å¼e obecna wersja (:2) dziaÅ‚a poprawnie, wszystkie 4 Pody sÄ… gotowe . ByÅ‚y rÃ³wnieÅ¼ przynajmniej 3 rÃ³Å¼ne zestawy replik (czyli zmieniany byÅ‚ obraz lub konfiguracja). W zakÅ‚adce events widzimy jak wyglÄ…daÅ‚y momenty skalowania np. z 3 â†’ 8 â†’ 0 â†’ 4 

- [x] **Napisz skrypt weryfikujÄ…cy, czy wdroÅ¼enie "zdÄ…Å¼yÅ‚o" siÄ™ wdroÅ¼yÄ‡ (60 sekund)**

UtworzyÅ‚am skrypt check_deploy.sh, ktÃ³ry automatycznie weryfikuje, czy wdroÅ¼enie aplikacji redis-app w Kubernetesie zostaÅ‚o zakoÅ„czone pomyÅ›lnie w okreÅ›lonym czasie (60 sekund). Skrypt monitoruje liczbÄ™ gotowych replik aplikacji, porÃ³wnujÄ…c jÄ… z liczbÄ… replik okreÅ›lonÄ… w specyfikacji wdroÅ¼enia. JeÅ›li liczba dostÄ™pnych replik rÃ³wna siÄ™ liczbie replik poÅ¼Ä…danych i jest rÃ³Å¼na od pustej wartoÅ›ci, skrypt informuje o sukcesie wdroÅ¼enia. JeÅ›li po upÅ‚ywie 60 sekund wdroÅ¼enie nadal nie jest gotowe, skrypt wypisuje komunikat o przekroczeniu czasu oczekiwania.

Przed uruchomieniem `./check_deploy.sh` nadaÅ‚am prawa `chmod +x check_deploy.sh`.

``` bash
#!/bin/bash

DEPLOYMENT_NAME="redis-app"
NAMESPACE="default"
TIMEOUT=60

echo "Sprawdzam wdroÅ¼enie: $DEPLOYMENT_NAME (maksymalnie ${TIMEOUT}s)..."

for ((i=1;i<=TIMEOUT;i++)); do
  READY=$(minikube kubectl -- get deploy $DEPLOYMENT_NAME -n $NAMESPACE -o jsonpath='{.status.readyReplicas}')
  DESIRED=$(minikube kubectl -- get deploy $DEPLOYMENT_NAME -n $NAMESPACE -o jsonpath='{.spec.replicas}')
  
  if [[ "$READY" == "$DESIRED" && "$READY" != "" ]]; then
    echo "âœ… WdroÅ¼enie zakoÅ„czone sukcesem: $READY/$DESIRED replik gotowych."
    exit 0
  fi

  echo "â³ [$i/$TIMEOUT] Czekam... ($READY/$DESIRED gotowych)"
  sleep 1
done

echo "âŒ Timeout: wdroÅ¼enie nie zakoÅ„czone w $TIMEOUT sekund."
exit 1
```

DziaÅ‚anie (W trakcie testu wdroÅ¼enie zakoÅ„czyÅ‚o siÄ™ sukcesem w czasie poniÅ¼ej 60 sekund.):

![image](https://github.com/user-attachments/assets/50c9e204-32ab-4a9d-9655-2c669be3a102)

 
### Strategie wdroÅ¼enia
- [x] **Przygotuj wersje wdroÅ¼eÅ„ stosujÄ…ce nastÄ™pujÄ…ce strategie wdroÅ¼eÅ„**
  - **Recreate**
  - **Rolling Update (z parametrami `maxUnavailable` > 1, `maxSurge` > 20%)**
  - **Canary Deployment workload**
- [x] **Zaobserwuj i opisz rÃ³Å¼nice**
- [x] **Uzyj etykiet**

1. Recreate strategy - wyÅ‚Ä…cza wszystkie stare Pody, dopiero potem uruchamia nowe (na raz). W rollingUpdate stare pody sÄ… usuwane i zasÄ™powane **stopniowo** Aby to sprawdziÄ‡ najpierw uruchomiÅ‚am plik z wersjÄ… tagu 2, a nastÄ™pnie 3. Po zmianie wersji obrazu wszystkie stare Pody zostaÅ‚y usuniÄ™te i dopiero wtedy zostaÅ‚y utworzone nowe. W dashboardzie obserwujemy tylko Pody z nowym obrazem, co potwierdza, Å¼e wdroÅ¼enie byÅ‚o "czyste", bez rÃ³wnolegÅ‚ego dziaÅ‚ania starych i nowych instancji. Strategia Recreate powoduje chwilowÄ… niedostÄ™pnoÅ›Ä‡ usÅ‚ugi, ale wdroÅ¼enie jest proste i szybkie.

Dla 2: 

![image](https://github.com/user-attachments/assets/495932e9-4dcb-4405-9212-28b8ee699780)

Dla 3:

![image](https://github.com/user-attachments/assets/c783f4d3-0a75-4f15-b976-6464d68dfa0b)


redis-recreate.yaml:
```bash
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-recreate
spec:
  replicas: 3
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: redis-recreate
  template:
    metadata:
      labels:
        app: redis-recreate
    spec:
      containers:
      - name: redis
        image: kaoina666/redis_runtime:3
        ports:
        - containerPort: 6379
```

2. RollingUpdate z parametrami (maxUnavailable: 2, maxSurge: 50%) - stopniowo podmienia Pody, z wyraÅºnymi ustawieniami, ile moÅ¼na mieÄ‡ niedostÄ™pnych lub nadmiarowych jednoczeÅ›nie

Uruchomienie:
![image](https://github.com/user-attachments/assets/4cfc2810-b9a5-4888-a72a-7206108f1d28)

Podczas zmiany wersji obrazu wszystkie Pody byÅ‚y podmieniane stopniowo. Obserwacja kubectl get pods -w oraz dashboardu potwierdziÅ‚a, Å¼e Å¼adne Pody nie byÅ‚y jednoczeÅ›nie w stanie niedostÄ™pnoÅ›ci (Unavailable = 0). WdroÅ¼enie zakoÅ„czyÅ‚o siÄ™ sukcesem, a aplikacja nie byÅ‚a niedostÄ™pna ani przez moment.

redis-rolling.yaml:
```bash
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-rolling
spec:
  replicas: 4
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 2
      maxSurge: 50%
  selector:
    matchLabels:
      app: redis-rolling
  template:
    metadata:
      labels:
        app: redis-rolling
    spec:
      containers:
      - name: redis
        image: kaoina666/redis_runtime:2
        ports:
        - containerPort: 6379
```

3. Canary Deployment - uruchamia 2 rÃ³wnolegÅ‚e wersje aplikacji (np. 90% stabilna, 10% testowa)

W celu przetestowania stworzyam aÅ¼ 3 pliki yaml:
redis-canary-stable.yaml
```bash
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-stable
spec:
  replicas: 3
  selector:
    matchLabels:
      app: redis
      version: stable
  template:
    metadata:
      labels:
        app: redis
        version: stable
    spec:
      containers:
      - name: redis
        image: kaoina666/redis_runtime:2
        ports:
        - containerPort: 6379
```


redis-canary-experimental.yaml
```bash
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-canary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      version: canary
  template:
    metadata:
      labels:
        app: redis
        version: canary
    spec:
      containers:
      - name: redis
        image: kaoina666/redis_runtime:3
        ports:
        - containerPort: 6379

```

redis-service.yaml
```
apiVersion: v1
kind: Service
metadata:
  name: redis-service
spec:
  selector:
    app: redis
  ports:
  - protocol: TCP
    port: 6379
    targetPort: 6379
```

WdroÅ¼yÅ‚am Canary Deployment poprzez utworzenie dwÃ³ch niezaleÅ¼nych DeploymentÃ³w: redis-stable (obraz :2, 3 repliki) oraz redis-canary (obraz :3, 1 replika). Oba posiadajÄ… wspÃ³lnÄ… etykietÄ™ app=redis, co umoÅ¼liwia poÅ‚Ä…czenie ich pod wspÃ³lnym Service. DziÄ™ki temu 75% ruchu trafia do stabilnej wersji, a 25% do wersji eksperymentalnej. Pozwala to na testowanie nowej wersji aplikacji w Å›rodowisku produkcyjnym z minimalnym ryzykiem.

Efekty:
![image](https://github.com/user-attachments/assets/82f23937-a17c-45bf-b1fa-30b93b3c3ddf)




