# **Sprawozdanie 3** - Metodyki DevOps
_________________________________________________________________________________________________________________________________________________________
## **LAB 8 - Automatyzacja i zdalne wykonywanie poleceÅ„ za pomocÄ… Ansible** 

Celem Ä‡wiczeÅ„ byÅ‚o przygotowanie pliku odpowiedzi i wykorzystanie go do przeprowadzenia nienadzorowanej instalacji systemu Fedora. Podczas zajÄ™Ä‡ skonfigurowaÅ‚am instalator tak, aby system po uruchomieniu automatycznie zawieraÅ‚ wymagane repozytoria, zaleÅ¼noÅ›ci oraz uruchamiaÅ‚ moje oprogramowanie. DziÄ™ki temu udaÅ‚o siÄ™ w peÅ‚ni zautomatyzowaÄ‡ proces instalacji i wdroÅ¼enia Å›rodowiska testowego.

### Instalacja zarzÄ…dcy Ansible
- [x] **UtwÃ³rz drugÄ… maszynÄ™ wirtualnÄ… o **jak najmniejszym** zbiorze zainstalowanego oprogramowania**
  - **Zastosuj ten sam system operacyjny, co "gÅ‚Ã³wna" maszyna (najlepiej teÅ¼ w tej samej wersji)**
  - **Zapewnij obecnoÅ›Ä‡ programu `tar` i serwera OpenSSH (`sshd`)**

UÅ¼yÅ‚am poniÅ¼szych poleceÅ„ Å¼eby siÄ™ upeniÄ‡ Å¼e tar i sshd bÄ™dÄ… na maszynie:
```bash
sudo dnf install -y tar openssh-server
sudo systemctl enable sshd --now
```

  - **Nadaj maszynie *hostname* `ansible-target` (najlepiej jeszcze podczas instalacji)**

UstawiÅ‚am hostname poleceniem `sudo hostnamectl set-hostname ansible-target`. NastÄ™pnie zrestartowaÅ‚am system w celu wprowadzenia zmian dziÄ™ki `sudo reboot`

  - **UtwÃ³rz w systemie uÅ¼ytkownika `ansible` (najlepiej jeszcze podczas instalacji)**

StworzyÅ‚am uÅ¼ytkownika i stworzyÅ‚am do niego hasÅ‚o
``` bash
sudo useradd ansible
sudo passwd ansible
```
NadaÅ‚am mu teÅ¼ uprawnienia komendÄ… `sudo usermod -aG wheel ansible`.

  - **ZrÃ³b migawkÄ™ maszyny (i/lub przeprowadÅº jej eksport)**
W Hyper-V w odpowiednim miejscu w ustawieniach kliknÄ™Å‚am stworzeni migawki (punktu kontrolnego) maszyny. 
![image](https://github.com/user-attachments/assets/0a1ff97f-9f44-4d7d-a75e-d1f65f96a325)

ZaÅ› aby wykonaÄ‡ jej eksport musiaÅ‚abym tak jak poniÅ¼ej na wyÅ‚Ä…czonej maszynie kilkajÄ…c prawym przyciskiem na maszynÄ™ wybrac miejsce docelwoe jej eksportowania i mogÅ‚abym siÄ™ wÃ³wczas cieszyÄ‡ zapisem mojej maszyny:

![image](https://github.com/user-attachments/assets/f42364e0-512b-49f5-a73b-abf3c1d259a0)

- [x] **Na gÅ‚Ã³wnej maszynie wirtualnej, zainstaluj oprogramowanie Ansible**

Na maszynie "Fedora" pobraÅ‚am ansible `sudo dnf install ansible -y`

- [x] **WymieÅ„ klucze SSH miÄ™dzy uÅ¼ytkownikiem w gÅ‚Ã³wnej maszynie wirtualnej, a uÅ¼ytkownikiem `ansible` z nowej tak, by logowanie `ssh ansible@ansible-target` nie wymagaÅ‚o podania hasÅ‚a**

SkorzystaÅ‚am z polecenia  `ssh-copy-id ansible@ansible-target`, niestety coÅ› poszÅ‚o nie tak i poÅ‚czÄ…enie wymagaÅ‚o podania hasÅ‚a:

![image](https://github.com/user-attachments/assets/ad564db3-eca1-4644-a6a4-f7e7f3dd8bcb)

Jak widaÄ‡, miaÅ‚am maÅ‚y porblem z wymianÄ… kluczy. WystarczyÅ‚o jednak ruchomiÄ‡ proces `ssh-agent (eval "$(ssh-agent -s)")`, a nastÄ™pnie dodaÅ‚am do niego mÃ³j klucz prywatny (`ssh-add ~/.ssh/id_ed25519`). Agent SSH przechowuje klucz w pamiÄ™ci i automatycznie go udostÄ™pnia przy kaÅ¼dej prÃ³bie poÅ‚Ä…czenia, dziÄ™ki czemu polecenie
`ssh ansible@ansible-target` loguje siÄ™ od razu â€“ bez ponownego pytania o passphrase. Teraz wymiana kluczy miÄ™dzy maszynami przebiegÅ‚a poprawnie:

![image](https://github.com/user-attachments/assets/f5798d6d-f7dc-4047-9d3b-f4ff8a1cfc2a)

SprawdziÅ‚am szybko poÅ‚Ä…czenie, w tym celu w pliku */etc/ansible/hosts* dodaÅ‚am linijkÄ™ 
```bash
[targets]
ansible-target
```
Rezultaty:
![image](https://github.com/user-attachments/assets/1347efd4-0cb9-4ee0-af5d-777b62c4ce67)

### Inwentaryzacja
- [x] **Dokonaj inwentaryzacji systemÃ³w**
  - **Ustal przewidywalne nazwy komputerÃ³w (maszyn wirtualnych) stosujÄ…c `hostnamectl`, Unikaj `localhost`.**

Na maszynie gÅ‚Ã³wnej uruchomiÅ‚am `hostnamectl set-hostname ansible-orchestrator`, a dla maszyny docelowej pozostaÅ‚a nazwa *ansible-target*

![image](https://github.com/user-attachments/assets/b9fa36e8-5019-4a50-8452-477715e198a6)

  - **WprowadÅº nazwy DNS dla maszyn wirtualnych, stosujÄ…c `systemd-resolved` lub `resolv.conf` i `/etc/hosts`**

W pliku /etc/hosts dodaÅ‚am wpisy przypisujÄ…ce statyczne adresy IP do nazw maszyn wirtualnych. DziÄ™ki temu moÅ¼liwe jest odwoÅ‚ywanie siÄ™ do komputerÃ³w za pomocÄ… nazw (np. ansible-orchestrator, ansible-target), zamiast zapamiÄ™tywania adresÃ³w IP. 

![image](https://github.com/user-attachments/assets/9fca6e63-a822-42ab-b534-49a9fad4f84d)

  - **Zweryfikuj Å‚Ä…cznoÅ›Ä‡**

W tym celu poprostu wykonaÅ‚am *ping*:
![image](https://github.com/user-attachments/assets/1c0a4691-841c-48ca-a94c-f74276b3144d)


  - **StwÃ³rz plik inwentaryzacji]**
  - **UmieÅ›Ä‡ w nim sekcje `Orchestrators` oraz `Endpoints`. UmieÅ›Ä‡ nazwy maszyn wirtualnych w odpowiednich sekcjach**

UtowrzyÅ‚am plik hosts.ini (INI to format pliku konfiguracyjnego - nazwa od "initialization")
hosts.ini:
```
[Orchestrators]
ansible-orchestrator ansible_user=kaoina

[Endpoints]
ansible-target ansible_user=ansible

```

  - **WyÅ›lij Å¼Ä…danie `ping` do wszystkich maszyn**
Aby to zrobiÄ‡ wykonaÅ‚am polecenie `ansible all -i hosts.ini -m ping`. Polecenie to wykorzystuje Ansible do wysÅ‚ania Å¼Ä…dania ping do wszystkich hostÃ³w (all) zdefiniowanych w pliku hosts.ini. DziÄ™ki temu mogÅ‚am upewniÄ‡ siÄ™, Å¼e maszyny sÄ… dostÄ™pne przez SSH, poprawnie rozpoznajÄ… siÄ™ po nazwach
![image](https://github.com/user-attachments/assets/ad665af3-a1dc-49a5-8f1f-faf7a26dbf70)

- [x] **Zapewnij Å‚Ä…cznoÅ›Ä‡ miÄ™dzy maszynami**
  - **UÅ¼yj co najmniej dwÃ³ch maszyn wirtualnych**
  - **Dokonaj wymiany kluczy miÄ™dzy maszynÄ…-dyrygentem, a koÅ„cÃ³wkami**

Aby zapewniÄ‡ bezhasÅ‚owÄ… Å‚Ä…cznoÅ›Ä‡ SSH miÄ™dzy maszynÄ…-dyrygentem (ansible-orchestrator) a maszynami docelowymi (ansible-target oraz samym orchestrator), wygenerowaÅ‚am parÄ™ kluczy SSH (jeÅ›li jeszcze nie istniaÅ‚a), a nastÄ™pnie przesÅ‚aÅ‚am klucz publiczny na maszyny docelowe przy uÅ¼yciu polecenia ssh-copy-id. OperacjÄ™ wykonaÅ‚am dla obu maszyn:
```bash
ssh-copy-id kaoina@ansible-orchestrator 
ssh-copy-id ansible@ansible-target
```
  - **Upewnij siÄ™, Å¼e Å‚Ä…cznoÅ›Ä‡ SSH miÄ™dzy maszynami jest moÅ¼liwa i nie potrzebuje haseÅ‚**
Aby upewniÄ‡ siÄ™, Å¼e Å‚Ä…cznoÅ›Ä‡ SSH miÄ™dzy maszynami nie wymaga podawania hasÅ‚a, uruchomiÅ‚am agenta SSH poleceniem `eval "$(ssh-agent -s)"`. NastÄ™pnie dodaÅ‚am klucz prywatny do agenta za pomocÄ… `ssh-add ~/.ssh/id_ed25519`. Po podaniu hasÅ‚a do klucza klucz zostaÅ‚ zaÅ‚adowany i mogÅ‚am poÅ‚Ä…czyÄ‡ siÄ™ z maszynÄ… ansible-target poleceniem ssh ansible@ansible-target bez potrzeby podawania hasÅ‚a do uÅ¼ytkownika.

![image](https://github.com/user-attachments/assets/8096a149-55b3-4263-bd8a-c6cd1e97d930)

### Zdalne wywoÅ‚ywanie procedur
Za pomocÄ… playbooka Ansible:
  - [x] **WyÅ›lij Å¼Ä…danie `ping` do wszystkich maszyn**
  - [x] **Skopiuj plik inwentaryzacji na maszyny/Ä™ `Endpoints`**
  - [x] **PonÃ³w operacjÄ™, porÃ³wnaj rÃ³Å¼nice w wyjÅ›ciu**

ZawartoÅ›Ä‡ playbooka:
```bash
- name: Ping and copy inventory file
  hosts: all
  tasks:
    - name: Ping each host
      ansible.builtin.ping:

    - name: Copy hosts.ini to endpoints
      copy:
        src: ./hosts.ini
        dest: /tmp/hosts.ini
      when: "'ansible-target' in inventory_hostname"
```
WywoÅ‚anie playbooka komendÄ…: ` ansible-playbook -i hosts.ini ping_and_copy.yml `
NastÄ™pnie poÅ‚Ä…czyÅ‚am siÄ™ zdalnie z ansible-target, aby upewniÄ‡ siÄ™, Å¼e plik zostaÅ‚ poprawnie skopiowany do */tmp/hosts.ini*:
![image](https://github.com/user-attachments/assets/bf58eee1-abd2-4d70-b84d-8a888ca375c3)

Pierwsze uruchomienie:
![image](https://github.com/user-attachments/assets/78e005c4-331e-40ab-81c1-4e0fbff07084)

Kolejne uruchomienie:
![image](https://github.com/user-attachments/assets/b162ff46-49c5-4507-a687-2d97534202e5)

Przy kolejnym uruchomieniu:
-	ping dalej dziaÅ‚a
-	copy pokazuje ok zamiast changed
Przy ponownym uruchomieniu playbooka, zadanie ping nadal potwierdza Å‚Ä…cznoÅ›Ä‡ ze wszystkimi maszynami, natomiast zadanie kopiowania pliku zwraca ok zamiast changed, poniewaÅ¼ zawartoÅ›Ä‡ pliku hosts.ini na maszynie docelowej nie ulegÅ‚a zmianie. To pokazuje, Å¼e Ansible nie wykonuje zmian, jeÅ›li nie sÄ… potrzebne.

  - [x] **Zaktualizuj pakiety w systemie**

Aby zaktualizowaÄ‡ wszystkie pakiety na maszynach docelowych, przygotowaÅ‚am prosty playbook Ansible o nazwie update.yml. Playbook ten wykorzystuje moduÅ‚ dnf do bezpiecznej aktualizacji wszystkich pakietÃ³w do najnowszych wersji:
```bash
- name: Update all packages on Endpoints
  hosts: Endpoints
  become: true
  tasks:
    - name: Update all packages (safe way)
      ansible.builtin.dnf:
        name: "*"
        state: latest
        update_cache: yes
```
Playbook uruchomiÅ‚am komendÄ… ` ansible-playbook -i hosts.ini update.yml --ask-become-pass`. Ze wzglÄ™du na to, Å¼e aktualizacja pakietÃ³w wymaga uprawnieÅ„ administratora, musiaÅ‚am dodaÄ‡ opcjÄ™ --ask-become-pass, aby Ansible zapytaÅ‚o o hasÅ‚o do sudo.

![image](https://github.com/user-attachments/assets/c7748060-6cd8-4f54-a0dd-4580c4b13371)

  - [x] **Zrestartuj usÅ‚ugi `sshd` i `rngd`**

Aby zrealizowaÄ‡ zadanie polegajÄ…ce na restarcie usÅ‚ug sshd i rngd na maszynach koÅ„cowych, przygotowaÅ‚am playbook restart.yml, ktÃ³ry wyglÄ…daÅ‚ nastÄ™pujÄ…co:
plik *restart.yml*:
```bash
- name: Restart sshd and rngd services on Endpoints
  hosts: Endpoints
  become: true
  tasks:
    - name: Restart sshd
      ansible.builtin.service:
        name: sshd
        state: restarted

    - name: Restart rngd
      ansible.builtin.service:
        name: rngd
        state: restarted
```

PoczÄ…tkowo uruchomienie playbooka zakoÅ„czyÅ‚o siÄ™ bÅ‚Ä™dem
![image](https://github.com/user-attachments/assets/3a923204-5d2d-44da-89ef-58e68c370200)

UsÅ‚uga rngd nie byÅ‚a dostÄ™pna na systemie ("Could not find the requested service rngd"). SprawdziÅ‚am jej status (`systemctl status rngd`), a nastÄ™pnie zainstalowaÅ‚am brakujÄ…cy pakiet `sudo dnf install rng-tools`

![image](https://github.com/user-attachments/assets/db742689-6452-4ea0-ba85-779cfedbdc6d)

Ponowne wykonanie playbooka zakoÅ„czyÅ‚o siÄ™ peÅ‚nym sukcesem. DziÄ™ki temu oba demony (sshd i rngd) zostaÅ‚y zrestartowane automatycznie z poziomu Ansible.
![image](https://github.com/user-attachments/assets/e8c31f7c-ebab-4335-9741-af94d5f00faf)

  - [x] **PrzeprowadÅº operacje wzglÄ™dem maszyny z wyÅ‚Ä…czonym serwerem SSH, odpiÄ™tÄ… kartÄ… sieciowÄ…**
Po odÅ‚Ä…czeniu ssh na maszynie ansible (` sudo systemctl stop sshd`) otrzymaliÅ›my poniÅ¼szy wynik:

![image](https://github.com/user-attachments/assets/4a59f249-c842-4fe4-ac15-ccb7cc9533c0)

### ZarzÄ…dzanie stworzonym artefaktem
Za pomocÄ… playbooka Ansible:

- [x] **JeÅ¼eli artefaktem z Twojego *pipeline'u* byÅ‚ kontener:
  - **Zbuduj i uruchom kontener sekcji `Deploy` z poprzednich zajÄ™Ä‡**
  - **Pobierz z Docker Hub aplikacjÄ™ "opublikowanÄ…" w ramach kroku `Publish`**
  - **Na maszynie docelowej, **Dockera zainstaluj Ansiblem!**
  - **Zweryfikuj Å‚Ä…cznoÅ›Ä‡ z kontenerem**
  - **Zatrzymaj i usuÅ„ kontener**

W ramach zadania stworzyÅ‚am playbook do zarzÄ…dzania kontenerem Docker, ktÃ³ry jest efektem dziaÅ‚ania pipeline'u. CaÅ‚oÅ›Ä‡ zostaÅ‚a zaimplementowana w postaci roli Ansible. UtworzyÅ‚am strukturÄ™ roli przy pomocy komendy ` ansible-galaxy init docker_artifact`

![image](https://github.com/user-attachments/assets/c56cb3b6-159c-4aa0-b4a9-86e537405131)

W pliku docker_artifact/tasks/main.yml umieÅ›ciÅ‚am zadania odpowiedzialne za:
  * instalacjÄ™ i uruchomienie Dockera
  * pobranie kontenera kaoina666/redis_runtime:2 z Docker Hub
  * jego uruchomienie z odpowiednim przekierowaniem portu
  * sprawdzenie dostÄ™pnoÅ›ci usÅ‚ugi Redis
  * a nastÄ™pnie jego zatrzymanie i usuniÄ™cie

```bash
- name: Install Docker
  ansible.builtin.dnf:
    name: docker
    state: present
  become: true

- name: Enable and start Docker
  ansible.builtin.service:
    name: docker
    state: started
    enabled: true
  become: true

- name: Pull Redis image from Docker Hub
  community.docker.docker_image:
    name: kaoina666/redis_runtime
    tag: "2"
    source: pull

- name: Run Redis container
  community.docker.docker_container:
    name: redis_test
    image: kaoina666/redis_runtime:2
    state: started
    restart_policy: always
    published_ports:
      - "6380:6379"

- name: Wait for Redis to respond on port 6380
  ansible.builtin.wait_for:
    host: "127.0.0.1"
    port: 6380
    timeout: 15

- name: Stop Redis container
  community.docker.docker_container:
    name: redis_test
    state: stopped

- name: Remove Redis container
  community.docker.docker_container:
    name: redis_test
    state: absent
```
Aby dziaÅ‚aÅ‚o, muszÄ… byÄ‡ zainstalowane wszystkie potrzebne kolekcje:
![image](https://github.com/user-attachments/assets/2dfacd4a-2637-48e1-99d3-68bab6b054b4)

Plik requirements.yml:
```bash
collections:
  - name: community.docker
```
CaÅ‚oÅ›Ä‡ uruchomiÅ‚am przez playbook run_artifact.yml komendÄ… ` ansible-playbook -i hosts.ini run_artifact.yml --ask-become-pass`

![image](https://github.com/user-attachments/assets/d2c868f8-b548-40d9-b078-ca5252966640)

Moje run artifact:
```
- name: Manage Redis container artifact
  hosts: Endpoints
  become: true

  roles:
    - docker_artifact
```
Po uruchomieniu playbook wykonaÅ‚ wszystkie zadania poprawnie: Redis zostaÅ‚ pobrany i uruchomiony, a nastÄ™pnie zatrzymany i usuniÄ™ty.
_______________________________________________________________________
## **LAB 9 Pliki odpowiedzi dla wdroÅ¼eÅ„ nienadzorowanych (Kickstart)** 

Tematem laboratorium byÅ‚o przygotowanie automatycznego ÅºrÃ³dÅ‚a instalacji systemu operacyjnego Fedora z wykorzystaniem pliku odpowiedzi Kickstart. Celem byÅ‚o przeprowadzenie nienadzorowanej instalacji systemu oraz konfiguracja Å›rodowiska tak, aby po uruchomieniu system automatycznie uruchamiaÅ‚ nasze oprogramowanie. W ramach zadania naleÅ¼aÅ‚o takÅ¼e zadbaÄ‡ o odpowiednie repozytoria, zaleÅ¼noÅ›ci oraz sposÃ³b pobrania i instalacji aplikacji.

### Zadania do wykonania
PrzeprowadÅº instalacjÄ™ nienadzorowanÄ… systemu Fedora z pliku odpowiedzi z naszego repozytorium
- [x] **Zainstaluj system Fedora**
  * zastosuj instalator sieciowy (*Everything Netinst*) lub
  * zastosuj instalator wariantu *Server* z wbudowanymi pakietami, przyjmujÄ…cy plik odpowiedzi (dobra opcja dla osÃ³b z ograniczeniami transferu internetowego)

- [x] **Pobierz plik odpowiedzi `/root/anaconda-ks.cfg`**

Za pomocÄ… polecenia cat wyciÄ…gnÄ™Å‚am zawartoÅ›Ä‡ pliku *anaconda-ks.cfg* i skopiowaÅ‚am go

![image](https://github.com/user-attachments/assets/cb34927f-e1a9-4e76-8548-76ab9bfd1b82)


- [x] **Zapoznaj siÄ™ z dokumentacjÄ… pliku odpowiedzi i zmodyfikuj swÃ³j plik:**
  - **Plik odpowiedzi moÅ¼e nie zawieraÄ‡ wzmianek na temat potrzebnych repozytoriÃ³w. JeÅ¼eli Twoja pÅ‚yta instalacyjna nie zawiera pakietÃ³w, dodaj wzmiankÄ™ o repozytoriach skÄ…d je pobraÄ‡. Na przykÅ‚ad, dla systemu Fedora 38:
      * `url --mirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=fedora-38&arch=x86_64`
      * `repo --name=update --mirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=updates-released-f38&arch=x86_64`
  - **Plik odpowiedzi moÅ¼e zakÅ‚adaÄ‡ pusty dysk. Zapewnij, Å¼e zawsze bÄ™dzie formatowaÄ‡ caÅ‚oÅ›Ä‡, stosujÄ…c `clearpart --all`**
  - **Ustaw *hostname* inny niÅ¼ domyÅ›lny `localhost`**
  
Na podstawie dokumentacji Kickstarta oraz wymagaÅ„ zadania, wprowadziÅ‚am nastÄ™pujÄ…ce zmiany w pliku anaconda-ks.cfg:

  * Dodanie repozytoriÃ³w internetowych â€“ umoÅ¼liwia pobranie pakietÃ³w:
```bash
url --mirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=fedora-41&arch=x86_64
repo --name=update --mirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=updates-released-f41&arch=x86_64
```
 
  * Ustawienie niestandardowej nazwy hosta
```bash
network --hostname=NowyHostKaoiny
```
(Uwaga: poczÄ…tkowo uÅ¼yÅ‚am Nowy_host_Kaoiny, ale znak _ powodowaÅ‚ bÅ‚Ä…d â€“ zostaÅ‚ usuniÄ™ty.)

  * Dodanie wyczyszczenia wszystkich partycji â€“ zapewnia, Å¼e instalator nadpisze caÅ‚y dysk i nie zostanÄ… Å¼adne dane
``` bash
clearpart --all --initlabel
```

  * Dodanie automatycznego restartu systemu po instalacji:
`reboot`


```bash
# Generated by Anaconda 41.35
# Generated by pykickstart v3.58
#version=DEVEL

# Keyboard layouts
keyboard --vckeymap=pl --xlayouts='pl'
# System language
lang pl_PL.UTF-8

# Repozytoria i ÅºrÃ³dÅ‚o instalacji
url --mirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=fedora-41&arch=x86_64
repo --name=update --mirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=updates-released-f41&arch=x86_64

# Network information
network  --bootproto=dhcp --device=eth0 --ipv6=auto --activate
network  --hostname=Nowy_host_Kaoiny

%packages
@^server-product-environment

%end

# Run the Setup Agent on first boot
firstboot --enable

# Generated using Blivet version 3.11.0
ignoredisk --only-use=sda
# System bootloader configuration
bootloader --location=mbr --boot-drive=sda
# Partition clearing information
clearpart --all --initlabel
# Disk partitioning information
part pv.159 --fstype="lvmpv" --ondisk=sda --size=8500
part /boot --fstype="ext4" --ondisk=sda --size=1024
part /boot/efi --fstype="efi" --ondisk=sda --size=600 --fsoptions="umask=0077,shortname=winnt"
volgroup fedora_kaoinafedora --pesize=4096 pv.159
logvol / --fstype="ext4" --grow --maxsize=71680 --size=1024 --name=root --vgname=fedora_kaoinafedora

# System timezone
timezone Europe/Warsaw --utc

# Root password
rootpw --lock
user --groups=wheel --name=kaoina --password=$y$j9T$Za.bEcfmtiXOZcJwU4ZjeVc3$J1zgPRZWQk29WsCQCxdmuuGHUw3fkISdkP1RJfpqYQ6 --iscrypted --gecos="Kaoina"

reboot
```

- [x] **UÅ¼yj pliku odpowiedzi do przeprowadzenia instalacji nienadzorowanej**
  - **Uruchom nowÄ… maszynÄ™ wirtualnÄ… z pÅ‚yty ISO i wskaÅ¼ instalatorowi przygotowany plik odpowiedzi stosownÄ… dyrektywÄ…**

W celu uruchomienia nienadzorowanej instalacji, przygotowany plik Kickstarta umieÅ›ciÅ‚am w repozytorium GitHub, a nastÄ™pnie, podczas startu maszyny wirtualnej z obrazu ISO Fedory, nacisnÄ™Å‚am klawisz e w menu i dopisaÅ‚am do linii startowej jÄ…dra parametr `inst.ks=<link_do_pliku_ks>`, gdzie <link_do_pliku_ks> wskazywaÅ‚ bezpoÅ›redni link do wersji Raw pliku Kickstarta w repozytorium.

![image](https://github.com/user-attachments/assets/6171688c-1edb-4a48-bc6b-57f841c2b86f)

SkÄ…d link do pliku odpowiedzi(Raw File):

![image](https://github.com/user-attachments/assets/7d857342-bfb3-42a5-aec8-13d8ef6776a7)

BÅ‚ad z powodu znaku "_":C

![image](https://github.com/user-attachments/assets/195cdb53-f057-4808-8f05-42d6e47cffd6)


Instalacja rozpoczÄ™Å‚a siÄ™ automatycznie i zakoÅ„czyÅ‚a sukcesem, bez koniecznoÅ›ci dalszej ingerencji z mojej strony. Maszyna zostaÅ‚a poprawnie skonfigurowana, zainstalowana i zrestartowana â€“ zgodnie z zawartoÅ›ciÄ… przygotowanego pliku .ks.

![image](https://github.com/user-attachments/assets/f92c53fd-6fca-4fcc-9367-5ff64409687e)
![image](https://github.com/user-attachments/assets/33e1a1be-e6e3-4c9b-9618-d72f555779b4)


---
- [x] **Rozszerz plik odpowiedzi o repozytoria i oprogramowanie potrzebne do uruchomienia programu, zbudowanego w ramach projektu - naszego *pipeline'u*.**
  - **W przypadku kontenera, jest to po prostu Docker.
      - **UtwÃ³rz w sekcji `%post` mechanizm umoÅ¼liwiajÄ…cy pobranie i uruchomienie kontenera
      - **JeÅ¼eli efektem pracy pipeline'u nie byÅ‚ kontener, a aplikacja samodzielna - zainstaluj jÄ…
    - **PamiÄ™taj, Å¼e **Docker zadziaÅ‚a dopiero na uruchomionym systemie!** - nie da siÄ™ wdaÄ‡ w interakcjÄ™ z Dockerem z poziomu instalatora systemu: polecenia `docker run` nie powiodÄ… siÄ™ na tym etapie. Nie zadziaÅ‚a teÅ¼ `systemctl start` (ale `systemctl enable` juÅ¼ tak)

W ramach poprzednich laboratoriÃ³w korzystaÅ‚am z kontenera Dockera zawierajÄ…cego Redis. W zwiÄ…zku z tym, aby system po instalacji automatycznie pobieraÅ‚ i uruchamiaÅ‚ ten kontener, wykonaÅ‚am nastÄ™pujÄ…ce modyfikacje w pliku *anaconda*:

  * Dodanie repozytorium Dockera â€“ umoÅ¼liwia instalacjÄ™ Dockera
```bash
repo --name=docker-ce --baseurl=https://download.docker.com/linux/fedora/41/x86_64/stable
```
  * Dodanie pakietÃ³w Dockera w sekcji %packages â€“ wymagane komponenty do uruchamiania kontenerÃ³w
```bash
docker-ce
docker-ce-cli
containerd.io
```
  * Konfiguracja w sekcji %post

    * WÅ‚Ä…czono usÅ‚ugÄ™ Docker przy starcie:
      
    ```bash
    systemctl enable docker
    ```
    
    * Utworzono katalog */opt/scripts* i plik *start-redis.sh*, ktÃ³ry uruchamia kontener Redis:
    
    ```bash
    cat > /opt/scripts/start-redis.sh << 'EOF'
    #!/bin/bash
    sleep 10
    docker run -d --name redis --restart=always -p 6379:6379 kaoina666/redis_runtime:2
    EOF

    chmod +x /opt/scripts/start-redis.sh
    ```
    
    * Utworzono usÅ‚ugÄ™ *systemd start-redis-container.service*, ktÃ³ra uruchamia skrypt:
  
    ```bash
    cat > /etc/systemd/system/start-redis-container.service << 'EOF'
    [Unit]
    Description=Start Redis container from Docker Hub
    After=network-online.target docker.service
    Wants=network-online.target

    [Service]
    Type=oneshot
    ExecStart=/opt/scripts/start-redis.sh
    RemainAfterExit=yes

    [Install]
    WantedBy=multi-user.target
    EOF

    systemctl enable start-redis-container.service
    ```



Zmodyfikowany plik:
```bash
# Generated by Anaconda 41.35
# Generated by pykickstart v3.58
#version=DEVEL

# Keyboard layouts
keyboard --vckeymap=pl --xlayouts='pl'
# System language
lang pl_PL.UTF-8

# Repozytoria i ÅºrÃ³dÅ‚o instalacji
url --mirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=fedora-41&arch=x86_64
repo --name=update --mirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=updates-released-f41&arch=x86_64
repo --name=docker-ce --baseurl=https://download.docker.com/linux/fedora/41/x86_64/stable

# Network information
network  --bootproto=dhcp --device=eth0 --ipv6=auto --activate
network  --hostname=NowyHostKaoiny

%packages
@^server-product-environment
docker-ce
docker-ce-cli
containerd.io
%end

%post --log=/root/ks-post.log
# Docker do uruchamiania przy starcie
systemctl enable docker

# Pobranie i przygotowanie Redisa (wÅ‚asny kontener z Docker Huba)
mkdir -p /opt/scripts

cat > /opt/scripts/start-redis.sh << 'EOF'
#!/bin/bash
# Start dockera
sleep 10
# Uruchom kontener redis z dockerhub
docker run -d --name redis --restart=always -p 6379:6379 kaoina666/redis_runtime:2
EOF

chmod +x /opt/scripts/start-redis.sh

# Jednostka systemd do uruchamiania kontenera przy starcie
cat > /etc/systemd/system/start-redis-container.service << 'EOF'
[Unit]
Description=Start Redis container from Docker Hub
After=network-online.target docker.service
Wants=network-online.target

[Service]
Type=oneshot
ExecStart=/opt/scripts/start-redis.sh
RemainAfterExit=yes

[Install]
WantedBy=multi-user.target
EOF

# Jednostka systemd
systemctl enable start-redis-container.service
%end

# Run the Setup Agent on first boot
firstboot --enable

# Generated using Blivet version 3.11.0
ignoredisk --only-use=sda
# System bootloader configuration
bootloader --location=mbr --boot-drive=sda
# Partition clearing information
clearpart --all --initlabel
# Disk partitioning information
part pv.159 --fstype="lvmpv" --ondisk=sda --size=8500
part /boot --fstype="ext4" --ondisk=sda --size=1024
part /boot/efi --fstype="efi" --ondisk=sda --size=600 --fsoptions="umask=0077,shortname=winnt"
volgroup fedora_kaoinafedora --pesize=4096 pv.159
logvol / --fstype="ext4" --grow --maxsize=71680 --size=1024 --name=root --vgname=fedora_kaoinafedora

# System timezone
timezone Europe/Warsaw --utc

# Root password
rootpw --lock
user --groups=wheel --name=kaoina --password=$y$j9T$Za.bEcfmtiXOZcJwU4ZjeVc3$J1zgPRZWQk29WsCQCxdmuuGHUw3fkISdkP1RJfpqYQ6 --iscrypted --gecos="Kaoina"

reboot
```

Uruchamianie i konfiguracja maszyny:

![image](https://github.com/user-attachments/assets/0f83c469-8c14-4d7e-8474-c79c4c8adf37)


- [x] **Zadbaj o automatyczne ponowne uruchomienie na koÅ„cu instalacji**

Za automatyczny restart systemu po zakoÅ„czeniu instalacji odpowiada ostatnia linijka w pliku: `reboot`

- [x] **Zapewnij, by od razu po pierwszym uruchomieniu systemu, oprogramowanie zostaÅ‚o uruchomione (w dowolny sposÃ³b)**

1. Czy Docker dziaÅ‚a?
`systemctl status docker`
Docker.service ma status active (running)
![image](https://github.com/user-attachments/assets/f7d7b1ec-a3eb-4768-af4e-ab4bae495930)

2. Czy kontener Redis dziaÅ‚a?
`docker ps`
Widoczny kontener Redis z STATUS: Up 
![image](https://github.com/user-attachments/assets/c6712ff9-bff4-46a5-b805-66f7ee5ae659)

3. Czy Redis nasÅ‚uchuje?
`ss -lntp | grep 6379`
Widoczny proces nasÅ‚uchujÄ…cy na porcie 6379.
![image](https://github.com/user-attachments/assets/944a4a2c-6e54-496a-b969-e96712b87f9b)

4. Czy usÅ‚uga systemd dziaÅ‚a?
`systemctl status start-redis-container.service`
UsÅ‚uga start-redis-container.service dziaÅ‚aÅ‚a i zakoÅ„czyÅ‚a siÄ™ sukcesem:
![image](https://github.com/user-attachments/assets/f8fc7bc9-adc4-4801-a4ae-1941ea5247d0)

_________________________________________________________________
## **LAB 10 WdraÅ¼anie na zarzÄ…dzalne kontenery: Kubernetes (1)**

Celem tych laboratoriÃ³w byÅ‚o zapoznanie siÄ™ z podstawami dziaÅ‚ania Kubernetesa oraz nauczenie siÄ™, jak uruchamiaÄ‡ i zarzÄ…dzaÄ‡ aplikacjami kontenerowymi w lokalnym klastrze za pomocÄ… Minikube. W ramach Ä‡wiczeÅ„ zainstalowaÅ‚am Minikube, uruchomiÅ‚am Dashboard, wdroÅ¼yÅ‚am wÅ‚asny kontener oraz przygotowaÅ‚am plik YAML opisujÄ…cy deployment z replikacjÄ…. 
     
### Instalacja klastra Kubernetes
- [x] **Zaopatrz siÄ™ w implementacjÄ™ stosu k8s: minikube**

ZainstalowaÅ‚am Minikube, korzystajÄ…c z wersji RPM package, zgodnie z instrukcjÄ… na stronie

![image](https://github.com/user-attachments/assets/450d2659-a3fa-4d25-b80c-fb995d861c5f)

- [x] **PrzeprowadÅº instalacjÄ™, wykaÅ¼ poziom bezpieczeÅ„stwa instalacji**

Aby zainstalowaÄ‡ Minikube, wykonaÅ‚am nastÄ™pujÄ…ce komendy:

```bash
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-latest.x86_64.rpm
sudo rpm -Uvh minikube-latest.x86_64.rpm
```

![image](https://github.com/user-attachments/assets/226e2b91-e9ec-48f2-980a-fca29f9c259a)

Po instalacji, aby sprawdziÄ‡ stan klastra, uÅ¼yÅ‚am komend:
``` bash
minikube kubectl -- get pods -A -o wide
```
Ta komenda pozwala uzyskaÄ‡ szczegÃ³Å‚owe informacje o podach systemowych. Na podstawie wynikÃ³w mogÅ‚am zweryfikowaÄ‡ poprawnoÅ›Ä‡ dziaÅ‚ania aplikacji na klastrze.
![image](https://github.com/user-attachments/assets/3d61c98b-e8b6-4223-abba-4c1b46f0214b)

Ponadto, aby uzyskaÄ‡ dodatkowe informacje o poszczegÃ³lnych podach, skorzystaÅ‚am z komendy:
```bash
minikube kubectl -- describe pod <nazwa> -n kube-system
```
Wynik tej komendy pozwoliÅ‚ mi upewniÄ‡ siÄ™, Å¼e wszystkie pody dziaÅ‚ajÄ… zgodnie z oczekiwaniami.
![image](https://github.com/user-attachments/assets/29ae13b2-ba75-4b09-8b63-41606c6d5f95)


- [x] **Zaopatrz siÄ™ w polecenie `kubectl` w wariancie minikube, moÅ¼e byÄ‡ alias `minikubctl`, jeÅ¼eli masz juÅ¼ "prawdziwy" `kubectl`**

Zgodnie z instruckjÄ… na stornie 
![image](https://github.com/user-attachments/assets/175523b4-6e8c-404b-b43a-ebdb1dc95872)

UruchomiÅ‚am dwie poniÅ¼sze komendy aby uÅ¼ywaÄ‡ narzÄ™dzia kubectl
```bash
minikube kubectl -- get po -A
alias kubectl="minikube kubectl --"
```

![image](https://github.com/user-attachments/assets/b99989cb-7ccf-42ca-8ce9-e581879ef064)

- [x] **Uruchom Kubernetes, pokaÅ¼ dziaÅ‚ajÄ…cy kontener/worker**

Aby uruchomiÄ‡ klaster Kubernetes, uÅ¼yÅ‚am komendy
```minikube start```

![image](https://github.com/user-attachments/assets/627e5c21-b6ed-4b1c-8b8d-1401f539dc91)

Po uruchomieniu klastra Minikube sprawdziÅ‚am jego stan komendÄ… `minikube kubectl -- get nodes` 

![image](https://github.com/user-attachments/assets/5b60c2a4-8442-410c-ad8b-7181b952a1a1)

Wynik pokazuje, Å¼e wÄ™zeÅ‚ o nazwie minikube ma status Ready, peÅ‚ni rolÄ™ control-plane i dziaÅ‚a od kilku minut, co potwierdza poprawne uruchomienie klastra Kubernetes. NastÄ™pnie sprawdziÅ‚am dziaÅ‚ajÄ…ce pody systemowe w przestrzeni nazw kube-system, korzystajÄ…c z polecenia `minikube kubectl -- get pods -A`.

![image](https://github.com/user-attachments/assets/841c83ab-fa24-4ce5-ac4c-aed92b0cd0e5)

Wszystkie pody sÄ… w stanie Running, co oznacza, Å¼e komponenty systemowe Kubernetesa dziaÅ‚ajÄ… poprawnie.

- [x] **Zmityguj problemy wynikajÄ…ce z wymagaÅ„ sprzÄ™towych lub odnieÅ› siÄ™ do nich (wzglÄ™dem dokumentacji)**

Podczas uruchamiania klastra Minikube napotkaÅ‚am na problem zwiÄ…zany z pamiÄ™ciÄ… RAM. Minikube domyÅ›lnie przydziela okreÅ›lonÄ… iloÅ›Ä‡ pamiÄ™ci wirtualnej maszynie, co moÅ¼e byÄ‡ niewystarczajÄ…ce w przypadku komputerÃ³w o ograniczonych zasobach. Aby rozwiÄ…zaÄ‡ ten problem, dostosowaÅ‚am iloÅ›Ä‡ przydzielonej pamiÄ™ci, wykorzystujÄ…c opcjÄ™ --memory podczas uruchamiania klastra. Na przykÅ‚ad `minikube start --memory 4096`

- [x] **Uruchom Dashboard, otwÃ³rz w przeglÄ…darce, przedstaw Å‚Ä…cznoÅ›Ä‡**

UruchomiÅ‚am Dashbord poleceniem `minikube dashboard`, co umoÅ¼liwiÅ‚o mi teÅ¼ zarzÄ…dzanie prze zinterface webowy.
![image](https://github.com/user-attachments/assets/0ccd28ea-92bd-4615-a372-ba7cef1437e9)

- [x] **Zapoznaj siÄ™ z koncepcjami funkcji wyprowadzanych przez Kubernetesa (*pod*, *deployment* itp)**

  * Pod to najmniejsza jednostka w Kubernetes, ktÃ³ra moÅ¼e zawieraÄ‡ jeden lub wiÄ™cej kontenerÃ³w, dziaÅ‚ajÄ…cych w tym samym Å›rodowisku.
  * Deployment umoÅ¼liwia zarzÄ…dzanie aplikacjami w Kubernetes, zapewniajÄ…c skalowanie i aktualizowanie kontenerÃ³w w sposÃ³b automatyczny.
  * Service pozwala na tworzenie staÅ‚ych punktÃ³w dostÄ™pu do aplikacji dziaÅ‚ajÄ…cych w klastrze, umoÅ¼liwiajÄ…c komunikacjÄ™ z kontenerami.

 
### Analiza posiadanego kontenera
- [x] **Zdefiniuj krok "Deploy" swojego projektu jako "Deploy to cloud":**
  - **Deploy zbudowanej aplikacji powinien siÄ™ odbywaÄ‡ "na kontener"**
  - **Przygotuj obraz Docker ze swojÄ… aplikacjÄ… - sprawdÅº, Å¼e TwÃ³j kontener Deploy na pewno **pracuje**, a nie natychmiast koÅ„czy pracÄ™! ğŸ˜**
  - **WykaÅ¼, Å¼e wybrana aplikacja pracuje jako kontener

Ja miaÅ‚am juÅ¼ gotowy ten etap dzieki wczeÅ›niejszym laboratoriom. Kontener z aplikacjÄ… Redis miaÅ‚am juÅ¼ zdeployowany na DockerHubie (przygotowany obraz Docker kaoina666/redis_runtime:2).

### Uruchamianie oprogramowania
- [x] **Uruchom kontener ze swojÄ…/wybranÄ… aplikacjÄ… na stosie k8s**
- [x] **Kontener uruchomiony w minikubie zostanie automatycznie "ubrany" w *pod*.**
- [x] **```minikube kubectl run -- <nazwa-jednopodowego-wdroÅ¼enia> --image=<obraz-docker> --port=<wyprowadzany port> --labels app=<nazwa-jednopodowego-wdroÅ¼enia>```**

Uruchomienie za pomocÄ… `kubectl run redis-deployment --image=kaoina666/redis_runtime:2 --port=6379 --labels app=redis-deployment`. Kubectl run sÅ‚uÅ¼y do uruchamiania pojedynczego poda z okreÅ›lonym obrazem Docker oraz konfiguracjÄ… portÃ³w i etykiet. 

![image](https://github.com/user-attachments/assets/e2c0048a-29a1-46ec-9f38-ab0f6533e41d)

- [x] **Przedstaw Å¼e *pod* dziaÅ‚a (via Dashboard oraz `kubectl`)**

dziÄ™ki `kubectl get pods` moÅ¼emy w terminalu zoabczyÄ‡, Å¼e *redis-deployment* jest aktywnym podem
![image](https://github.com/user-attachments/assets/f2271ddc-af73-4cd9-86a6-e1c6b7aa4843)

rÃ³wnieÅ¼ w webowej aplikacji widoczny bÄ™dzie nasz pod

![image](https://github.com/user-attachments/assets/4a631cd5-21d4-4a1c-bfc6-89e930523f74)

  
- [x] **WyprowadÅº port celem dotarcia do eksponowanej funkcjonalnoÅ›ci**
- [x] **```kubectl port-forward pod/<nazwa-wdroÅ¼enia> <LO_PORT>:<PODMAIN_CNTNR_PORT> ```**

`kubectl port-forward pod/redis-deployment 6379:6379`
Komenda ta przekierowuje port 6379 z kontenera Redis (uruchomionego w podzie o nazwie redis-deployment) na lokalny port 6379 na maszynie. Przekierowanie portÃ³w pozwala na dostÄ™p do usÅ‚ugi Redis, dziaÅ‚ajÄ…cej wewnÄ…trz klastra Kubernetes, poprzez lokalny port na komputerze uÅ¼ytkownika.

Po wykonaniu komendy, terminal zwrÃ³ciÅ‚ komunikat: 
![image](https://github.com/user-attachments/assets/48f9516a-2db5-4338-b6c3-f24a0c38b08c)

- [x] **Przedstaw komunikacjÄ™ z eskponowanÄ… funkcjonalnoÅ›ciÄ…**
W celu weryfikacji poprawnoÅ›ci dziaÅ‚ania, uÅ¼yto klienta Redis, Å‚Ä…czÄ…c siÄ™ za pomocÄ… nastÄ™pujÄ…cej komendy: `redis-cli -h 127.0.0.1 -p 6379`.

![image](https://github.com/user-attachments/assets/25fb98d4-8e6e-4ca9-83e5-3f4247a5212e)

Port forwarding dziaÅ‚a poprawnie, a usÅ‚uga Redis jest dostÄ™pna lokalnie na porcie 6379.


### Przekucie wdroÅ¼enia manualnego w plik wdroÅ¼enia (wprowadzenie)
- [x] **Zapisz wdroÅ¼enie jako plik YML**
- [x] **PrzeprowadÅº prÃ³bne wdroÅ¼enie przykÅ‚adowego *deploymentu***
  - **Wykonaj ```kubectl apply``` na pliku**
  - **Upewnij siÄ™, Å¼e posiadasz wdroÅ¼enie zapisane jako plik**
  - **WzbogaÄ‡ swÃ³j *deployment* o 4 repliki**
  - **Rozpocznij wdroÅ¼enie za pomocÄ… ```kubectl apply```**
  - **Zbadaj stan za pomocÄ… ```kubectl rollout status```**
- [x] **Wyeksponuj wdroÅ¼enie jako serwis**
- [x] **Przekieruj port do serwisu (tak, jak powyÅ¼ej)**

W ramach tego zadania stworzyÅ‚am plik redis.yaml, ktÃ³ry umoÅ¼liwia wdroÅ¼enie aplikacji Redis w Kubernetes. Celem tego wdroÅ¼enia byÅ‚o utworzenie instancji Redis, ktÃ³rÄ… moÅ¼na bÄ™dzie wystawiÄ‡ na zewnÄ…trz klastra. Aby uruchomiÄ‡ to wdroÅ¼enie, uÅ¼yÅ‚am komendy kubectl apply -f redis.yaml.

Oto zawartoÅ›Ä‡ pliku redis.yaml:

```bash
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-deployment
spec:
  replicas: 4
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - name: redis
        image: kaoina666/redis_runtime:2
        ports:
        - containerPort: 6379
---
apiVersion: v1
kind: Service
metadata:
  name: redis-service
spec:
  selector:
    app: redis
  type: NodePort
  ports:
    - port: 6379
      targetPort: 6379
      nodePort: 30079
```

Po uruchomieniu tego pliku utworzyÅ‚y siÄ™ cztery pody, co mogÅ‚am zweryfikowaÄ‡, sprawdzajÄ…c status wdroÅ¼enia (`kubectl get pods`)
![image](https://github.com/user-attachments/assets/3ce1a97d-ae98-4f94-b245-5d97509009c0)

Aby upewniÄ‡ siÄ™, Å¼e wdroÅ¼enie poszÅ‚o pomyÅ›lnie, uÅ¼yÅ‚am komendy `kubectl rollout status deployment/redis-deployment`. Zgodnie z oczekiwaniami, proces zakoÅ„czyÅ‚ siÄ™ sukcesem, a wszystkie repliki zostaÅ‚y uruchomione:
![image](https://github.com/user-attachments/assets/3950cd09-ecf5-4d30-9214-5e992918f858)

Na koÅ„cu, zgodnie z planem, przekierowaÅ‚am port do serwisu, uÅ¼ywajÄ…c komendy `kubectl port-forward svc/redis-service 6379:6379`. Aby sprawdziÄ‡, czy wszystko dziaÅ‚a, poÅ‚Ä…czyÅ‚am siÄ™ z Redisem, uÅ¼ywajÄ…c *redis-cli* komendÄ… `redis-cli -h localhost -p 6379`. Rezultat:

![image](https://github.com/user-attachments/assets/7fb51887-a4fa-433c-847b-fe4febdb1bb9)

_________________________________________________________________
## **LAB 11 WdraÅ¼anie na zarzÄ…dzalne kontenery: Kubernetes (2)**

Cel - 

### Zadanie
- [x] **podpunkt**
  - **podpunkt**
