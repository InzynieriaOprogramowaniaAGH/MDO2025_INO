# Ansible

## Start

Przygotowanie drugiej VM z systemem Fedora41 (jest to ten sam obraz co moja pierwsza VM). 


![Terminal](virtualbox.png)

NastÄ™pnie stworzyÅ‚em sieÄ‡ NAT aby obie VM-ki siÄ™ widziaÅ‚y.

![Terminal](NAT.png)

NastÄ™pnie na nowej  VM-ce zainstalowaÅ‚em tar oraz openssh-server

![Terminal](SSH.png)

Potem na maszynie gÅ‚Ã³wnej dodaÅ‚em uÅ¼ytkownika ansible z prawami SUDO co bÄ™dzie przydatne pÃ³Åºniej w trakcie uruchamia playbooka.

![Terminal](user.png)

Dalej utworzyÅ‚em migawkÄ™

![Terminal](migawka.png)

Na maszynie gÅ‚Ã³wnej zainstalowaÅ‚em Ansible

![Terminal](ansible.png)

PÃ³Åºniej wygenerowaÅ‚em parÄ™ kluczy rsa i przekazaÅ‚em je drugiej maszynie ansible po to by mÃ³c siÄ™ Å‚Ä…czyÄ‡ przez ssh bez koniecznoÅ›ci podawania hasÅ‚a

![Terminal](klucze.png)

Dalej ustawiÅ‚em nazwy VM-ek uÅ¼ywajÄ…c komendy hostnamectl

![Terminal](main.png)
![Terminal](ansiblename.png)

Dalszym krokiem aby mÃ³c Å‚Ä…czyÄ‡ siÄ™ po samej nazwie byÅ‚o doadnie IP VM-ki do pliku etc/hosts

![Terminal](dodanieip.png)

SkopiowaÅ‚em wczeÅ›niej utworzone klucze do Ansible

![Terminal](klucze2.png)

DziÄ™ki tym krokom mogÅ‚em dostaÄ‡ siÄ™ przez ssh do vmki drugiej ansible :) 

![Terminal](logowanie.png)

NastÄ™pnie utworzyÅ‚em plik hosts, w ktÃ³rym okreÅ›liÅ‚em hosty, na jakich Ansible bÄ™dzie wykonywaÅ‚ zadania. Plik ten zawiera dwie gÅ‚Ã³wne sekcje: Orchestrators, do ktÃ³rej przypisaÅ‚em gÅ‚Ã³wnÄ… maszynÄ™ â€“ main, z ktÃ³rej uruchamiane bÄ™dÄ… playbooki Ansible, oraz Endpoints, gdzie znalazÅ‚y siÄ™ pozostaÅ‚e hosty docelowe, w tym przypadku ansible.

```bash
all:
  children:
    orchestrators:
      hosts:
        localhost:
          ansible_connection: local

    endpoints:
      hosts:
        ansible:
          ansible_host: 10.2.0.5
          ansible_user: ansible
          ansible_python_interpreter: /usr/bin/python3
          ansible_become_pass: "************"
        
```

DodaÅ‚em tutaj 

```bash
          ansible_python_interpreter: /usr/bin/python3
           ansible_become_pass: "************"
```

Aby umoÅ¼liwiÄ‡ maszynie ansible wykonywaÄ‡ polecenia z uprawnieniami administratora i aby wskazaÄ‡ na jakiej wersji pythona zdalna maszyna ma uruchamiaÄ‡ moduÅ‚y (miaÅ‚e bez tej linijki bÅ‚Ä™dy lecz robie sprawozdanie po czasie i zapomniaÅ‚em z czym byÅ‚y zwiÄ…zane ...)

Potem uruchomiÅ‚em hosts.yaml

![Terminal](hosts.png)

NastÄ™pnie stworzyÅ‚em playbook, ktÃ³ry:
-wysyÅ‚a Å¼adanie ping do wszystkich maszyn
-kopiuje plik hosts na maszynÄ™ docelowÄ…
-restartuje usÅ‚ugÄ™ sshd i rngd

```bash
---
- name: ZarzÄ…dzanie maszynÄ… koÅ„cowÄ… (endpoints)
  hosts: endpoints
  become: true
  gather_facts: true
  tasks:
    - name: Sprawdzane poÅ‚aczenia (ping)
      ansible.builtin.ping:

    - name: Kopiowanie pliku hosts na maszynÄ™ docelowÄ…
      ansible.builtin.copy:
        src: /home/jzborowski/lab1/MDO2025_INO/ITE/GCL08/JZ415998/Sprawko3original/ANSIBLE/hosts.yaml
        dest: /tmp/hosts.yaml
        owner: ansible
        group: ansible
        mode: '0644'

    -  name: Restart usÅ‚ugi sshd
       ansible.builtin.service:
         name: sshd
         state: restarted

    -  name: Restart usÅ‚ugi rngd
       ansible.builtin.service:
         name: rngd
         state: restarted
```
Efekt:

![Terminal](playbook.png)

Nie mam screena z drugiega puszczenia ale wtedy kopiowanie byÅ‚o pominiÄ™te (plik zostaÅ‚ juÅ¼ skopiowany przy pierwszym uruchomieniu)

NastÄ™pnie zatrzymaÅ‚em sshd

![Terminal](sshd.png)

I odpaliÅ‚em playbook jeszcze raz, co z oczywistych wzglÄ™dÃ³w siÄ™ nie powiodÅ‚o - Å‚Ä…cznoÅ›Ä‡ z hostami odbywa siÄ™ przez ssh a jego nie byÅ‚o.

![Terminal](bladbezssh.png)

Po wykonaniu krokÃ³w z playbookiem zabraÅ‚em siÄ™ za playbook z artefaktem 

ZaczÄ…Å‚em od jego utworzenia:

![Terminal](artefakt.png)

*Do stworzenia plikÃ³w yaml uÅ¼yÅ‚em AI*

Plik w tasks/main

```bash
---
# tasks file for cjson
- name: "Adding Docker repo"
  get_url:
    url: https://download.docker.com/linux/fedora/docker-ce.repo
    dest: /etc/yum.repos.d/docker-ce.repo

- name: "Installing Docker"
  dnf:
    name: docker-ce
    state: present

- name: "Startin Docker service"
  service:
    name: docker
    state: started
    enabled: true

- name: "Installing Python3 required by ansible docker modules"
  dnf:
    name:
      - python3-pip
    state: present

- name: "Installing Python modules"
  pip:
    name:
      - packing
      - docker
      - requests

- name: "Pulling app container image"
  docker_image:
    name: "{{ container_name }}"
    image: "{{ dockerhub_image }}"
    state: started

- name: "Checking if containar is running"
  command: docker ps -a
  register: container_list

- name: "Displaying containers"
  debug:
    var: container_list.stdout_lines

- name: "Stopping and removing app container"
  docker_container:
    name: "{{ container_name }}"
    state: absent

```

Plik w vars/main

```bash
---
# vars file for cjson
dockerhub_image: zboro02/cjson-deploy
container_name: cjson_app


```


Plik cjson-deploy

```bash
---
- name: Deploy cjson
  hosts: endpoints
  become: true
  roles:
    - cjson

```

NastÄ™pnie uruchomiÅ‚em playbook <- wszystko przebiegÅ‚o pomyÅ›lnie

![Terminal](artefakt-ready.png)


# Pliki odpowiedzi dla wdroÅ¼eÅ„ nienadzorowanych

Ä†wiczenie rozpoczÄ…Å‚em od instalacji systemu Fedora 41 na nowej VM z pliku ISO Fedora_server_netinst-x86.

Po instalacji musiaÅ‚em wyciÄ…gnÄ…Ä‡ plik anaconda znajdujacy siÄ™ w /root/anaconda-ks.cfg

ZrobiÅ‚em to za pomocÄ… SSH poprzez FileZilla.

UmieÅ›ciÅ‚em go na repo na Github, z ktÃ³rego potem realizowaÅ‚em nienadzorowane instalacje.

Jeszcze przed instalacjÄ… w ekranie wyboru systemu naleÅ¼aÅ‚o wcisnÄ…Ä‡ e Å¼eby przejÅ›Ä‡ do GRUBA.

NastÄ™pnie w Å›cieÅ¼ce po quiet naleÅ¼aÅ‚o podaÄ‡ Å›cieÅ¼kÄ™ do pliku raw anaconda na repo na Github

Dla mnie byÅ‚a ona taka:
inst.ks=https://raw.githubusercontent.com/InzynieriaOprogramowaniaAGH/MDO2025_INO/JZ415998v2/ITE/GCL08/JZ415998/Sprawko3original/anaconda-ks.cfg

![Terminal](grub.png)


Plik anaconda wyglÄ…daÅ‚ wtedy tak:

```bash
#version=DEVEL
text
keyboard --xlayouts='pl'
lang pl_PL.UTF-8

network --bootproto=dhcp --device=link --activate
network --hostname=fedora-pipeline-vm  

url --mirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=fedora-41&arch=x86_64 
repo --name=updates --mirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=updates-released-f41&arch=x86_64 

rootpw --plaintext TwojeSuperSilneHasloRoota  

timezone Europe/Warsaw --isUtc

bootloader --location=mbr --boot-drive=sda 

clearpart --all --initlabel 

autopart --type=lvm

reboot 

%packages
@^minimal-environment 
@core
wget
curl
git
%end

%post --log=/root/ks-post.log
echo "Kickstart post-installation script started"

echo "Kickstart post-installation script finished"
%end
```

PrzeprowadziÅ‚em instalacjÄ™, wszystko przebiegÅ‚o pomyÅ›lnie, system siÄ™ zainstalowaÅ‚.

![Terminal](kickstart.png)

![Terminal](sukces.png)

Potem rozszerzyÅ‚em plik o repozytoria i potrzebne programowanie do uruchomienia zbudowanego projektu - nginx

Nowa anaconda wyglÄ…da tak:

```bash
%packages
@^minimal-environment 
@core
wget
curl
git
dnf-plugins-core
%end

%post --log=/root/ks-post.log
echo "Kickstart post-installation script started"

echo "Installing Docker CE..."
dnf config-manager --add-repo https://download.docker.com/linux/fedora/docker-ce.repo
dnf install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin

echo "Enabling Docker service..."
systemctl enable docker.service

echo "Creating Nginx Docker container startup service..."
cat << EOF > /etc/systemd/system/run-nginx-container.service
[Unit]
Description=Run Nginx Docker Container on First Boot
Wants=network-online.target docker.service
After=network-online.target docker.service

ConditionPathExists=!/opt/nginx-container-started.flag

[Service]
Type=oneshot
RemainAfterExit=yes

ExecStartPre=/bin/sh -c 'count=0; until docker info >/dev/null 2>&1; do if [ "$count" -ge 12 ]; then echo "Docker did not start after 60s"; exit 1; fi; echo "Waiting for Docker daemon..."; sleep 5; count=$((count+1)); done'

ExecStart=/usr/bin/docker pull nginx:latest

ExecStart=/usr/bin/docker run --name nginx-server -d -p 80:80 --restart unless-stopped nginx:latest

ExecStartPost=/usr/bin/touch /opt/nginx-container-started.flag

[Install]
WantedBy=multi-user.target
EOF

systemctl enable run-nginx-container.service

echo "Kickstart post-installation script finished"
%end

reboot
```
ZadbaÅ‚em o automatyczne ponowne uruchomienie na koÅ„cu instalacji <- reboot


KrÃ³tki opis dziaÅ‚ania:

Ten skrypt Kickstart instaluje minimalny system operacyjny z podstawowymi narzÄ™dziami (wget, curl, git, dnf-plugins-core), a nastÄ™pnie w fazie post-instalacyjnej konfiguruje Å›rodowisko Docker. Dodaje repozytorium Dockera, instaluje Docker CE wraz z dodatkowymi komponentami, a nastÄ™pnie wÅ‚Ä…cza usÅ‚ugÄ™ Dockera. Dodatkowo tworzy jednostkÄ™ systemd o nazwie run-nginx-container.service, ktÃ³ra przy pierwszym uruchomieniu systemu czeka na dostÄ™pnoÅ›Ä‡ demona Dockera, pobiera obraz nginx:latest i uruchamia kontener Nginx nasÅ‚uchujÄ…cy na porcie 80 z politykÄ… automatycznego restartu.

PonowiÅ‚em instalacjÄ™ dla nowej anacondy, zalogowaÅ‚em siÄ™.

SprawdziÅ‚em, czy Docker dziaÅ‚a

```bash
sudo systemctl status docker
```

Efekt -> active

Potem sprawdziÅ‚em stan usÅ‚ugi

```bash
sudo systemctl status run-nginx-container.service
```

NastÄ™pnie sprawdziÅ‚em, czy kontener nginx dziaÅ‚a komendÄ…

```bash
sudo docker ps
```
I test koÅ„cowy, po wpisaniu adresu IP maszyny w przeglÄ…darce otrzymujemy domyÅ›lnÄ… powitalnÄ… stronÄ™ Nginx - Welcome to Nginx

![Terminal](nginxfinish.png)




# Kubernetes - 1


ZaczÄ…Å‚em od aktualizacji systemu

```bash
sudo dnf update -y
```

ZainstalowaÅ‚em potrzebne narzÄ™dzia

``` bash
sudo dnf install -y curl git conntrack-tools
```

Instalacja Dockera pod Minikube. *Fedora domyÅ›lnie uÅ¼ywa Podmana*

```bash
sudo dnf install -y moby-engine docker-compose 
sudo systemctl enable --now docker
sudo usermod -aG docker $USER
```

PobraÅ‚em Minikube
```bash
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
```
![Terminal](instalacjaminikube.png)

Instalacja Minikube i usuniÄ™cie pliku

```bash
sudo install minikube-linux-amd64 /usr/local/bin/minikube
rm minikube-linux-amd64
```

Poziom bezpieczeÅ„stwa instalacji:

- Pobieram Minikube bezpoÅ›rednio z storage.googleapis.com, ktÃ³re jest oficjalnym repozytorium udostÄ™pnianym przez projekt Kubernetes. Jest to zaufane ÅºrÃ³dÅ‚o.

- Dla peÅ‚nego bezpieczeÅ„stwa mÃ³gÅ‚bym pobraÄ‡ sumÄ™ kontrolnÄ… SHA256 dla pliku i zweryfikowaÄ‡ jÄ…. (np. curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64.sha256 a nastÄ™pnie sha256sum -c minikube-linux-amd64.sha256). W praktyce dla szybkiej instalacji czÄ™sto siÄ™ to pomija, ufajÄ…c HTTPS i oficjalnemu ÅºrÃ³dÅ‚u.

- Komenda sudo install umieszcza plik wykonywalny w /usr/local/bin/, co jest standardowÄ… lokalizacjÄ… dla oprogramowania instalowanego przez uÅ¼ytkownika, dostÄ™pnego dla wszystkich uÅ¼ytkownikÃ³w systemu. Sam Minikube, gdy jest uruchamiany, dziaÅ‚a z uprawnieniami uÅ¼ytkownika, ktÃ³ry go uruchamia, chyba Å¼e specyficzne operacje (jak konfiguracja sterownika VM) wymagajÄ… sudo.

ZaoptrzyÅ‚em siÄ™ w polecenie kubectl

```bash

minikube kubectl --

```

StworzyÅ‚em alias 

![Terminal](alias.png)


UruchomiÅ‚em Kubernetes ze sterownikiem Docker

```bash
minikube start --driver=docker
```
![Terminal](sterownik.png)


Minikube status

![Terminal](status.png)

DziaÅ‚ajÄ…cy kontener / worker

![Terminal](worker.png)

Wykaz podstawowych podÃ³ dziaÅ‚ajacych w klastrze

![Terminal](wykaz.png)


Nie mitygowaÅ‚em problemÃ³w wynikajÄ…cych z wymagaÅ„ sprzÄ™towych poniewaÅ¼ takowych nie miaÅ‚em

Minikube domyÅ›lnie stara siÄ™ byÄ‡ oszczÄ™dny, ale ma pewne minimalne wymagania:
- CPU: Zalecane 2+, Minikube domyÅ›lnie przydzieli 2.
- PamiÄ™Ä‡ RAM: Zalecane 2GB+, Minikube domyÅ›lnie przydzieli ok. 2GB (moÅ¼e byÄ‡ wiÄ™cej w zaleÅ¼noÅ›ci od wersji).
- Miejsce na dysku: Zalecane 20GB+ wolnego miejsca.

Uruchomienie dashboardu Kubernetesa

![Terminal](dashboard.png)

Dashboard w przeglÄ…darce

![Terminal](dashboardp.png)

UtworzyÅ‚em katalog dla projektu i pliku HTML

```bash
mkdir ~/my-k8s-app
cd ~/my-k8s-app
mkdir html
echo "<h1>Witaj w mojej aplikacji na Kubernetes</h1> <p>DziaÅ‚a w Minikube na Fedorze 41</p>" > html/index.html
```

StworzyÅ‚em tam Dockerfile o treÅ›ci:

```bash
FROM nginx:alpine
RUN rm -rf /usr/share/nginx/html/*
COPY ./html/ /usr/share/nginx/html/
EXPOSE 80
CMD ["nginx", "-g", "daemon off;"]
```

ZbudowaÅ‚em obraz oczywiÅ›cie bÄ™dÄ…c ciagle w katalogu

```bash
docker build -t my-custom-nginx:v1 .
```

![Terminal](budowa.png)


SprawdziÅ‚em, czy kontener dziaÅ‚a lokalnie - poza Kubernetesem

![Terminal](sprawdzenie.png)

SprawdziÅ‚em, czy dziaÅ‚a i jak wyÅ›wietla siÄ™ w przeglÄ…darce

```bash
curl localhost:8080
```

![Terminal](sprawdzenie2.png)

Sprawdzenie czy kontener dziaÅ‚a czy moÅ¼e zakoÅ„czyÅ‚ pracÄ™

![Terminal](dziala.png)

ZaÅ‚adowaÅ‚em lokalny obraz Docker do Minikube

```bash
minikube image load my-custom-nginx:v1
```

![Terminal](zaÅ‚adowanie.png)

UruchomiÅ‚em kontener jako deployment

```bash
minikubectl run my-nginx-deployment --image=my-custom-nginx:v1 --port=80 --labels app=my-nginx-app
```

Pod dziaÅ‚a via kubectl

![Terminal](pod1.png)

Pod dziaÅ‚a via dashboard

![Terminal](pod2.png)

PrzekierowaÅ‚em port 

```bash
minikubectl port-forward pod/my-nginx-deployment-5bcf779c4c-abcde 8081:80
```

Przekierowanie dziaÅ‚a:

```bash
curl localhost:8081
```

![Terminal](8081.png)

WyeksportowaÅ‚em deployment do pliku yaml

```bash
minikubectl get deployment my-nginx-deployment -o yaml > ~/my-k8s-app/my-nginx-deployment.yaml
```

ZawieraÅ‚ on wiele pÃ³l, ktÃ³re nie byÅ‚y potrzebne wiÄ™c usunÄ…Å‚em je i stworzyÅ‚em plik:

```bash
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: my-nginx-app 
  name: my-nginx-deployment
spec:
  replicas: 1 
  selector:
    matchLabels:
      app: my-nginx-app 
  template:
    metadata:
      labels:
        app: my-nginx-app 
    spec:
      containers:
      - image: my-custom-nginx:v1
        name: my-custom-nginx-container 
        ports:
        - containerPort: 80

```


WdroÅ¼yÅ‚em aplikacjÄ™ za pomocÄ… kubectl apply

![Terminal](wdroÅ¼enie.png)

ğŸŒµ Upewnij siÄ™, Å¼e posiadasz wdroÅ¼enie zapisane jako plik

Tak, plik ~/my-k8s-app/my-nginx-deployment.yaml jest naszym zapisanym wdroÅ¼eniem.

WzbogaciÅ‚em deploment o 4 repliki

Zmiana replicas: 4 # Zmienione z 1 na 4

WdroÅ¼enie ponownie za pomocÄ… kubectl apply i sprawdzenie statusu kubectl rollout status

![Terminal](repliki.png)

Sprawdzenie liczby podÃ³w

```bash
minikubectl get pods -l app=my-nginx-app
```

UtworzyÅ‚em serwis za pomocÄ… expose

```bash
minikubectl expose deployment my-nginx-deployment --type=NodePort --port=80 --target-port=80 --name=my-nginx-service
```

![Terminal](serwis.png)

WykonaÅ‚em rÄ™czne przekirowanie portu do serwisu

```bash
minikubectl port-forward service/my-nginx-service 8082:80
```
I odpaliÅ‚em w przeglÄ…darce

```bash
curl localhost:8082
```

DziaÅ‚a

![Terminal](8082.png)


# Kubernetes 2

PowtÃ³rnie dziaÅ‚am na nginx

PrzygotowaÅ‚em 2 nowe obrazy html_v1 i html_v2

```bash
mkdir html_v1
echo "<h1>Witaj w Nginx v1!</h1> <p>To jest wersja 1 mojej aplikacji.</p>" > html_v1/index.html
mkdir html_v2
echo "<h1>Witaj w Nginx v2!</h1> <p>To jest NOWSZA wersja 2 mojej aplikacji.</p>" > html_v2/index.html
```

![Terminal](2wersje.png)

StworzyÅ‚em dockerfile.nginx dla v1 i v2

```bash
FROM nginx:alpine
ARG HTML_DIR=html_default
RUN rm -rf /usr/share/nginx/html/*
COPY ./${HTML_DIR}/ /usr/share/nginx/html/
EXPOSE 80
CMD ["nginx", "-g", "daemon off;"]
```

NastÄ™pnie zalogowaÅ‚em sie do Dockerhub a potem zbudowaÅ‚em i wypchnÄ…Å‚em v1 i v2

```bash
docker build -t YOUR_DOCKERHUB_USERNAME/my-custom-nginx:v1 --build-arg HTML_DIR=html_v1 -f Dockerfile.nginx .
docker push YOUR_DOCKERHUB_USERNAME/my-custom-nginx:v1

docker build -t YOUR_DOCKERHUB_USERNAME/my-custom-nginx:v2 --build-arg HTML_DIR=html_v2 -f Dockerfile.nginx .
docker push YOUR_DOCKERHUB_USERNAME/my-custom-nginx:v2

```
![Terminal](budowaiwypych.png)

PrzygotowaÅ‚em trzeci wadliwy obraz:

Dockerfile.broken

```bash
FROM alpine:latest
CMD ["/bin/false"]
```

ZbudowaÅ‚em go i wypchnÄ…Å‚em

```bash
docker build -t YOUR_DOCKERHUB_USERNAME/my-custom-nginx:v3-broken -f Dockerfile.broken .
docker push YOUR_DOCKERHUB_USERNAME/my-custom-nginx:v3-broken
```

![Terminal](broken.png)

Obrazy znalazÅ‚y siÄ™ na Dockerhub

![Terminal](dockerhub.png)

StworzyÅ‚em plik do deploymentu my-nginx-deployment2.yaml

```bash
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-nginx-deployment2
  labels:
    app: my-nginx-app
spec:
  replicas: 4
  selector:
    matchLabels:
      app: my-nginx-app
  template:
    metadata:
      labels:
        app: my-nginx-app
    spec:
      containers:
      - name: my-custom-nginx-container
        image: my-custom-nginx:v1
        ports:
        - containerPort: 80

```

Potem go skonfigurowaÅ‚em dla obrazu v1

```bash
minikubectl apply -f ~/my-k8s-app/my-nginx-deployment2.yaml
minikubectl rollout status deployment/my-nginx-deployment2
minikubectl get pods -l app=my-nginx-app
```

![Terminal](4.png)


ZwiÄ™kszyÅ‚em iloÅ›Ä‡ replik do 8 replicas: 4 -> 8

I skonfigurowaÅ‚em ponownie

![Terminal](4-8.png)

Potem do 1 repliki

![Terminal](8-1.png)

Potem do 0 replik

![Terminal](1-0.png)

Wszystkie Pody powinny zostaÄ‡ usuniÄ™te. Deployment nadal istnieje, ale nie zarzÄ…dza Å¼adnymi Podami.

PÃ³Åºniej ponownie wrÃ³ciÅ‚em do 4 replik

ZastosowaÅ‚em nowÄ… wersjÄ™ obrazu czyli dla obrazu v2

```bash
minikubectl apply -f ~/my-k8s-app/my-nginx-deployment2.yaml
minikubectl rollout status deployment/my-nginx-deployment2

```

![Terminal](nowav2.png)

PowrtÃ³t do starszej wersji obrazu v1

```bash
minikubectl apply -f ~/my-k8s-app/my-nginx-deployment2.yaml
minikubectl rollout status deployment/my-nginx-deployment2
```

![Terminal](powrÃ³t.png)


Potem zastosowaÅ‚em wadliwy obraz v3

Standardowo podmieniÅ‚em my-nginx-deployment.yaml na v3

```bash
minikubectl apply -f ~/my-nginx-deployment2.yaml
```

OdpaliÅ‚em rollout status i ...

```bash
minikubectl rollout status deployment/my-nginx-deployment2 --watch 
minikubectl get pods -l app=my-nginx-app -w

```
... i otrzymaÅ‚em CrashLoopBackOff, nie czekaÅ‚em na skoÅ„czenie siÄ™ Rollout mogÅ‚o to zajÄ…Ä‡ dÅ‚ugo lub siÄ™ zawiesiÄ‡, wiec po niedÅ‚ugim czasie uÅ¼yÅ‚em ctr+C

![Terminal](bÅ‚Ä…dv3.png)

NastÄ™pnie przeszedÅ‚em do przywracania poprzednich wersji za pomocÄ… poleceÅ„:

kubectl rollout history <- pokazuje listÄ™ rewizji
kubectl rollout undo <- pozwala wycofaÄ‡ siÄ™ do poprzedniej dziaÅ‚ajÄ…cej wersji

```bash
minikubectl rollout history deployment/my-nginx-deployment2

```
UÅ¼ylismy tego aby cofnÄ…Ä‡ siÄ™ przed wadliwÄ… wersje 

![Terminal](undo.png)

Potem stworzyÅ‚em skrypt weryfikujÄ…cy, czy wdroÅ¼enie "zdÄ…Å¼yÅ‚o" siÄ™ wdroÅ¼yÄ‡ (60 sekund) w pliku check_deployment.sh

*Do tego wykorzystaÅ‚em AI*

```bash

#!/bin/bash

DEPLOYMENT_NAME="my-nginx-deployment2"
NAMESPACE="default" 
TIMEOUT_SECONDS=60

echo "Sprawdzanie statusu wdroÅ¼enia $DEPLOYMENT_NAME w przestrzeni nazw $NAMESPACE..."


if minikube kubectl rollout status deployment/$DEPLOYMENT_NAME -n $NAMESPACE --timeout=${TIMEOUT_SECONDS}s; then
  echo "WdroÅ¼enie $DEPLOYMENT_NAME zakoÅ„czone pomyÅ›lnie."

  
  DESIRED_REPLICAS=$(minikube kubectl get deployment/$DEPLOYMENT_NAME -n $NAMESPACE -o jsonpath='{.spec.replicas}')
  READY_REPLICAS=$(minikube kubectl get deployment/$DEPLOYMENT_NAME -n $NAMESPACE -o jsonpath='{.status.readyReplicas}')

  if [[ "$DESIRED_REPLICAS" == "$READY_REPLICAS" && "$READY_REPLICAS" -gt "0" ]]; then
    echo "Wszystkie $READY_REPLICAS repliki sÄ… gotowe."
    exit 0
  elif [[ "$DESIRED_REPLICAS" == "0" && "$READY_REPLICAS" == "" || "$READY_REPLICAS" == "0" ]]; then
    echo "WdroÅ¼enie przeskalowane do 0 replik, co jest zgodne z oczekiwaniami."
    exit 0
  else
    echo "BÅ‚Ä…d: Liczba gotowych replik ($READY_REPLICAS) nie zgadza siÄ™ z poÅ¼Ä…danÄ… ($DESIRED_REPLICAS) lub jest zerowa."
    exit 1
  fi
else
  echo "BÅ‚Ä…d: WdroÅ¼enie $DEPLOYMENT_NAME nie zakoÅ„czyÅ‚o siÄ™ pomyÅ›lnie w ciÄ…gu $TIMEOUT_SECONDS sekund."
  
  minikube kubectl get pods -n $NAMESPACE -l app=my-nginx-app
  exit 1
fi
```

NadaÅ‚em uprawnienia do wykonywania i uruchomiÅ‚em

```bash
chmod +x ~/my-k8s-app/check_deployment.sh
~/my-k8s-app/check_deployment.sh
```
Skrypt sprawdzi status wdroÅ¼enia. JeÅ›li wdroÅ¼enie uÅ¼ywa wadliwego obrazu, skrypt powinien zgÅ‚osiÄ‡ bÅ‚Ä…d po upÅ‚ywie timeoutu.

## Strategie wdroÅ¼enia

PrzywrÃ³ciÅ‚em deployment do wersji stabilnej aby mieÄ‡ bazÄ™ do testowania strategii

- 4 repliki
- RollingUpdate
- rollingUpdate:
    maxSurge: 25%
    maxUnavailable: 25%


![Terminal](rollout.png)


UruchomiÅ‚em

```bash

minikubectl apply -f ~/my-k8s-app/my-nginx-deployment2.yaml
minikubectl rollout status deployment/my-nginx-deployment2

```

Modyfikacja dla strategii recreate, zmieniamy type na Recreate 

![Terminal](recreate.png)


Zastosowanie

```bash
minikubectl apply -f ~/my-k8s-app/my-nginx-deployment2.yaml

```
![Terminal](srecreate.png)

Obserwacja: Wszystkie stare Pody (z v2) zostanÄ… natychmiast zakoÅ„czone. Dopiero po ich znikniÄ™ciu zacznÄ… byÄ‡ tworzone nowe Pody (z v1). Powoduje to chwilowy przestÃ³j aplikacji.

Teraz testujemy Rolling Update z parametrami maxUnavailable > 1, maxSurge > 20%

![Terminal](update.png)

Zastosowanie

```bash
minikubectl apply -f ~/my-k8s-app/my-nginx-deployment2.yaml
minikubectl rollout status deployment/my-nginx-deployment2 --watch
```
Obserwacja: Kubernetes bÄ™dzie stopniowo wymieniaÅ‚ Pody. Najpierw moÅ¼e utworzyÄ‡ do 2 nowych PodÃ³w (maxSurge: 2), a nastÄ™pnie zacznie usuwaÄ‡ stare, upewniajÄ…c siÄ™, Å¼e nie wiÄ™cej niÅ¼ 1 Pod jest niedostÄ™pny (maxUnavailable: 1). Aktualizacja odbywa siÄ™ bez przestoju.

## Strategia Canary Deploment 

Canary jest bardziej zÅ‚oÅ¼one i zwykle wymaga dwÃ³ch DeploymentÃ³w lub narzÄ™dzi do zarzÄ…dzania ruchem (np. Istio, Flagger). ZrobiÅ‚em uproszczonÄ… wersjÄ™ z dwoma Deploymentami i jednym Serwisem.

PrzygotowaÅ‚em plik my-nginx-deployment-stable.yaml

```bash
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-nginx-stable
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-nginx-app 
      version: v1       
  template:
    metadata:
      labels:
        app: my-nginx-app 
        version: v1
    spec:
      containers:
      - name: nginx-stable-container
        image: YOUR_DOCKERHUB_USERNAME/my-custom-nginx:v1
        ports:
        - containerPort: 80
        imagePullPolicy: Always
```

UsunÄ…Å‚em stary deployment i zastosowaÅ‚em nowy Stable

```bash
minikubectl delete deployment my-nginx-deployment2 --ignore-not-found=true
minikubectl apply -f ~/my-k8s-app/my-nginx-deployment-stable.yaml
minikubectl rollout status deployment/my-nginx-stable
```

PrzygotowaÅ‚em plik dla Canary Deployment ten drugi my-nginx-deployment-canary.yaml

```bash
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-nginx-canary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-nginx-app 
      version: v2       
  template:
    metadata:
      labels:
        app: my-nginx-app 
        version: v2
    spec:
      containers:
      - name: nginx-canary-container
        image: YOUR_DOCKERHUB_USERNAME/my-custom-nginx:v2
        ports:
        - containerPort: 80
        imagePullPolicy: Always
```

ZastosowaÅ‚em Canary Deployment

```bash
minikubectl apply -f ~/my-k8s-app/my-nginx-deployment-canary.yaml
minikubectl rollout status deployment/my-nginx-canary
```

UtworzyÅ‚em serwis my-nginx-service, ktÃ³ry selekcjonuje Pody na podstawie etykiety app: my-nginx-app.

```bash
apiVersion: v1
kind: Service
metadata:
  name: my-nginx-service
spec:
  selector:
    app: my-nginx-app 
  ports:
    - protocol: TCP
      port: 80        
      targetPort: 80  
  type: NodePort      

```

ZastosowaÅ‚em serwis

```bash
minikubectl apply -f ~/my-k8s-app/my-nginx-service.yaml
```

I na koniec przetestowaÅ‚em wszystko

```bash
minikube service my-nginx-service
```

25% ruchu trafia do wersji Canary

Obserwacja: Canary deployment pozwala na skierowanie niewielkiej czÄ™Å›ci ruchu produkcyjnego do nowej wersji aplikacji. JeÅ›li wersja canary dziaÅ‚a poprawnie, moÅ¼na stopniowo zwiÄ™kszaÄ‡ liczbÄ™ jej replik (i zmniejszaÄ‡ repliki wersji stabilnej), aÅ¼ caÅ‚a produkcja przejdzie na nowÄ… wersjÄ™. JeÅ›li sÄ… problemy, wersjÄ™ canary moÅ¼na szybko wycofaÄ‡, minimalizujÄ…c wpÅ‚yw na uÅ¼ytkownikÃ³w.

Serwis zapewnia pojedynczy punkt dostÄ™pu do wielu replik PodÃ³w, rozkÅ‚adajÄ…c miÄ™dzy nimi ruch (load balancing). Jest to niezbÄ™dne dla aplikacji z wieloma replikami.