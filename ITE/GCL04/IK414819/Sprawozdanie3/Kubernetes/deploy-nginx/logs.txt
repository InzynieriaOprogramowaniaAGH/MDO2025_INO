
==> Audit <==
|-----------|-----------------------|----------|------|---------|----------------------|----------------------|
|  Command  |         Args          | Profile  | User | Version |      Start Time      |       End Time       |
|-----------|-----------------------|----------|------|---------|----------------------|----------------------|
| start     | --driver=docker       | minikube | igor | v1.35.0 | 20 May 25 17:56 CEST | 20 May 25 18:00 CEST |
| service   | hello-minikube        | minikube | igor | v1.35.0 | 20 May 25 18:00 CEST |                      |
| start     | --driver=docker       | minikube | igor | v1.35.0 | 20 May 25 18:03 CEST | 20 May 25 18:03 CEST |
| dashboard | --url                 | minikube | igor | v1.35.0 | 20 May 25 18:14 CEST |                      |
| addons    | enable metrics-server | minikube | igor | v1.35.0 | 20 May 25 18:22 CEST | 20 May 25 18:22 CEST |
| dashboard | --url                 | minikube | igor | v1.35.0 | 27 May 25 17:18 CEST |                      |
| start     |                       | minikube | igor | v1.35.0 | 27 May 25 17:18 CEST | 27 May 25 17:19 CEST |
| dashboard | --url                 | minikube | igor | v1.35.0 | 27 May 25 17:20 CEST |                      |
| service   | nginx-deploy          | minikube | igor | v1.35.0 | 27 May 25 17:32 CEST |                      |
| image     | load my-nginx         | minikube | igor | v1.35.0 | 27 May 25 17:35 CEST | 27 May 25 17:37 CEST |
| service   | nginx-deploy          | minikube | igor | v1.35.0 | 27 May 25 17:37 CEST |                      |
| service   | nginx-deploy          | minikube | igor | v1.35.0 | 27 May 25 17:38 CEST |                      |
|-----------|-----------------------|----------|------|---------|----------------------|----------------------|


==> Last Start <==
Log file created at: 2025/05/27 17:18:34
Running on machine: ansible-host
Binary: Built with gc go1.23.4 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0527 17:18:34.106573    4981 out.go:345] Setting OutFile to fd 1 ...
I0527 17:18:34.107250    4981 out.go:397] isatty.IsTerminal(1) = true
I0527 17:18:34.107262    4981 out.go:358] Setting ErrFile to fd 2...
I0527 17:18:34.107277    4981 out.go:397] isatty.IsTerminal(2) = true
I0527 17:18:34.108022    4981 root.go:338] Updating PATH: /home/igor/.minikube/bin
W0527 17:18:34.108685    4981 root.go:314] Error reading config file at /home/igor/.minikube/config/config.json: open /home/igor/.minikube/config/config.json: no such file or directory
I0527 17:18:34.110334    4981 out.go:352] Setting JSON to false
I0527 17:18:34.164503    4981 start.go:129] hostinfo: {"hostname":"ansible-host","uptime":405,"bootTime":1748358709,"procs":157,"os":"linux","platform":"fedora","platformFamily":"fedora","platformVersion":"41","kernelVersion":"6.14.5-200.fc41.x86_64","kernelArch":"x86_64","virtualizationSystem":"vbox","virtualizationRole":"guest","hostId":"c63f55d1-db16-47f1-9a59-a46d9da6451b"}
I0527 17:18:34.164565    4981 start.go:139] virtualization: vbox guest
I0527 17:18:34.177684    4981 out.go:177] üòÑ  minikube v1.35.0 on Fedora 41 (vbox/amd64)
I0527 17:18:34.194414    4981 notify.go:220] Checking for updates...
I0527 17:18:34.195107    4981 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0527 17:18:34.195226    4981 driver.go:394] Setting default libvirt URI to qemu:///system
I0527 17:18:34.402570    4981 lock.go:35] WriteFile acquiring /home/igor/.minikube/last_update_check: {Name:mkfcd3a54962f97b684a20272710134ecac9ce56 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0527 17:18:34.424515    4981 out.go:177] üéâ  minikube 1.36.0 is available! Download it: https://github.com/kubernetes/minikube/releases/tag/v1.36.0
I0527 17:18:34.445719    4981 out.go:177] üí°  To disable this notice, run: 'minikube config set WantUpdateNotification false'

I0527 17:18:34.813750    4981 docker.go:123] docker version: linux-27.3.1:
I0527 17:18:34.814418    4981 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0527 17:18:35.662161    4981 info.go:266] docker info: {ID:f1674a69-a709-48ea-8ee0-7a6f69fe2796 Containers:19 ContainersRunning:2 ContainersPaused:0 ContainersStopped:17 Images:11 Driver:overlay2 DriverStatus:[[Backing Filesystem btrfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:44 OomKillDisable:false NGoroutines:53 SystemTime:2025-05-27 17:18:35.529650552 +0200 CEST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.14.5-200.fc41.x86_64 OperatingSystem:Fedora Linux 41 (Forty One) OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:2 MemTotal:4093759488 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:ansible-host Labels:[] ExperimentalBuild:false ServerVersion:27.3.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:/usr/bin/tini-static ContainerdCommit:{ID:1.fc41 Expected:1.fc41} RuncCommit:{ID: Expected:} InitCommit:{ID: Expected:} SecurityOptions:[name=seccomp,profile=builtin name=selinux name=cgroupns] ProductLicense: Warnings:[WARNING: bridge-nf-call-iptables is disabled WARNING: bridge-nf-call-ip6tables is disabled] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:0.18.0]] Warnings:<nil>}}
I0527 17:18:35.664689    4981 docker.go:318] overlay module found
I0527 17:18:35.685411    4981 out.go:177] ‚ú®  Using the docker driver based on existing profile
I0527 17:18:35.718537    4981 start.go:297] selected driver: docker
I0527 17:18:35.718547    4981 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/igor:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0527 17:18:35.718677    4981 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0527 17:18:35.719130    4981 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0527 17:18:35.835890    4981 info.go:266] docker info: {ID:f1674a69-a709-48ea-8ee0-7a6f69fe2796 Containers:19 ContainersRunning:2 ContainersPaused:0 ContainersStopped:17 Images:11 Driver:overlay2 DriverStatus:[[Backing Filesystem btrfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:44 OomKillDisable:false NGoroutines:53 SystemTime:2025-05-27 17:18:35.80307482 +0200 CEST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.14.5-200.fc41.x86_64 OperatingSystem:Fedora Linux 41 (Forty One) OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:2 MemTotal:4093759488 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:ansible-host Labels:[] ExperimentalBuild:false ServerVersion:27.3.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:/usr/bin/tini-static ContainerdCommit:{ID:1.fc41 Expected:1.fc41} RuncCommit:{ID: Expected:} InitCommit:{ID: Expected:} SecurityOptions:[name=seccomp,profile=builtin name=selinux name=cgroupns] ProductLicense: Warnings:[WARNING: bridge-nf-call-iptables is disabled WARNING: bridge-nf-call-ip6tables is disabled] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:0.18.0]] Warnings:<nil>}}
I0527 17:18:35.840388    4981 cni.go:84] Creating CNI manager for ""
I0527 17:18:35.840479    4981 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0527 17:18:35.840552    4981 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/igor:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0527 17:18:35.856545    4981 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0527 17:18:35.872446    4981 cache.go:121] Beginning downloading kic base image for docker with docker
I0527 17:18:35.891220    4981 out.go:177] üöú  Pulling base image v0.0.46 ...
I0527 17:18:35.908056    4981 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0527 17:18:35.907937    4981 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0527 17:18:35.912836    4981 preload.go:146] Found local preload: /home/igor/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0527 17:18:35.912849    4981 cache.go:56] Caching tarball of preloaded images
I0527 17:18:35.913169    4981 preload.go:172] Found /home/igor/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0527 17:18:35.913177    4981 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0527 17:18:35.913397    4981 profile.go:143] Saving config to /home/igor/.minikube/profiles/minikube/config.json ...
I0527 17:18:36.073031    4981 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon, skipping pull
I0527 17:18:36.073094    4981 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 exists in daemon, skipping load
I0527 17:18:36.073121    4981 cache.go:227] Successfully downloaded all kic artifacts
I0527 17:18:36.073167    4981 start.go:360] acquireMachinesLock for minikube: {Name:mke12636dd7d0bd7db399b0b762ae1bd266c8433 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0527 17:18:36.073752    4981 start.go:364] duration metric: took 152.531¬µs to acquireMachinesLock for "minikube"
I0527 17:18:36.073793    4981 start.go:96] Skipping create...Using existing machine configuration
I0527 17:18:36.073802    4981 fix.go:54] fixHost starting: 
I0527 17:18:36.075087    4981 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0527 17:18:36.152539    4981 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0527 17:18:36.152583    4981 fix.go:138] unexpected machine state, will restart: <nil>
I0527 17:18:36.176242    4981 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I0527 17:18:36.196471    4981 cli_runner.go:164] Run: docker start minikube
I0527 17:18:41.714078    4981 cli_runner.go:217] Completed: docker start minikube: (5.517562881s)
I0527 17:18:41.714219    4981 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0527 17:18:41.759084    4981 kic.go:430] container "minikube" state is running.
I0527 17:18:41.763416    4981 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0527 17:18:41.811017    4981 profile.go:143] Saving config to /home/igor/.minikube/profiles/minikube/config.json ...
I0527 17:18:41.811451    4981 machine.go:93] provisionDockerMachine start ...
I0527 17:18:41.811520    4981 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0527 17:18:42.283050    4981 main.go:141] libmachine: Using SSH client type: native
I0527 17:18:42.285511    4981 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0527 17:18:42.285520    4981 main.go:141] libmachine: About to run SSH command:
hostname
I0527 17:18:42.325107    4981 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:33122->127.0.0.1:32768: read: connection reset by peer
I0527 17:18:48.276670    4981 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0527 17:18:48.276684    4981 ubuntu.go:169] provisioning hostname "minikube"
I0527 17:18:48.276745    4981 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0527 17:18:48.431853    4981 main.go:141] libmachine: Using SSH client type: native
I0527 17:18:48.432094    4981 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0527 17:18:48.432099    4981 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0527 17:18:51.825842    4981 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0527 17:18:51.825949    4981 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0527 17:18:52.033547    4981 main.go:141] libmachine: Using SSH client type: native
I0527 17:18:52.033835    4981 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0527 17:18:52.033857    4981 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0527 17:18:52.288949    4981 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0527 17:18:52.288966    4981 ubuntu.go:175] set auth options {CertDir:/home/igor/.minikube CaCertPath:/home/igor/.minikube/certs/ca.pem CaPrivateKeyPath:/home/igor/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/igor/.minikube/machines/server.pem ServerKeyPath:/home/igor/.minikube/machines/server-key.pem ClientKeyPath:/home/igor/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/igor/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/igor/.minikube}
I0527 17:18:52.288987    4981 ubuntu.go:177] setting up certificates
I0527 17:18:52.288993    4981 provision.go:84] configureAuth start
I0527 17:18:52.289149    4981 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0527 17:18:52.348091    4981 provision.go:143] copyHostCerts
I0527 17:18:52.349542    4981 exec_runner.go:144] found /home/igor/.minikube/ca.pem, removing ...
I0527 17:18:52.349570    4981 exec_runner.go:203] rm: /home/igor/.minikube/ca.pem
I0527 17:18:52.350540    4981 exec_runner.go:151] cp: /home/igor/.minikube/certs/ca.pem --> /home/igor/.minikube/ca.pem (1070 bytes)
I0527 17:18:52.353493    4981 exec_runner.go:144] found /home/igor/.minikube/cert.pem, removing ...
I0527 17:18:52.353512    4981 exec_runner.go:203] rm: /home/igor/.minikube/cert.pem
I0527 17:18:52.353551    4981 exec_runner.go:151] cp: /home/igor/.minikube/certs/cert.pem --> /home/igor/.minikube/cert.pem (1115 bytes)
I0527 17:18:52.354004    4981 exec_runner.go:144] found /home/igor/.minikube/key.pem, removing ...
I0527 17:18:52.354010    4981 exec_runner.go:203] rm: /home/igor/.minikube/key.pem
I0527 17:18:52.354040    4981 exec_runner.go:151] cp: /home/igor/.minikube/certs/key.pem --> /home/igor/.minikube/key.pem (1679 bytes)
I0527 17:18:52.354700    4981 provision.go:117] generating server cert: /home/igor/.minikube/machines/server.pem ca-key=/home/igor/.minikube/certs/ca.pem private-key=/home/igor/.minikube/certs/ca-key.pem org=igor.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0527 17:18:52.582165    4981 provision.go:177] copyRemoteCerts
I0527 17:18:52.583742    4981 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0527 17:18:52.583812    4981 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0527 17:18:52.657603    4981 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/igor/.minikube/machines/minikube/id_rsa Username:docker}
I0527 17:18:52.821137    4981 ssh_runner.go:362] scp /home/igor/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1070 bytes)
I0527 17:18:52.938034    4981 ssh_runner.go:362] scp /home/igor/.minikube/machines/server.pem --> /etc/docker/server.pem (1172 bytes)
I0527 17:18:53.087274    4981 ssh_runner.go:362] scp /home/igor/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0527 17:18:53.350395    4981 provision.go:87] duration metric: took 1.061382948s to configureAuth
I0527 17:18:53.350429    4981 ubuntu.go:193] setting minikube options for container-runtime
I0527 17:18:53.350814    4981 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0527 17:18:53.350903    4981 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0527 17:18:53.448019    4981 main.go:141] libmachine: Using SSH client type: native
I0527 17:18:53.448611    4981 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0527 17:18:53.448629    4981 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0527 17:18:53.718156    4981 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0527 17:18:53.718167    4981 ubuntu.go:71] root file system type: overlay
I0527 17:18:53.718283    4981 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0527 17:18:53.718360    4981 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0527 17:18:53.847778    4981 main.go:141] libmachine: Using SSH client type: native
I0527 17:18:53.847991    4981 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0527 17:18:53.848111    4981 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0527 17:18:54.218955    4981 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0527 17:18:54.219069    4981 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0527 17:18:54.267657    4981 main.go:141] libmachine: Using SSH client type: native
I0527 17:18:54.267782    4981 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0527 17:18:54.267791    4981 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0527 17:18:54.541293    4981 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0527 17:18:54.541309    4981 machine.go:96] duration metric: took 12.729848239s to provisionDockerMachine
I0527 17:18:54.541320    4981 start.go:293] postStartSetup for "minikube" (driver="docker")
I0527 17:18:54.541331    4981 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0527 17:18:54.541397    4981 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0527 17:18:54.541423    4981 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0527 17:18:54.618532    4981 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/igor/.minikube/machines/minikube/id_rsa Username:docker}
I0527 17:18:55.044116    4981 ssh_runner.go:195] Run: cat /etc/os-release
I0527 17:18:55.075505    4981 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0527 17:18:55.075559    4981 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0527 17:18:55.075579    4981 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0527 17:18:55.075592    4981 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0527 17:18:55.075614    4981 filesync.go:126] Scanning /home/igor/.minikube/addons for local assets ...
I0527 17:18:55.075777    4981 filesync.go:126] Scanning /home/igor/.minikube/files for local assets ...
I0527 17:18:55.075828    4981 start.go:296] duration metric: took 534.498581ms for postStartSetup
I0527 17:18:55.076381    4981 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0527 17:18:55.076471    4981 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0527 17:18:55.146535    4981 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/igor/.minikube/machines/minikube/id_rsa Username:docker}
I0527 17:18:55.379097    4981 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0527 17:18:55.402124    4981 fix.go:56] duration metric: took 19.328316128s for fixHost
I0527 17:18:55.402142    4981 start.go:83] releasing machines lock for "minikube", held for 19.328376805s
I0527 17:18:55.402619    4981 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0527 17:18:55.479331    4981 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0527 17:18:55.479316    4981 ssh_runner.go:195] Run: cat /version.json
I0527 17:18:55.479383    4981 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0527 17:18:55.479341    4981 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0527 17:18:55.532711    4981 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/igor/.minikube/machines/minikube/id_rsa Username:docker}
I0527 17:18:55.598021    4981 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/igor/.minikube/machines/minikube/id_rsa Username:docker}
I0527 17:18:55.733660    4981 ssh_runner.go:195] Run: systemctl --version
I0527 17:18:56.701236    4981 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (1.221879177s)
I0527 17:18:56.701435    4981 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0527 17:18:56.715305    4981 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0527 17:18:56.839791    4981 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0527 17:18:56.840640    4981 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0527 17:18:56.901269    4981 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0527 17:18:56.901294    4981 start.go:495] detecting cgroup driver to use...
I0527 17:18:56.901515    4981 detect.go:190] detected "systemd" cgroup driver on host os
I0527 17:18:56.901638    4981 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0527 17:18:56.978459    4981 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0527 17:18:57.030971    4981 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0527 17:18:57.193826    4981 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0527 17:18:57.193971    4981 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0527 17:18:57.261558    4981 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0527 17:18:57.344915    4981 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0527 17:18:57.404244    4981 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0527 17:18:57.589653    4981 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0527 17:18:57.667355    4981 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0527 17:18:57.791814    4981 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0527 17:18:57.832971    4981 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0527 17:18:57.865391    4981 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0527 17:18:57.916971    4981 crio.go:166] couldn't verify netfilter by "sudo sysctl net.bridge.bridge-nf-call-iptables" which might be okay. error: sudo sysctl net.bridge.bridge-nf-call-iptables: Process exited with status 255
stdout:

stderr:
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory
I0527 17:18:57.917724    4981 ssh_runner.go:195] Run: sudo modprobe br_netfilter
I0527 17:18:58.056579    4981 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0527 17:18:58.195180    4981 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0527 17:18:58.472612    4981 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0527 17:18:58.717223    4981 start.go:495] detecting cgroup driver to use...
I0527 17:18:58.717262    4981 detect.go:190] detected "systemd" cgroup driver on host os
I0527 17:18:58.717321    4981 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0527 17:18:58.791603    4981 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0527 17:18:58.791661    4981 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0527 17:18:58.847784    4981 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0527 17:18:58.903760    4981 ssh_runner.go:195] Run: which cri-dockerd
I0527 17:18:58.914751    4981 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0527 17:18:58.942385    4981 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0527 17:18:59.082615    4981 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0527 17:18:59.284813    4981 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0527 17:19:00.335392    4981 ssh_runner.go:235] Completed: sudo systemctl enable docker.socket: (1.050088764s)
I0527 17:19:00.335666    4981 docker.go:574] configuring docker to use "systemd" as cgroup driver...
I0527 17:19:00.341253    4981 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0527 17:19:00.472586    4981 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0527 17:19:00.829806    4981 ssh_runner.go:195] Run: sudo systemctl restart docker
I0527 17:19:04.994921    4981 ssh_runner.go:235] Completed: sudo systemctl restart docker: (4.165073465s)
I0527 17:19:04.995006    4981 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0527 17:19:05.030437    4981 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0527 17:19:05.072636    4981 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0527 17:19:05.098962    4981 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0527 17:19:05.232526    4981 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0527 17:19:05.419554    4981 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0527 17:19:05.624674    4981 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0527 17:19:05.662544    4981 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0527 17:19:05.684920    4981 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0527 17:19:05.808564    4981 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0527 17:19:06.321992    4981 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0527 17:19:06.322979    4981 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0527 17:19:06.336918    4981 start.go:563] Will wait 60s for crictl version
I0527 17:19:06.336979    4981 ssh_runner.go:195] Run: which crictl
I0527 17:19:06.352455    4981 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0527 17:19:06.890392    4981 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0527 17:19:06.890586    4981 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0527 17:19:07.203004    4981 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0527 17:19:07.334450    4981 out.go:235] üê≥  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
I0527 17:19:07.334970    4981 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0527 17:19:07.432662    4981 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0527 17:19:07.457908    4981 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0527 17:19:07.519830    4981 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/igor:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0527 17:19:07.522508    4981 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0527 17:19:07.522670    4981 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0527 17:19:07.575496    4981 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/metrics-server/metrics-server:<none>
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
kubernetesui/dashboard:<none>
kicbase/echo-server:1.0
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0527 17:19:07.575508    4981 docker.go:619] Images already preloaded, skipping extraction
I0527 17:19:07.575581    4981 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0527 17:19:07.658358    4981 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/metrics-server/metrics-server:<none>
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
kubernetesui/dashboard:<none>
kicbase/echo-server:1.0
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0527 17:19:07.658370    4981 cache_images.go:84] Images are preloaded, skipping loading
I0527 17:19:07.658378    4981 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I0527 17:19:07.658463    4981 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0527 17:19:07.658627    4981 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0527 17:19:08.193515    4981 cni.go:84] Creating CNI manager for ""
I0527 17:19:08.193537    4981 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0527 17:19:08.193550    4981 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0527 17:19:08.193576    4981 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0527 17:19:08.197183    4981 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0527 17:19:08.197687    4981 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0527 17:19:08.236337    4981 binaries.go:44] Found k8s binaries, skipping transfer
I0527 17:19:08.236393    4981 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0527 17:19:08.271572    4981 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0527 17:19:08.319570    4981 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0527 17:19:08.426551    4981 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2285 bytes)
I0527 17:19:08.495390    4981 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0527 17:19:08.506151    4981 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0527 17:19:08.534621    4981 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0527 17:19:08.706409    4981 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0527 17:19:08.749000    4981 certs.go:68] Setting up /home/igor/.minikube/profiles/minikube for IP: 192.168.49.2
I0527 17:19:08.749339    4981 certs.go:194] generating shared ca certs ...
I0527 17:19:08.749403    4981 certs.go:226] acquiring lock for ca certs: {Name:mk2fd8e77b11d93e0fb710535a074119a70b8f2d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0527 17:19:08.750137    4981 certs.go:235] skipping valid "minikubeCA" ca cert: /home/igor/.minikube/ca.key
I0527 17:19:08.752645    4981 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/igor/.minikube/proxy-client-ca.key
I0527 17:19:08.752816    4981 certs.go:256] generating profile certs ...
I0527 17:19:08.754679    4981 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/igor/.minikube/profiles/minikube/client.key
I0527 17:19:08.754925    4981 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/igor/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0527 17:19:08.759172    4981 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/igor/.minikube/profiles/minikube/proxy-client.key
I0527 17:19:08.759376    4981 certs.go:484] found cert: /home/igor/.minikube/certs/ca-key.pem (1675 bytes)
I0527 17:19:08.759406    4981 certs.go:484] found cert: /home/igor/.minikube/certs/ca.pem (1070 bytes)
I0527 17:19:08.759430    4981 certs.go:484] found cert: /home/igor/.minikube/certs/cert.pem (1115 bytes)
I0527 17:19:08.759458    4981 certs.go:484] found cert: /home/igor/.minikube/certs/key.pem (1679 bytes)
I0527 17:19:08.760465    4981 ssh_runner.go:362] scp /home/igor/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0527 17:19:08.884564    4981 ssh_runner.go:362] scp /home/igor/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0527 17:19:08.999826    4981 ssh_runner.go:362] scp /home/igor/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0527 17:19:09.112758    4981 ssh_runner.go:362] scp /home/igor/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0527 17:19:09.281961    4981 ssh_runner.go:362] scp /home/igor/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0527 17:19:09.364196    4981 ssh_runner.go:362] scp /home/igor/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0527 17:19:09.818190    4981 ssh_runner.go:362] scp /home/igor/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0527 17:19:09.975641    4981 ssh_runner.go:362] scp /home/igor/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0527 17:19:10.101395    4981 ssh_runner.go:362] scp /home/igor/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0527 17:19:10.177350    4981 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (740 bytes)
I0527 17:19:10.223784    4981 ssh_runner.go:195] Run: openssl version
I0527 17:19:10.254142    4981 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0527 17:19:10.342374    4981 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0527 17:19:10.350273    4981 certs.go:528] hashing: -rw-r--r--. 1 root root 1111 May 20 15:59 /usr/share/ca-certificates/minikubeCA.pem
I0527 17:19:10.350337    4981 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0527 17:19:10.414913    4981 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0527 17:19:10.449213    4981 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0527 17:19:10.465807    4981 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0527 17:19:10.501617    4981 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0527 17:19:10.537856    4981 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0527 17:19:10.584383    4981 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0527 17:19:10.617669    4981 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0527 17:19:10.632719    4981 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0527 17:19:10.655935    4981 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/igor:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0527 17:19:10.656434    4981 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0527 17:19:10.864323    4981 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0527 17:19:10.941754    4981 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0527 17:19:10.941762    4981 kubeadm.go:593] restartPrimaryControlPlane start ...
I0527 17:19:10.941844    4981 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0527 17:19:11.069510    4981 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0527 17:19:11.072252    4981 kubeconfig.go:125] found "minikube" server: "https://192.168.49.2:8443"
I0527 17:19:11.075350    4981 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0527 17:19:11.237849    4981 kubeadm.go:630] The running cluster does not require reconfiguration: 192.168.49.2
I0527 17:19:11.237874    4981 kubeadm.go:597] duration metric: took 296.108138ms to restartPrimaryControlPlane
I0527 17:19:11.237884    4981 kubeadm.go:394] duration metric: took 581.964946ms to StartCluster
I0527 17:19:11.237899    4981 settings.go:142] acquiring lock: {Name:mkb49c455f39d311c94fc962d5f66554bb20681c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0527 17:19:11.238019    4981 settings.go:150] Updating kubeconfig:  /home/igor/.kube/config
I0527 17:19:11.239422    4981 lock.go:35] WriteFile acquiring /home/igor/.kube/config: {Name:mkdeb5865489a85486f2e3f20a1b7d0a201dd6f9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0527 17:19:11.241313    4981 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0527 17:19:11.242864    4981 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0527 17:19:11.242897    4981 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:true nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0527 17:19:11.243103    4981 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0527 17:19:11.243120    4981 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W0527 17:19:11.243126    4981 addons.go:247] addon storage-provisioner should already be in state true
I0527 17:19:11.243150    4981 host.go:66] Checking if "minikube" exists ...
I0527 17:19:11.244239    4981 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0527 17:19:11.244265    4981 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0527 17:19:11.244626    4981 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0527 17:19:11.246846    4981 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0527 17:19:11.250085    4981 addons.go:69] Setting dashboard=true in profile "minikube"
I0527 17:19:11.250109    4981 addons.go:238] Setting addon dashboard=true in "minikube"
W0527 17:19:11.250117    4981 addons.go:247] addon dashboard should already be in state true
I0527 17:19:11.250154    4981 host.go:66] Checking if "minikube" exists ...
I0527 17:19:11.260125    4981 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0527 17:19:11.262583    4981 addons.go:69] Setting metrics-server=true in profile "minikube"
I0527 17:19:11.262618    4981 addons.go:238] Setting addon metrics-server=true in "minikube"
W0527 17:19:11.262624    4981 addons.go:247] addon metrics-server should already be in state true
I0527 17:19:11.262658    4981 host.go:66] Checking if "minikube" exists ...
I0527 17:19:11.263110    4981 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0527 17:19:11.306935    4981 out.go:177] üîé  Verifying Kubernetes components...
I0527 17:19:11.344796    4981 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0527 17:19:11.829423    4981 out.go:177]     ‚ñ™ Using image docker.io/kubernetesui/dashboard:v2.7.0
I0527 17:19:11.884480    4981 out.go:177]     ‚ñ™ Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I0527 17:19:12.033657    4981 addons.go:435] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0527 17:19:12.033676    4981 ssh_runner.go:362] scp dashboard/dashboard-ns.yaml --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0527 17:19:12.033789    4981 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0527 17:19:12.261610    4981 cli_runner.go:217] Completed: docker container inspect minikube --format={{.State.Status}}: (1.014726586s)
I0527 17:19:12.293485    4981 cli_runner.go:217] Completed: docker container inspect minikube --format={{.State.Status}}: (1.048822242s)
I0527 17:19:12.296845    4981 out.go:177]     ‚ñ™ Using image registry.k8s.io/metrics-server/metrics-server:v0.7.2
I0527 17:19:12.302733    4981 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0527 17:19:12.302747    4981 addons.go:247] addon default-storageclass should already be in state true
I0527 17:19:12.302774    4981 host.go:66] Checking if "minikube" exists ...
I0527 17:19:12.303106    4981 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0527 17:19:12.312656    4981 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0527 17:19:12.351574    4981 addons.go:435] installing /etc/kubernetes/addons/metrics-apiservice.yaml
I0527 17:19:12.351599    4981 ssh_runner.go:362] scp metrics-server/metrics-apiservice.yaml --> /etc/kubernetes/addons/metrics-apiservice.yaml (424 bytes)
I0527 17:19:12.351716    4981 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0527 17:19:12.414623    4981 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0527 17:19:12.414644    4981 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0527 17:19:12.418390    4981 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0527 17:19:12.512935    4981 ssh_runner.go:235] Completed: sudo systemctl daemon-reload: (1.168114282s)
I0527 17:19:12.513010    4981 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0527 17:19:12.690704    4981 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/igor/.minikube/machines/minikube/id_rsa Username:docker}
I0527 17:19:12.728065    4981 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/igor/.minikube/machines/minikube/id_rsa Username:docker}
I0527 17:19:12.779617    4981 api_server.go:52] waiting for apiserver process to appear ...
I0527 17:19:12.779742    4981 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0527 17:19:12.811620    4981 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/igor/.minikube/machines/minikube/id_rsa Username:docker}
I0527 17:19:12.826266    4981 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0527 17:19:12.826278    4981 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0527 17:19:12.826339    4981 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0527 17:19:13.029542    4981 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/igor/.minikube/machines/minikube/id_rsa Username:docker}
I0527 17:19:13.289308    4981 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0527 17:19:13.780899    4981 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0527 17:19:14.052740    4981 addons.go:435] installing /etc/kubernetes/addons/metrics-server-deployment.yaml
I0527 17:19:14.052752    4981 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-deployment.yaml (1907 bytes)
I0527 17:19:14.089994    4981 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0527 17:19:14.129826    4981 addons.go:435] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0527 17:19:14.129842    4981 ssh_runner.go:362] scp dashboard/dashboard-clusterrole.yaml --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0527 17:19:14.240655    4981 addons.go:435] installing /etc/kubernetes/addons/metrics-server-rbac.yaml
I0527 17:19:14.240677    4981 ssh_runner.go:362] scp metrics-server/metrics-server-rbac.yaml --> /etc/kubernetes/addons/metrics-server-rbac.yaml (2175 bytes)
I0527 17:19:14.244550    4981 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0527 17:19:14.280221    4981 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0527 17:19:14.444850    4981 addons.go:435] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0527 17:19:14.444877    4981 ssh_runner.go:362] scp dashboard/dashboard-clusterrolebinding.yaml --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0527 17:19:14.595261    4981 addons.go:435] installing /etc/kubernetes/addons/metrics-server-service.yaml
I0527 17:19:14.595276    4981 ssh_runner.go:362] scp metrics-server/metrics-server-service.yaml --> /etc/kubernetes/addons/metrics-server-service.yaml (446 bytes)
I0527 17:19:14.803780    4981 addons.go:435] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0527 17:19:14.803796    4981 ssh_runner.go:362] scp dashboard/dashboard-configmap.yaml --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0527 17:19:14.999249    4981 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml
I0527 17:19:15.214811    4981 addons.go:435] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0527 17:19:15.214825    4981 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I0527 17:19:15.426397    4981 addons.go:435] installing /etc/kubernetes/addons/dashboard-role.yaml
I0527 17:19:15.426409    4981 ssh_runner.go:362] scp dashboard/dashboard-role.yaml --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0527 17:19:16.156559    4981 addons.go:435] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0527 17:19:16.156701    4981 ssh_runner.go:362] scp dashboard/dashboard-rolebinding.yaml --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0527 17:19:16.311282    4981 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (2.221263066s)
W0527 17:19:16.311311    4981 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:16.311328    4981 retry.go:31] will retry after 141.667099ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:16.433412    4981 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (2.153155954s)
I0527 17:19:16.433501    4981 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0527 17:19:16.433612    4981 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: (1.43434114s)
W0527 17:19:16.433630    4981 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/metrics-apiservice.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-deployment.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-rbac.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-service.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:16.434369    4981 retry.go:31] will retry after 209.822128ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/metrics-apiservice.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-deployment.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-rbac.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-service.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:16.437954    4981 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.193376198s)
W0527 17:19:16.437981    4981 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:16.438002    4981 retry.go:31] will retry after 133.200467ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:16.476284    4981 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0527 17:19:16.481137    4981 addons.go:435] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0527 17:19:16.481154    4981 ssh_runner.go:362] scp dashboard/dashboard-sa.yaml --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0527 17:19:16.571914    4981 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0527 17:19:16.644769    4981 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml
I0527 17:19:16.783414    4981 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0527 17:19:16.929455    4981 addons.go:435] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0527 17:19:16.929472    4981 ssh_runner.go:362] scp dashboard/dashboard-secret.yaml --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0527 17:19:17.635309    4981 addons.go:435] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0527 17:19:17.635327    4981 ssh_runner.go:362] scp dashboard/dashboard-svc.yaml --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0527 17:19:17.828224    4981 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.256285643s)
W0527 17:19:17.828249    4981 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:17.828263    4981 retry.go:31] will retry after 309.616111ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:17.828962    4981 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (1.352662433s)
W0527 17:19:17.828978    4981 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:17.828989    4981 retry.go:31] will retry after 291.713454ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:17.932165    4981 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0527 17:19:17.957713    4981 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (1.174269768s)
I0527 17:19:17.957868    4981 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0527 17:19:17.958354    4981 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: (1.313540206s)
W0527 17:19:17.958379    4981 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/metrics-apiservice.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-deployment.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-rbac.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-service.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:17.958402    4981 retry.go:31] will retry after 269.203104ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/metrics-apiservice.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-deployment.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-rbac.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-service.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:18.122223    4981 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0527 17:19:18.138510    4981 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0527 17:19:18.253963    4981 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml
I0527 17:19:18.906416    4981 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0527 17:19:18.912371    4981 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:18.912401    4981 retry.go:31] will retry after 182.591327ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:19.095927    4981 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0527 17:19:19.209195    4981 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.070653736s)
W0527 17:19:19.209230    4981 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:19.209249    4981 retry.go:31] will retry after 589.575489ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:19.210696    4981 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (1.088436121s)
W0527 17:19:19.210718    4981 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:19.210736    4981 retry.go:31] will retry after 812.831339ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:19.458319    4981 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0527 17:19:19.463304    4981 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: (1.209264281s)
W0527 17:19:19.463368    4981 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/metrics-apiservice.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-deployment.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-rbac.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-service.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:19.463407    4981 retry.go:31] will retry after 805.660854ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/metrics-apiservice.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-deployment.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-rbac.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-service.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0527 17:19:19.646246    4981 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:19.646289    4981 retry.go:31] will retry after 270.164151ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:19.781586    4981 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0527 17:19:19.801965    4981 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0527 17:19:19.924792    4981 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0527 17:19:20.025010    4981 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0527 17:19:20.191431    4981 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:20.191473    4981 retry.go:31] will retry after 426.840963ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:20.269824    4981 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml
I0527 17:19:20.280457    4981 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0527 17:19:20.623518    4981 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0527 17:19:20.811562    4981 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:20.811583    4981 retry.go:31] will retry after 796.238294ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0527 17:19:20.811613    4981 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:20.811616    4981 retry.go:31] will retry after 735.661693ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:21.537600    4981 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (1.257114411s)
I0527 17:19:21.537686    4981 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0527 17:19:21.537782    4981 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:21.538163    4981 retry.go:31] will retry after 1.85634542s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:21.538508    4981 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: (1.268505967s)
W0527 17:19:21.538527    4981 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/metrics-apiservice.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-deployment.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-rbac.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-service.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:21.538598    4981 retry.go:31] will retry after 1.13955655s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/metrics-apiservice.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-deployment.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-rbac.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-service.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:21.551070    4981 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0527 17:19:21.609572    4981 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0527 17:19:21.780197    4981 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0527 17:19:22.545544    4981 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:22.545564    4981 retry.go:31] will retry after 1.021880024s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:22.605914    4981 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0527 17:19:22.610868    4981 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (1.001264155s)
W0527 17:19:22.610912    4981 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:22.610930    4981 retry.go:31] will retry after 572.838152ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:22.678314    4981 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml
I0527 17:19:22.701095    4981 api_server.go:72] duration metric: took 11.459742007s to wait for apiserver process to appear ...
I0527 17:19:22.701145    4981 api_server.go:88] waiting for apiserver healthz status ...
I0527 17:19:22.701162    4981 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0527 17:19:22.703261    4981 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0527 17:19:23.187201    4981 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0527 17:19:23.219707    4981 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0527 17:19:23.266689    4981 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0527 17:19:23.560723    4981 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0527 17:19:23.580475    4981 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0527 17:19:23.966615    4981 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0527 17:19:23.970738    4981 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0527 17:19:24.203515    4981 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0527 17:19:24.206718    4981 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0527 17:19:24.750300    4981 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0527 17:19:24.759427    4981 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0527 17:19:25.228497    4981 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0527 17:19:25.656886    4981 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0527 17:19:25.768605    4981 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0527 17:19:26.592693    4981 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0527 17:19:26.592718    4981 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0527 17:19:27.045960    4981 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0527 17:19:27.045987    4981 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0527 17:19:27.054440    4981 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0527 17:19:27.287174    4981 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0527 17:19:27.845112    4981 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0527 17:19:27.845143    4981 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0527 17:19:27.845316    4981 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0527 17:19:28.246071    4981 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0527 17:19:28.253068    4981 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0527 17:19:28.689448    4981 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: (6.01108082s)
W0527 17:19:28.689520    4981 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/metrics-apiservice.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-deployment.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-rbac.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-service.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:28.689548    4981 retry.go:31] will retry after 1.635499908s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/metrics-apiservice.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-deployment.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-rbac.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-service.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:28.704624    4981 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0527 17:19:28.706878    4981 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0527 17:19:29.201272    4981 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0527 17:19:29.202101    4981 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0527 17:19:29.291803    4981 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (6.10457096s)
W0527 17:19:29.292414    4981 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:29.292463    4981 retry.go:31] will retry after 1.706173796s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:29.292555    4981 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (5.712067217s)
W0527 17:19:29.292577    4981 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:29.292581    4981 retry.go:31] will retry after 2.667294645s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:29.292633    4981 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (5.731897703s)
W0527 17:19:29.292638    4981 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:29.292641    4981 retry.go:31] will retry after 1.986206606s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:29.730049    4981 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0527 17:19:29.738337    4981 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0527 17:19:30.253902    4981 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0527 17:19:30.277106    4981 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0527 17:19:30.328243    4981 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml
I0527 17:19:31.243724    4981 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0527 17:19:31.246677    4981 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0527 17:19:31.252057    4981 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0527 17:19:31.281280    4981 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0527 17:19:31.813696    4981 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0527 17:19:31.814206    4981 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0527 17:19:32.251731    4981 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0527 17:19:32.259592    4981 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0527 17:19:32.260119    4981 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0527 17:19:32.766414    4981 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0527 17:19:32.769383    4981 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0527 17:19:32.868235    4981 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: (2.539959472s)
W0527 17:19:32.868276    4981 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/metrics-apiservice.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-deployment.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-rbac.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-service.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:32.868299    4981 retry.go:31] will retry after 1.967092089s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/metrics-apiservice.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-deployment.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-rbac.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-service.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:33.052570    4981 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.77126318s)
W0527 17:19:33.052595    4981 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:33.052608    4981 retry.go:31] will retry after 1.532797968s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:33.052975    4981 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (1.806265111s)
W0527 17:19:33.053019    4981 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:33.053031    4981 retry.go:31] will retry after 1.955126907s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0527 17:19:33.127458    4981 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:33.127483    4981 retry.go:31] will retry after 2.286353743s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0527 17:19:33.201951    4981 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0527 17:19:34.694573    4981 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0527 17:19:34.843577    4981 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml
I0527 17:19:35.068622    4981 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0527 17:19:35.471380    4981 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0527 17:19:38.204272    4981 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0527 17:19:38.204309    4981 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0527 17:19:41.218074    4981 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0527 17:19:41.218105    4981 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0527 17:19:41.218131    4981 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0527 17:19:41.721668    4981 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0527 17:19:41.721688    4981 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0527 17:19:41.721702    4981 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0527 17:19:41.804933    4981 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0527 17:19:41.804948    4981 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0527 17:19:42.251578    4981 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0527 17:19:42.673961    4981 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0527 17:19:42.673978    4981 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0527 17:19:42.707267    4981 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0527 17:19:42.730670    4981 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0527 17:19:42.730688    4981 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0527 17:19:43.202418    4981 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0527 17:19:43.212714    4981 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0527 17:19:43.212733    4981 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0527 17:19:43.701301    4981 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0527 17:19:43.710359    4981 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0527 17:19:43.710390    4981 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0527 17:19:44.202746    4981 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0527 17:19:44.221487    4981 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0527 17:19:44.221513    4981 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0527 17:19:44.703343    4981 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0527 17:19:44.711466    4981 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0527 17:19:44.711663    4981 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0527 17:19:45.202108    4981 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0527 17:19:45.282779    4981 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0527 17:19:45.282805    4981 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0527 17:19:45.402544    4981 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (10.707931611s)
I0527 17:19:45.702626    4981 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0527 17:19:45.710701    4981 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0527 17:19:45.729453    4981 api_server.go:141] control plane version: v1.32.0
I0527 17:19:45.729478    4981 api_server.go:131] duration metric: took 23.028325496s to wait for apiserver health ...
I0527 17:19:45.729500    4981 system_pods.go:43] waiting for kube-system pods to appear ...
I0527 17:19:45.835672    4981 system_pods.go:59] 8 kube-system pods found
I0527 17:19:45.835708    4981 system_pods.go:61] "coredns-668d6bf9bc-dhj2t" [8f047629-14ee-4d10-ac40-7f599c28a2e0] Running
I0527 17:19:45.835724    4981 system_pods.go:61] "etcd-minikube" [33e11231-649d-433a-b25a-88679d0ad116] Running
I0527 17:19:45.835731    4981 system_pods.go:61] "kube-apiserver-minikube" [3185fbdf-d718-439a-8bd4-9af502850d0b] Running
I0527 17:19:45.835748    4981 system_pods.go:61] "kube-controller-manager-minikube" [1fa5e186-434f-4620-8035-0dbe7f6e499e] Running
I0527 17:19:45.835754    4981 system_pods.go:61] "kube-proxy-898wf" [3d2cf648-341a-41d9-982f-ab0976e0f495] Running
I0527 17:19:45.835759    4981 system_pods.go:61] "kube-scheduler-minikube" [524b4f9a-5606-4894-9926-420cb78d5205] Running
I0527 17:19:45.835765    4981 system_pods.go:61] "metrics-server-7fbb699795-kfssn" [ed484c28-2c41-4f80-bdba-20148aaac680] Running
I0527 17:19:45.835770    4981 system_pods.go:61] "storage-provisioner" [4e4240e3-7673-44d5-b44f-0c9e5634b100] Running
I0527 17:19:45.835781    4981 system_pods.go:74] duration metric: took 106.270539ms to wait for pod list to return data ...
I0527 17:19:45.835806    4981 kubeadm.go:582] duration metric: took 34.594450065s to wait for: map[apiserver:true system_pods:true]
I0527 17:19:45.835833    4981 node_conditions.go:102] verifying NodePressure condition ...
I0527 17:19:45.937557    4981 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: (11.09395271s)
I0527 17:19:45.937581    4981 addons.go:479] Verifying addon metrics-server=true in "minikube"
I0527 17:19:45.946632    4981 node_conditions.go:122] node storage ephemeral capacity is 39933Mi
I0527 17:19:45.946669    4981 node_conditions.go:123] node cpu capacity is 2
I0527 17:19:45.946697    4981 node_conditions.go:105] duration metric: took 110.857196ms to run NodePressure ...
I0527 17:19:45.946730    4981 start.go:241] waiting for startup goroutines ...
I0527 17:19:47.193987    4981 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (11.722539656s)
I0527 17:19:47.195469    4981 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (12.12682088s)
I0527 17:19:47.211123    4981 out.go:177] üí°  Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server

I0527 17:19:47.235412    4981 out.go:177] üåü  Enabled addons: storage-provisioner, metrics-server, default-storageclass, dashboard
I0527 17:19:47.258390    4981 addons.go:514] duration metric: took 36.015482428s for enable addons: enabled=[storage-provisioner metrics-server default-storageclass dashboard]
I0527 17:19:47.258587    4981 start.go:246] waiting for cluster config update ...
I0527 17:19:47.258605    4981 start.go:255] writing updated cluster config ...
I0527 17:19:47.286552    4981 ssh_runner.go:195] Run: rm -f paused
I0527 17:19:47.745204    4981 start.go:600] kubectl: 1.33.1, cluster: 1.32.0 (minor skew: 1)
I0527 17:19:47.782760    4981 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
May 27 15:19:04 minikube dockerd[800]: time="2025-05-27T15:19:04.984673024Z" level=info msg="API listen on [::]:2376"
May 27 15:19:05 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
May 27 15:19:06 minikube cri-dockerd[1106]: time="2025-05-27T15:19:06Z" level=info msg="Starting cri-dockerd dev (HEAD)"
May 27 15:19:06 minikube cri-dockerd[1106]: time="2025-05-27T15:19:06Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
May 27 15:19:06 minikube cri-dockerd[1106]: time="2025-05-27T15:19:06Z" level=info msg="Start docker client with request timeout 0s"
May 27 15:19:06 minikube cri-dockerd[1106]: time="2025-05-27T15:19:06Z" level=info msg="Hairpin mode is set to hairpin-veth"
May 27 15:19:06 minikube cri-dockerd[1106]: time="2025-05-27T15:19:06Z" level=info msg="Loaded network plugin cni"
May 27 15:19:06 minikube cri-dockerd[1106]: time="2025-05-27T15:19:06Z" level=info msg="Docker cri networking managed by network plugin cni"
May 27 15:19:06 minikube cri-dockerd[1106]: time="2025-05-27T15:19:06Z" level=info msg="Setting cgroupDriver systemd"
May 27 15:19:06 minikube cri-dockerd[1106]: time="2025-05-27T15:19:06Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
May 27 15:19:06 minikube cri-dockerd[1106]: time="2025-05-27T15:19:06Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
May 27 15:19:06 minikube cri-dockerd[1106]: time="2025-05-27T15:19:06Z" level=info msg="Start cri-dockerd grpc backend"
May 27 15:19:06 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
May 27 15:19:11 minikube cri-dockerd[1106]: time="2025-05-27T15:19:11Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"metrics-server-7fbb699795-kfssn_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"34623245451f75c99c66394ac6210364fbf7ebe308d3f8767a62f9099fa5b0e1\""
May 27 15:19:11 minikube cri-dockerd[1106]: time="2025-05-27T15:19:11Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-5d59dccf9b-2dm8x_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"8ac514e6338c96f121de4c94efd01c475c256263f2e334cf0dccc4be0f725a67\""
May 27 15:19:11 minikube cri-dockerd[1106]: time="2025-05-27T15:19:11Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"hello-minikube-ffcbb5874-26k49_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"2283899c37dd8d60ed003c38acd224b6a5b5dc1f9fbc3869d9a8bd42ad76db6a\""
May 27 15:19:11 minikube cri-dockerd[1106]: time="2025-05-27T15:19:11Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"hello-minikube-ffcbb5874-26k49_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"069c96c09b9502b5f14095e9cd2eb4913a7bba2ac70b16c4fd8b4d6dd6248152\""
May 27 15:19:11 minikube cri-dockerd[1106]: time="2025-05-27T15:19:11Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-dhj2t_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"8140f9cb861a46443e9c4a535b75f121738d96a65a5e4afe8e5912badd7164d1\""
May 27 15:19:11 minikube cri-dockerd[1106]: time="2025-05-27T15:19:11Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-dhj2t_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"badb0dc55cc85db884f7db306ccf7451445f096246e20a00aa266944b6641784\""
May 27 15:19:11 minikube cri-dockerd[1106]: time="2025-05-27T15:19:11Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-dashboard-7779f9b69b-tz6tc_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"fef3b2a9e6ad04a6cdb192814c27bb827cf702be6431e85dd106c0bedd6128de\""
May 27 15:19:13 minikube cri-dockerd[1106]: time="2025-05-27T15:19:13Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"hello-minikube-ffcbb5874-26k49_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"2283899c37dd8d60ed003c38acd224b6a5b5dc1f9fbc3869d9a8bd42ad76db6a\""
May 27 15:19:13 minikube cri-dockerd[1106]: time="2025-05-27T15:19:13Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"hello-minikube-ffcbb5874-26k49_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"069c96c09b9502b5f14095e9cd2eb4913a7bba2ac70b16c4fd8b4d6dd6248152\""
May 27 15:19:13 minikube cri-dockerd[1106]: time="2025-05-27T15:19:13Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-dhj2t_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"8140f9cb861a46443e9c4a535b75f121738d96a65a5e4afe8e5912badd7164d1\""
May 27 15:19:14 minikube cri-dockerd[1106]: time="2025-05-27T15:19:14Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"hello-minikube-ffcbb5874-26k49_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"2283899c37dd8d60ed003c38acd224b6a5b5dc1f9fbc3869d9a8bd42ad76db6a\""
May 27 15:19:14 minikube cri-dockerd[1106]: time="2025-05-27T15:19:14Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-dhj2t_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"8140f9cb861a46443e9c4a535b75f121738d96a65a5e4afe8e5912badd7164d1\""
May 27 15:19:17 minikube cri-dockerd[1106]: time="2025-05-27T15:19:17Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/16a3b2eb882ac130472a9a739246537b93ceba0ceb596f237a5e05aad59c2e8b/resolv.conf as [nameserver 192.168.49.1 options ndots:0 edns0 trust-ad]"
May 27 15:19:17 minikube cri-dockerd[1106]: time="2025-05-27T15:19:17Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/fc3f1ce1e24e66e1bdb2ad1392788da77e2c9b74b274a65c7abf316117bd96d3/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
May 27 15:19:18 minikube cri-dockerd[1106]: time="2025-05-27T15:19:18Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ab38b1199fbacebc16884d857dd3aee9e374747987d27ce38ee3f72a9037429d/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
May 27 15:19:18 minikube cri-dockerd[1106]: time="2025-05-27T15:19:18Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ba9c31b7036e9a9f5d8e7a4e790d419be0c675e0b8e58f3f54c013c50d5beb2b/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
May 27 15:19:42 minikube cri-dockerd[1106]: time="2025-05-27T15:19:42Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
May 27 15:19:44 minikube dockerd[800]: time="2025-05-27T15:19:44.742801499Z" level=info msg="ignoring event" container=ef9a8b4071ca2eaddeb31e42f17f48f5cf4f4de9445eedc59f552a820b5e5add module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 27 15:19:53 minikube cri-dockerd[1106]: time="2025-05-27T15:19:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a2b16c1103288751b2527e464fa76ddc94e3962b7248c125a4eb8eb65738e1f5/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
May 27 15:19:55 minikube cri-dockerd[1106]: time="2025-05-27T15:19:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c888141e3596ad7728a7228bc6d7a2328e9643c4e0ad7d1311d342e16c621b78/resolv.conf as [nameserver 10.96.0.10 search kube-system.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 27 15:19:55 minikube cri-dockerd[1106]: time="2025-05-27T15:19:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3c534dbdb6c217db25f1ce4acf7837c2ece30598e5441c615efa77955a30fb5e/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 27 15:19:55 minikube cri-dockerd[1106]: time="2025-05-27T15:19:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9f92432eab37261c40386ef4809ff431ef897ae552cc5ae3b76bd1574b6ff2fb/resolv.conf as [nameserver 192.168.49.1 options trust-ad ndots:0 edns0]"
May 27 15:19:57 minikube cri-dockerd[1106]: time="2025-05-27T15:19:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cb0d397dd4cae01c3f0b155254badb2ed8fb64c934c3386bdfbff318871a89eb/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 27 15:19:58 minikube cri-dockerd[1106]: time="2025-05-27T15:19:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/47e03fbac7a255cfa67cbb97da261934c8714c2229ecd79d5ae6cfa4d94f1fc9/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
May 27 15:19:58 minikube cri-dockerd[1106]: time="2025-05-27T15:19:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cb2f80f309002bf1bd5c24f11eda50a6ff7b833561f16c11be4ec6d65c004d35/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 27 15:20:29 minikube dockerd[800]: time="2025-05-27T15:20:29.694767273Z" level=info msg="ignoring event" container=2d70f2ca015b45d9d793f1475d4ab059be26d53fa386b158c01bbf68f2171f16 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 27 15:20:44 minikube dockerd[800]: time="2025-05-27T15:20:44.633833615Z" level=info msg="ignoring event" container=185151de1a2b114c91d7d662d34b796ffb79615e545e5e14750dfdac778052fc module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 27 15:20:45 minikube dockerd[800]: time="2025-05-27T15:20:45.563929156Z" level=info msg="ignoring event" container=938ceae4fe1bb0658eff02ce3958c4c0312071fff1180d903b30542889b1e633 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 27 15:32:33 minikube cri-dockerd[1106]: time="2025-05-27T15:32:33Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cd69a474640de48d21306a07c8e0f170b0a4ea5dd36a030d4b2dc9ac9f130f4d/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 27 15:32:36 minikube dockerd[800]: time="2025-05-27T15:32:36.884039337Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 27 15:32:36 minikube dockerd[800]: time="2025-05-27T15:32:36.891192189Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 27 15:32:53 minikube dockerd[800]: time="2025-05-27T15:32:53.718481675Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 27 15:32:53 minikube dockerd[800]: time="2025-05-27T15:32:53.721902445Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 27 15:33:22 minikube dockerd[800]: time="2025-05-27T15:33:22.090441946Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 27 15:33:22 minikube dockerd[800]: time="2025-05-27T15:33:22.091552189Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 27 15:34:15 minikube dockerd[800]: time="2025-05-27T15:34:15.842927756Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 27 15:34:15 minikube dockerd[800]: time="2025-05-27T15:34:15.843163001Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 27 15:35:37 minikube dockerd[800]: time="2025-05-27T15:35:37.944199600Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 27 15:35:37 minikube dockerd[800]: time="2025-05-27T15:35:37.944443089Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 27 15:37:21 minikube dockerd[800]: time="2025-05-27T15:37:21.710211433Z" level=info msg="ignoring event" container=cd69a474640de48d21306a07c8e0f170b0a4ea5dd36a030d4b2dc9ac9f130f4d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 27 15:37:25 minikube cri-dockerd[1106]: time="2025-05-27T15:37:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9d236fcbfe4204cca514263f4558f0f9073519d9226a86bc4d6120b1bdc74724/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 27 15:37:27 minikube dockerd[800]: time="2025-05-27T15:37:27.344276626Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 27 15:37:27 minikube dockerd[800]: time="2025-05-27T15:37:27.344575531Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 27 15:37:40 minikube dockerd[800]: time="2025-05-27T15:37:40.671987586Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 27 15:37:40 minikube dockerd[800]: time="2025-05-27T15:37:40.672335661Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 27 15:38:05 minikube dockerd[800]: time="2025-05-27T15:38:05.663505250Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 27 15:38:05 minikube dockerd[800]: time="2025-05-27T15:38:05.663707159Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE                                                                                                  CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
ae478048c46ee       07655ddf2eebe                                                                                          17 minutes ago      Running             kubernetes-dashboard        2                   cb0d397dd4cae       kubernetes-dashboard-7779f9b69b-tz6tc
9aa871a829796       48d9cfaaf3904                                                                                          17 minutes ago      Running             metrics-server              2                   c888141e3596a       metrics-server-7fbb699795-kfssn
e41059225be2d       6e38f40d628db                                                                                          17 minutes ago      Running             storage-provisioner         5                   9f92432eab372       storage-provisioner
4333dd7570f75       8cab3d2a8bd0f                                                                                          18 minutes ago      Running             kube-controller-manager     3                   16a3b2eb882ac       kube-controller-manager-minikube
8179a2b18da55       115053965e86b                                                                                          18 minutes ago      Running             dashboard-metrics-scraper   1                   cb2f80f309002       dashboard-metrics-scraper-5d59dccf9b-2dm8x
b344b5d284f99       c69fa2e9cbf5f                                                                                          18 minutes ago      Running             coredns                     2                   47e03fbac7a25       coredns-668d6bf9bc-dhj2t
938ceae4fe1bb       07655ddf2eebe                                                                                          18 minutes ago      Exited              kubernetes-dashboard        1                   cb0d397dd4cae       kubernetes-dashboard-7779f9b69b-tz6tc
185151de1a2b1       48d9cfaaf3904                                                                                          18 minutes ago      Exited              metrics-server              1                   c888141e3596a       metrics-server-7fbb699795-kfssn
4e76be86421b6       9056ab77afb8e                                                                                          18 minutes ago      Running             echo-server                 2                   3c534dbdb6c21       hello-minikube-ffcbb5874-26k49
2d70f2ca015b4       6e38f40d628db                                                                                          18 minutes ago      Exited              storage-provisioner         4                   9f92432eab372       storage-provisioner
1d0bec77a195b       040f9f8aac8cd                                                                                          18 minutes ago      Running             kube-proxy                  2                   a2b16c1103288       kube-proxy-898wf
44dae3fa039aa       a9e7e6b294baf                                                                                          19 minutes ago      Running             etcd                        2                   fc3f1ce1e24e6       etcd-minikube
78f9be7622f10       c2e17b8d0f4a3                                                                                          19 minutes ago      Running             kube-apiserver              2                   ab38b1199fbac       kube-apiserver-minikube
49253a43e31f6       a389e107f4ff1                                                                                          19 minutes ago      Running             kube-scheduler              2                   ba9c31b7036e9       kube-scheduler-minikube
ef9a8b4071ca2       8cab3d2a8bd0f                                                                                          19 minutes ago      Exited              kube-controller-manager     2                   16a3b2eb882ac       kube-controller-manager-minikube
16b06dbcd6a90       kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c   6 days ago          Exited              dashboard-metrics-scraper   0                   8ac514e6338c9       dashboard-metrics-scraper-5d59dccf9b-2dm8x
9a172709cdec4       c69fa2e9cbf5f                                                                                          7 days ago          Exited              coredns                     1                   8140f9cb861a4       coredns-668d6bf9bc-dhj2t
6de04f41f4aa4       9056ab77afb8e                                                                                          7 days ago          Exited              echo-server                 1                   2283899c37dd8       hello-minikube-ffcbb5874-26k49
e5c99e887fbbb       c2e17b8d0f4a3                                                                                          7 days ago          Exited              kube-apiserver              1                   95b33edfa0ce1       kube-apiserver-minikube
7a1d6ead37902       a9e7e6b294baf                                                                                          7 days ago          Exited              etcd                        1                   eaa59d2547853       etcd-minikube
4ad63f452b385       040f9f8aac8cd                                                                                          7 days ago          Exited              kube-proxy                  1                   b2178de168e48       kube-proxy-898wf
46ea0ea6d9b07       a389e107f4ff1                                                                                          7 days ago          Exited              kube-scheduler              1                   1f328507cdbdd       kube-scheduler-minikube


==> coredns [9a172709cdec] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:34478 - 12688 "HINFO IN 7679559775532112879.5792441740596876238. udp 57 false 512" - - 0 6.022859135s
[ERROR] plugin/errors: 2 7679559775532112879.5792441740596876238. HINFO: read udp 10.244.0.5:40912->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:46982 - 35164 "HINFO IN 7679559775532112879.5792441740596876238. udp 57 false 512" - - 0 6.009234078s
[ERROR] plugin/errors: 2 7679559775532112879.5792441740596876238. HINFO: read udp 10.244.0.5:34485->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:37077 - 39987 "HINFO IN 7679559775532112879.5792441740596876238. udp 57 false 512" - - 0 6.004906186s
[ERROR] plugin/errors: 2 7679559775532112879.5792441740596876238. HINFO: read udp 10.244.0.5:39882->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:55746 - 431 "HINFO IN 7679559775532112879.5792441740596876238. udp 57 false 512" - - 0 6.007592882s
[ERROR] plugin/errors: 2 7679559775532112879.5792441740596876238. HINFO: read udp 10.244.0.5:48379->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:45210 - 31959 "HINFO IN 7679559775532112879.5792441740596876238. udp 57 false 512" - - 0 6.009002086s
[ERROR] plugin/errors: 2 7679559775532112879.5792441740596876238. HINFO: read udp 10.244.0.5:44436->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:51355 - 17259 "HINFO IN 7679559775532112879.5792441740596876238. udp 57 false 512" - - 0 6.015375214s
[ERROR] plugin/errors: 2 7679559775532112879.5792441740596876238. HINFO: read udp 10.244.0.5:46726->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:49893 - 3883 "HINFO IN 7679559775532112879.5792441740596876238. udp 57 false 512" - - 0 6.0182338s
[ERROR] plugin/errors: 2 7679559775532112879.5792441740596876238. HINFO: read udp 10.244.0.5:51465->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:42262 - 8614 "HINFO IN 7679559775532112879.5792441740596876238. udp 57 false 512" - - 0 6.01382789s
[ERROR] plugin/errors: 2 7679559775532112879.5792441740596876238. HINFO: read udp 10.244.0.5:40312->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:36873 - 55519 "HINFO IN 7679559775532112879.5792441740596876238. udp 57 false 512" - - 0 6.007914821s
[ERROR] plugin/errors: 2 7679559775532112879.5792441740596876238. HINFO: read udp 10.244.0.5:58134->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:54652 - 4088 "HINFO IN 7679559775532112879.5792441740596876238. udp 57 false 512" - - 0 6.005012196s
[ERROR] plugin/errors: 2 7679559775532112879.5792441740596876238. HINFO: read udp 10.244.0.5:33587->192.168.49.1:53: i/o timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.345176155s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.104267431s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.026603253s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.355481804s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.0160336s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.050725747s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.133320555s


==> coredns [b344b5d284f9] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] 127.0.0.1:43810 - 53773 "HINFO IN 6458646484379292648.4723667353548948835. udp 57 false 512" - - 0 6.012581961s
[ERROR] plugin/errors: 2 6458646484379292648.4723667353548948835. HINFO: read udp 10.244.0.12:49494->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:45258 - 48558 "HINFO IN 6458646484379292648.4723667353548948835. udp 57 false 512" - - 0 6.65538179s
[ERROR] plugin/errors: 2 6458646484379292648.4723667353548948835. HINFO: read udp 10.244.0.12:54073->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:53595 - 12230 "HINFO IN 6458646484379292648.4723667353548948835. udp 57 false 512" - - 0 6.045659133s
[ERROR] plugin/errors: 2 6458646484379292648.4723667353548948835. HINFO: read udp 10.244.0.12:48435->192.168.49.1:53: i/o timeout
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] 127.0.0.1:54443 - 46103 "HINFO IN 6458646484379292648.4723667353548948835. udp 57 false 512" - - 0 6.246990319s
[ERROR] plugin/errors: 2 6458646484379292648.4723667353548948835. HINFO: read udp 10.244.0.12:34839->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:48393 - 22368 "HINFO IN 6458646484379292648.4723667353548948835. udp 57 false 512" - - 0 6.017783766s
[ERROR] plugin/errors: 2 6458646484379292648.4723667353548948835. HINFO: read udp 10.244.0.12:42955->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:58209 - 38080 "HINFO IN 6458646484379292648.4723667353548948835. udp 57 false 512" - - 0 6.008274331s
[ERROR] plugin/errors: 2 6458646484379292648.4723667353548948835. HINFO: read udp 10.244.0.12:35505->192.168.49.1:53: i/o timeout
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[870903151]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (27-May-2025 15:20:09.366) (total time: 30001ms):
Trace[870903151]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30001ms (15:20:39.368)
Trace[870903151]: [30.001958299s] [30.001958299s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1109652147]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (27-May-2025 15:20:09.369) (total time: 30001ms):
Trace[1109652147]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30001ms (15:20:39.371)
Trace[1109652147]: [30.001527878s] [30.001527878s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[2058812606]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (27-May-2025 15:20:09.371) (total time: 30001ms):
Trace[2058812606]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30001ms (15:20:39.372)
Trace[2058812606]: [30.001664335s] [30.001664335s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] 127.0.0.1:43912 - 24122 "HINFO IN 6458646484379292648.4723667353548948835. udp 57 false 512" - - 0 6.360418195s
[ERROR] plugin/errors: 2 6458646484379292648.4723667353548948835. HINFO: read udp 10.244.0.12:43111->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:50683 - 28041 "HINFO IN 6458646484379292648.4723667353548948835. udp 57 false 512" - - 0 6.381261804s
[ERROR] plugin/errors: 2 6458646484379292648.4723667353548948835. HINFO: read udp 10.244.0.12:34112->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:41465 - 44131 "HINFO IN 6458646484379292648.4723667353548948835. udp 57 false 512" - - 0 6.533616707s
[ERROR] plugin/errors: 2 6458646484379292648.4723667353548948835. HINFO: read udp 10.244.0.12:33549->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:37165 - 63464 "HINFO IN 6458646484379292648.4723667353548948835. udp 57 false 512" - - 0 6.010167134s
[ERROR] plugin/errors: 2 6458646484379292648.4723667353548948835. HINFO: read udp 10.244.0.12:43417->192.168.49.1:53: i/o timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.162108853s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_05_20T18_00_30_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 20 May 2025 16:00:18 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 27 May 2025 15:38:32 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 27 May 2025 15:37:29 +0000   Tue, 20 May 2025 16:00:17 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 27 May 2025 15:37:29 +0000   Tue, 20 May 2025 16:00:17 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 27 May 2025 15:37:29 +0000   Tue, 20 May 2025 16:00:17 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 27 May 2025 15:37:29 +0000   Tue, 20 May 2025 16:00:19 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                2
  ephemeral-storage:  39933Mi
  hugepages-2Mi:      0
  memory:             3997812Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  39933Mi
  hugepages-2Mi:      0
  memory:             3997812Ki
  pods:               110
System Info:
  Machine ID:                 51779fc12a28429987a43a13f33bea66
  System UUID:                f16fd32c-7752-4a25-984e-f0bf0b47d398
  Boot ID:                    1b9f28b9-b550-49d8-bca6-f16f73e683e4
  Kernel Version:             6.14.5-200.fc41.x86_64
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (12 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     hello-minikube-ffcbb5874-26k49                0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d23h
  default                     nginx-deploy-75cff64695-gvdpt                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         82s
  kube-system                 coredns-668d6bf9bc-dhj2t                      100m (5%)     0 (0%)      70Mi (1%)        170Mi (4%)     6d23h
  kube-system                 etcd-minikube                                 100m (5%)     0 (0%)      100Mi (2%)       0 (0%)         6d23h
  kube-system                 kube-apiserver-minikube                       250m (12%)    0 (0%)      0 (0%)           0 (0%)         6d23h
  kube-system                 kube-controller-manager-minikube              200m (10%)    0 (0%)      0 (0%)           0 (0%)         6d23h
  kube-system                 kube-proxy-898wf                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d23h
  kube-system                 kube-scheduler-minikube                       100m (5%)     0 (0%)      0 (0%)           0 (0%)         6d23h
  kube-system                 metrics-server-7fbb699795-kfssn               100m (5%)     0 (0%)      200Mi (5%)       0 (0%)         6d23h
  kube-system                 storage-provisioner                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d23h
  kubernetes-dashboard        dashboard-metrics-scraper-5d59dccf9b-2dm8x    0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d23h
  kubernetes-dashboard        kubernetes-dashboard-7779f9b69b-tz6tc         0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d23h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (42%)  0 (0%)
  memory             370Mi (9%)  170Mi (4%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                   Age                    From             Message
  ----     ------                   ----                   ----             -------
  Normal   Starting                 6d23h                  kube-proxy       
  Normal   Starting                 18m                    kube-proxy       
  Normal   Starting                 6d23h                  kube-proxy       
  Normal   NodeHasSufficientPID     6d23h (x7 over 6d23h)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeHasNoDiskPressure    6d23h (x8 over 6d23h)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientMemory  6d23h (x8 over 6d23h)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeAllocatableEnforced  6d23h                  kubelet          Updated Node Allocatable limit across pods
  Normal   Starting                 6d23h                  kubelet          Starting kubelet.
  Normal   NodeAllocatableEnforced  6d23h                  kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory  6d23h                  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    6d23h                  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     6d23h                  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   RegisteredNode           6d23h                  node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   NodeNotReady             6d23h                  kubelet          Node minikube status is now: NodeNotReady
  Warning  ContainerGCFailed        6d23h                  kubelet          rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing: dial unix /var/run/cri-dockerd.sock: connect: connection refused"
  Normal   RegisteredNode           6d23h                  node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   Starting                 19m                    kubelet          Starting kubelet.
  Normal   NodeHasNoDiskPressure    19m (x8 over 19m)      kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientMemory  19m (x8 over 19m)      kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasSufficientPID     19m (x7 over 19m)      kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  19m                    kubelet          Updated Node Allocatable limit across pods
  Warning  Rebooted                 19m                    kubelet          Node minikube has been rebooted, boot id: 1b9f28b9-b550-49d8-bca6-f16f73e683e4
  Normal   RegisteredNode           18m                    node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[May27 15:11] APIC calibration not consistent with PM-Timer: 130ms instead of 100ms
[  +0.015926] TSC synchronization [CPU#0 -> CPU#1]:
[  +0.000000] Measured 109519 cycles TSC warp between CPUs, turning off TSC clock.
[  +0.077487] acpi PNP0A03:00: fail to add MMCONFIG information, can't access extended configuration space under this bridge
[  +0.931372] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +0.378437] systemd[1]: Invalid DMI field header.
[  +1.958954] vmwgfx 0000:00:02.0: [drm] *ERROR* vmwgfx seems to be running on an unsupported hypervisor.
[  +0.000004] vmwgfx 0000:00:02.0: [drm] *ERROR* This configuration is likely broken.
[  +0.000003] vmwgfx 0000:00:02.0: [drm] *ERROR* Please switch to a supported graphics device to avoid problems.
[  +3.476524] systemd[1]: Invalid DMI field header.
[  +1.979857] systemd-journald[538]: File /var/log/journal/c63f55d1db1647f19a59a46d9da6451b/system.journal corrupted or uncleanly shut down, renaming and replacing.
[May27 15:12] kauditd_printk_skb: 16 callbacks suppressed
[ +19.415800] systemd-journald[538]: File /var/log/journal/c63f55d1db1647f19a59a46d9da6451b/user-1000.journal corrupted or uncleanly shut down, renaming and replacing.


==> etcd [44dae3fa039a] <==
{"level":"info","ts":"2025-05-27T15:37:19.623466Z","caller":"traceutil/trace.go:171","msg":"trace[1489797906] linearizableReadLoop","detail":"{readStateIndex:3972; appliedIndex:3971; }","duration":"414.95325ms","start":"2025-05-27T15:37:19.208493Z","end":"2025-05-27T15:37:19.623446Z","steps":["trace[1489797906] 'read index received'  (duration: 64.429258ms)","trace[1489797906] 'applied index is now lower than readState.Index'  (duration: 350.522771ms)"],"step_count":2}
{"level":"info","ts":"2025-05-27T15:37:19.636670Z","caller":"traceutil/trace.go:171","msg":"trace[942814245] transaction","detail":"{read_only:false; response_revision:3299; number_of_response:1; }","duration":"466.251494ms","start":"2025-05-27T15:37:19.170400Z","end":"2025-05-27T15:37:19.636652Z","steps":["trace[942814245] 'process raft request'  (duration: 102.56227ms)","trace[942814245] 'compare'  (duration: 249.38231ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-27T15:37:19.641407Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-27T15:37:19.170383Z","time spent":"470.964133ms","remote":"127.0.0.1:54574","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":3017,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/pods/default/nginx-deploy-75cff64695-btqq8\" mod_revision:3229 > success:<request_put:<key:\"/registry/pods/default/nginx-deploy-75cff64695-btqq8\" value_size:2957 >> failure:<request_range:<key:\"/registry/pods/default/nginx-deploy-75cff64695-btqq8\" > >"}
{"level":"warn","ts":"2025-05-27T15:37:19.689384Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"480.882805ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/prioritylevelconfigurations/\" range_end:\"/registry/prioritylevelconfigurations0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2025-05-27T15:37:19.689902Z","caller":"traceutil/trace.go:171","msg":"trace[1150865114] range","detail":"{range_begin:/registry/prioritylevelconfigurations/; range_end:/registry/prioritylevelconfigurations0; response_count:0; response_revision:3299; }","duration":"481.446667ms","start":"2025-05-27T15:37:19.208442Z","end":"2025-05-27T15:37:19.689888Z","steps":["trace[1150865114] 'agreement among raft nodes before linearized reading'  (duration: 428.129152ms)","trace[1150865114] 'count revisions from in-memory index tree'  (duration: 52.728701ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-27T15:37:19.689944Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-27T15:37:19.208419Z","time spent":"481.513178ms","remote":"127.0.0.1:54804","response type":"/etcdserverpb.KV/Range","request count":0,"request size":82,"response count":8,"response size":31,"request content":"key:\"/registry/prioritylevelconfigurations/\" range_end:\"/registry/prioritylevelconfigurations0\" count_only:true "}
{"level":"warn","ts":"2025-05-27T15:37:19.694206Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"150.518391ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-27T15:37:19.700813Z","caller":"traceutil/trace.go:171","msg":"trace[327138409] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:3300; }","duration":"157.127627ms","start":"2025-05-27T15:37:19.543672Z","end":"2025-05-27T15:37:19.700800Z","steps":["trace[327138409] 'agreement among raft nodes before linearized reading'  (duration: 150.505008ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-27T15:37:19.695449Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"380.015097ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 ","response":"range_response_count:1 size:1109"}
{"level":"warn","ts":"2025-05-27T15:37:19.695532Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"380.254107ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/specs/default/nginx-deploy\" limit:1 ","response":"range_response_count:1 size:685"}
{"level":"info","ts":"2025-05-27T15:37:19.701558Z","caller":"traceutil/trace.go:171","msg":"trace[853795349] range","detail":"{range_begin:/registry/services/specs/default/nginx-deploy; range_end:; response_count:1; response_revision:3300; }","duration":"386.296048ms","start":"2025-05-27T15:37:19.315250Z","end":"2025-05-27T15:37:19.701546Z","steps":["trace[853795349] 'agreement among raft nodes before linearized reading'  (duration: 380.216544ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-27T15:37:19.701603Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-27T15:37:19.315233Z","time spent":"386.348443ms","remote":"127.0.0.1:54580","response type":"/etcdserverpb.KV/Range","request count":0,"request size":49,"response count":1,"response size":709,"request content":"key:\"/registry/services/specs/default/nginx-deploy\" limit:1 "}
{"level":"info","ts":"2025-05-27T15:37:19.701799Z","caller":"traceutil/trace.go:171","msg":"trace[1293873738] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:3300; }","duration":"386.376975ms","start":"2025-05-27T15:37:19.315412Z","end":"2025-05-27T15:37:19.701789Z","steps":["trace[1293873738] 'agreement among raft nodes before linearized reading'  (duration: 379.97318ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-27T15:37:19.701883Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-27T15:37:19.315407Z","time spent":"386.426421ms","remote":"127.0.0.1:54548","response type":"/etcdserverpb.KV/Range","request count":0,"request size":69,"response count":1,"response size":1133,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 "}
{"level":"warn","ts":"2025-05-27T15:37:20.088011Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"115.4649ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128037532468694498 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/default/nginx-deploy\" mod_revision:3300 > success:<request_delete_range:<key:\"/registry/services/endpoints/default/nginx-deploy\" > > failure:<request_range:<key:\"/registry/services/endpoints/default/nginx-deploy\" > >>","response":"size:18"}
{"level":"info","ts":"2025-05-27T15:37:20.089332Z","caller":"traceutil/trace.go:171","msg":"trace[510888751] transaction","detail":"{read_only:false; number_of_response:1; response_revision:3304; }","duration":"292.042755ms","start":"2025-05-27T15:37:19.797273Z","end":"2025-05-27T15:37:20.089316Z","steps":["trace[510888751] 'compare'  (duration: 115.368139ms)"],"step_count":1}
{"level":"info","ts":"2025-05-27T15:37:20.117462Z","caller":"traceutil/trace.go:171","msg":"trace[1911227955] transaction","detail":"{read_only:false; response_revision:3307; number_of_response:1; }","duration":"116.013049ms","start":"2025-05-27T15:37:20.001432Z","end":"2025-05-27T15:37:20.117445Z","steps":["trace[1911227955] 'process raft request'  (duration: 115.975717ms)"],"step_count":1}
{"level":"info","ts":"2025-05-27T15:37:20.119305Z","caller":"traceutil/trace.go:171","msg":"trace[458397397] transaction","detail":"{read_only:false; number_of_response:1; response_revision:3305; }","duration":"316.552426ms","start":"2025-05-27T15:37:19.802740Z","end":"2025-05-27T15:37:20.119293Z","steps":["trace[458397397] 'process raft request'  (duration: 285.519102ms)","trace[458397397] 'compare'  (duration: 28.858454ms)"],"step_count":2}
{"level":"info","ts":"2025-05-27T15:37:20.119341Z","caller":"traceutil/trace.go:171","msg":"trace[1672967926] transaction","detail":"{read_only:false; number_of_response:1; response_revision:3305; }","duration":"315.849164ms","start":"2025-05-27T15:37:19.803486Z","end":"2025-05-27T15:37:20.119335Z","steps":["trace[1672967926] 'process raft request'  (duration: 313.707073ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-27T15:37:20.123985Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-27T15:37:19.803474Z","time spent":"320.475189ms","remote":"127.0.0.1:54548","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":53,"response count":0,"response size":38,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/default/nginx-deploy\" mod_revision:3300 > success:<request_delete_range:<key:\"/registry/services/endpoints/default/nginx-deploy\" > > failure:<request_range:<key:\"/registry/services/endpoints/default/nginx-deploy\" > >"}
{"level":"info","ts":"2025-05-27T15:37:20.119391Z","caller":"traceutil/trace.go:171","msg":"trace[101500797] linearizableReadLoop","detail":"{readStateIndex:3980; appliedIndex:3976; }","duration":"185.573335ms","start":"2025-05-27T15:37:19.933811Z","end":"2025-05-27T15:37:20.119384Z","steps":["trace[101500797] 'read index received'  (duration: 1.428364ms)","trace[101500797] 'applied index is now lower than readState.Index'  (duration: 184.144283ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-27T15:37:20.119438Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"185.614823ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2025-05-27T15:37:20.123142Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-27T15:37:19.802726Z","time spent":"320.362972ms","remote":"127.0.0.1:54660","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":55,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/endpointslices/default/nginx-deploy-d57nj\" mod_revision:3301 > success:<request_delete_range:<key:\"/registry/endpointslices/default/nginx-deploy-d57nj\" > > failure:<request_range:<key:\"/registry/endpointslices/default/nginx-deploy-d57nj\" > >"}
{"level":"info","ts":"2025-05-27T15:37:20.124649Z","caller":"traceutil/trace.go:171","msg":"trace[1362515748] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:3307; }","duration":"190.282158ms","start":"2025-05-27T15:37:19.933799Z","end":"2025-05-27T15:37:20.124081Z","steps":["trace[1362515748] 'agreement among raft nodes before linearized reading'  (duration: 185.606676ms)"],"step_count":1}
{"level":"info","ts":"2025-05-27T15:37:20.119461Z","caller":"traceutil/trace.go:171","msg":"trace[1330291191] transaction","detail":"{read_only:false; response_revision:3306; number_of_response:1; }","duration":"304.81599ms","start":"2025-05-27T15:37:19.814640Z","end":"2025-05-27T15:37:20.119456Z","steps":["trace[1330291191] 'process raft request'  (duration: 302.573834ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-27T15:37:20.126007Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-27T15:37:19.814627Z","time spent":"311.310768ms","remote":"127.0.0.1:54574","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":2798,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/pods/default/nginx-deploy-75cff64695-btqq8\" mod_revision:3299 > success:<request_put:<key:\"/registry/pods/default/nginx-deploy-75cff64695-btqq8\" value_size:2738 >> failure:<request_range:<key:\"/registry/pods/default/nginx-deploy-75cff64695-btqq8\" > >"}
{"level":"info","ts":"2025-05-27T15:37:21.140024Z","caller":"traceutil/trace.go:171","msg":"trace[1736612061] transaction","detail":"{read_only:false; response_revision:3315; number_of_response:1; }","duration":"117.925695ms","start":"2025-05-27T15:37:21.022057Z","end":"2025-05-27T15:37:21.139982Z","steps":["trace[1736612061] 'process raft request'  (duration: 94.227324ms)","trace[1736612061] 'compare'  (duration: 18.674462ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-27T15:37:21.236553Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"141.099029ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-27T15:37:21.237648Z","caller":"traceutil/trace.go:171","msg":"trace[49261889] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:3317; }","duration":"141.290097ms","start":"2025-05-27T15:37:21.095410Z","end":"2025-05-27T15:37:21.236700Z","steps":["trace[49261889] 'agreement among raft nodes before linearized reading'  (duration: 134.45609ms)"],"step_count":1}
{"level":"info","ts":"2025-05-27T15:37:21.237970Z","caller":"traceutil/trace.go:171","msg":"trace[907249693] transaction","detail":"{read_only:false; response_revision:3317; number_of_response:1; }","duration":"108.577671ms","start":"2025-05-27T15:37:21.129378Z","end":"2025-05-27T15:37:21.237955Z","steps":["trace[907249693] 'process raft request'  (duration: 100.376598ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-27T15:37:21.238110Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"116.80065ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/deployments/default/nginx-deploy\" limit:1 ","response":"range_response_count:1 size:1652"}
{"level":"info","ts":"2025-05-27T15:37:21.238123Z","caller":"traceutil/trace.go:171","msg":"trace[921895228] range","detail":"{range_begin:/registry/deployments/default/nginx-deploy; range_end:; response_count:1; response_revision:3317; }","duration":"116.851562ms","start":"2025-05-27T15:37:21.121268Z","end":"2025-05-27T15:37:21.238119Z","steps":["trace[921895228] 'agreement among raft nodes before linearized reading'  (duration: 116.763674ms)"],"step_count":1}
{"level":"info","ts":"2025-05-27T15:37:21.350763Z","caller":"traceutil/trace.go:171","msg":"trace[137808590] transaction","detail":"{read_only:false; response_revision:3318; number_of_response:1; }","duration":"164.81229ms","start":"2025-05-27T15:37:21.185935Z","end":"2025-05-27T15:37:21.350747Z","steps":["trace[137808590] 'process raft request'  (duration: 120.788942ms)","trace[137808590] 'compare'  (duration: 43.453987ms)"],"step_count":2}
{"level":"info","ts":"2025-05-27T15:37:21.824538Z","caller":"traceutil/trace.go:171","msg":"trace[432416323] linearizableReadLoop","detail":"{readStateIndex:4003; appliedIndex:4002; }","duration":"170.338543ms","start":"2025-05-27T15:37:21.654185Z","end":"2025-05-27T15:37:21.824524Z","steps":["trace[432416323] 'read index received'  (duration: 111.974047ms)","trace[432416323] 'applied index is now lower than readState.Index'  (duration: 58.364033ms)"],"step_count":2}
{"level":"info","ts":"2025-05-27T15:37:21.826914Z","caller":"traceutil/trace.go:171","msg":"trace[685139467] transaction","detail":"{read_only:false; response_revision:3327; number_of_response:1; }","duration":"172.900491ms","start":"2025-05-27T15:37:21.653987Z","end":"2025-05-27T15:37:21.826888Z","steps":["trace[685139467] 'process raft request'  (duration: 112.166166ms)","trace[685139467] 'compare'  (duration: 58.245572ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-27T15:37:21.828760Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"174.547332ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/endpointslices/default/kubernetes\" limit:1 ","response":"range_response_count:1 size:474"}
{"level":"info","ts":"2025-05-27T15:37:21.830226Z","caller":"traceutil/trace.go:171","msg":"trace[948339305] range","detail":"{range_begin:/registry/endpointslices/default/kubernetes; range_end:; response_count:1; response_revision:3327; }","duration":"176.033314ms","start":"2025-05-27T15:37:21.654172Z","end":"2025-05-27T15:37:21.830205Z","steps":["trace[948339305] 'agreement among raft nodes before linearized reading'  (duration: 174.465514ms)"],"step_count":1}
{"level":"info","ts":"2025-05-27T15:37:21.995529Z","caller":"traceutil/trace.go:171","msg":"trace[1932218046] linearizableReadLoop","detail":"{readStateIndex:4005; appliedIndex:4003; }","duration":"170.89787ms","start":"2025-05-27T15:37:21.824591Z","end":"2025-05-27T15:37:21.995489Z","steps":["trace[1932218046] 'read index received'  (duration: 99.200658ms)","trace[1932218046] 'applied index is now lower than readState.Index'  (duration: 71.696418ms)"],"step_count":2}
{"level":"info","ts":"2025-05-27T15:37:21.997893Z","caller":"traceutil/trace.go:171","msg":"trace[2104440009] transaction","detail":"{read_only:false; response_revision:3329; number_of_response:1; }","duration":"321.870377ms","start":"2025-05-27T15:37:21.675483Z","end":"2025-05-27T15:37:21.997353Z","steps":["trace[2104440009] 'process raft request'  (duration: 319.880852ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-27T15:37:22.008476Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-27T15:37:21.675467Z","time spent":"322.500658ms","remote":"127.0.0.1:54574","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":2680,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/pods/default/nginx-deploy-75cff64695-gvdpt\" mod_revision:3318 > success:<request_put:<key:\"/registry/pods/default/nginx-deploy-75cff64695-gvdpt\" value_size:2620 >> failure:<request_range:<key:\"/registry/pods/default/nginx-deploy-75cff64695-gvdpt\" > >"}
{"level":"warn","ts":"2025-05-27T15:37:22.014027Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"191.538185ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 ","response":"range_response_count:1 size:1109"}
{"level":"info","ts":"2025-05-27T15:37:22.014103Z","caller":"traceutil/trace.go:171","msg":"trace[386941680] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:3329; }","duration":"191.630978ms","start":"2025-05-27T15:37:21.822444Z","end":"2025-05-27T15:37:22.014075Z","steps":["trace[386941680] 'agreement among raft nodes before linearized reading'  (duration: 191.42194ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-27T15:37:22.031613Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"276.607497ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/default/default\" limit:1 ","response":"range_response_count:1 size:171"}
{"level":"info","ts":"2025-05-27T15:37:22.054102Z","caller":"traceutil/trace.go:171","msg":"trace[2090511333] range","detail":"{range_begin:/registry/serviceaccounts/default/default; range_end:; response_count:1; response_revision:3329; }","duration":"299.080105ms","start":"2025-05-27T15:37:21.754935Z","end":"2025-05-27T15:37:22.054015Z","steps":["trace[2090511333] 'agreement among raft nodes before linearized reading'  (duration: 259.381571ms)","trace[2090511333] 'range keys from bolt db'  (duration: 17.206141ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-27T15:37:22.055227Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"160.656659ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-27T15:37:22.055279Z","caller":"traceutil/trace.go:171","msg":"trace[1311985252] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:3329; }","duration":"160.720877ms","start":"2025-05-27T15:37:21.894543Z","end":"2025-05-27T15:37:22.055264Z","steps":["trace[1311985252] 'agreement among raft nodes before linearized reading'  (duration: 160.626115ms)"],"step_count":1}
{"level":"info","ts":"2025-05-27T15:37:22.056312Z","caller":"traceutil/trace.go:171","msg":"trace[1032296961] transaction","detail":"{read_only:false; response_revision:3328; number_of_response:1; }","duration":"332.982775ms","start":"2025-05-27T15:37:21.654218Z","end":"2025-05-27T15:37:21.987200Z","steps":["trace[1032296961] 'process raft request'  (duration: 269.619378ms)","trace[1032296961] 'compare'  (duration: 57.691849ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-27T15:37:22.056435Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-27T15:37:21.654211Z","time spent":"402.146852ms","remote":"127.0.0.1:54660","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1085,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/endpointslices/default/nginx-deploy-mx7mr\" mod_revision:0 > success:<request_put:<key:\"/registry/endpointslices/default/nginx-deploy-mx7mr\" value_size:1026 >> failure:<>"}
{"level":"info","ts":"2025-05-27T15:37:23.942618Z","caller":"traceutil/trace.go:171","msg":"trace[141965103] transaction","detail":"{read_only:false; response_revision:3333; number_of_response:1; }","duration":"117.570176ms","start":"2025-05-27T15:37:23.825028Z","end":"2025-05-27T15:37:23.942598Z","steps":["trace[141965103] 'process raft request'  (duration: 77.33809ms)","trace[141965103] 'compare'  (duration: 36.507228ms)"],"step_count":2}
{"level":"info","ts":"2025-05-27T15:38:23.008864Z","caller":"traceutil/trace.go:171","msg":"trace[308277337] linearizableReadLoop","detail":"{readStateIndex:4095; appliedIndex:4094; }","duration":"185.528397ms","start":"2025-05-27T15:38:22.823303Z","end":"2025-05-27T15:38:23.008831Z","steps":["trace[308277337] 'read index received'  (duration: 184.191028ms)","trace[308277337] 'applied index is now lower than readState.Index'  (duration: 1.33603ms)"],"step_count":2}
{"level":"info","ts":"2025-05-27T15:38:23.010011Z","caller":"traceutil/trace.go:171","msg":"trace[87138818] transaction","detail":"{read_only:false; response_revision:3405; number_of_response:1; }","duration":"243.168387ms","start":"2025-05-27T15:38:22.766816Z","end":"2025-05-27T15:38:23.009985Z","steps":["trace[87138818] 'process raft request'  (duration: 241.14429ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-27T15:38:23.010556Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"123.562358ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-27T15:38:23.010619Z","caller":"traceutil/trace.go:171","msg":"trace[1196897992] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:3405; }","duration":"123.657969ms","start":"2025-05-27T15:38:22.886945Z","end":"2025-05-27T15:38:23.010603Z","steps":["trace[1196897992] 'agreement among raft nodes before linearized reading'  (duration: 123.525365ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-27T15:38:23.010846Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"187.535319ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-27T15:38:23.010882Z","caller":"traceutil/trace.go:171","msg":"trace[2008464595] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:3405; }","duration":"187.573568ms","start":"2025-05-27T15:38:22.823297Z","end":"2025-05-27T15:38:23.010871Z","steps":["trace[2008464595] 'agreement among raft nodes before linearized reading'  (duration: 187.520927ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-27T15:38:23.558429Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"506.031729ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-27T15:38:23.558545Z","caller":"traceutil/trace.go:171","msg":"trace[2070325062] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:3405; }","duration":"506.18201ms","start":"2025-05-27T15:38:23.052351Z","end":"2025-05-27T15:38:23.558533Z","steps":["trace[2070325062] 'range keys from in-memory index tree'  (duration: 505.976299ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-27T15:38:23.558676Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-27T15:38:23.052333Z","time spent":"506.335405ms","remote":"127.0.0.1:54380","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2025-05-27T15:38:25.437369Z","caller":"traceutil/trace.go:171","msg":"trace[1527079911] transaction","detail":"{read_only:false; response_revision:3406; number_of_response:1; }","duration":"497.189032ms","start":"2025-05-27T15:38:24.940144Z","end":"2025-05-27T15:38:25.437334Z","steps":["trace[1527079911] 'process raft request'  (duration: 496.963275ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-27T15:38:25.437525Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-27T15:38:24.940125Z","time spent":"497.343743ms","remote":"127.0.0.1:54548","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:3404 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}


==> etcd [7a1d6ead3790] <==
{"level":"info","ts":"2025-05-20T16:27:44.804334Z","caller":"traceutil/trace.go:171","msg":"trace[401333816] linearizableReadLoop","detail":"{readStateIndex:2262; appliedIndex:2261; }","duration":"469.197117ms","start":"2025-05-20T16:27:44.335119Z","end":"2025-05-20T16:27:44.804316Z","steps":["trace[401333816] 'read index received'  (duration: 361.672062ms)","trace[401333816] 'applied index is now lower than readState.Index'  (duration: 107.523053ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-20T16:27:44.804439Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"469.326915ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-20T16:27:44.804459Z","caller":"traceutil/trace.go:171","msg":"trace[1799491949] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1915; }","duration":"469.38904ms","start":"2025-05-20T16:27:44.335064Z","end":"2025-05-20T16:27:44.804453Z","steps":["trace[1799491949] 'agreement among raft nodes before linearized reading'  (duration: 469.302555ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-20T16:27:44.804481Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-20T16:27:44.335054Z","time spent":"469.421702ms","remote":"127.0.0.1:57992","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2025-05-20T16:27:44.805915Z","caller":"traceutil/trace.go:171","msg":"trace[1300787877] transaction","detail":"{read_only:false; response_revision:1915; number_of_response:1; }","duration":"471.271761ms","start":"2025-05-20T16:27:44.334625Z","end":"2025-05-20T16:27:44.805897Z","steps":["trace[1300787877] 'process raft request'  (duration: 361.749066ms)","trace[1300787877] 'compare'  (duration: 102.171484ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-20T16:27:44.806003Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-20T16:27:44.334615Z","time spent":"471.335801ms","remote":"127.0.0.1:58012","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":117,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:1907 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128037378318501438 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >"}
{"level":"warn","ts":"2025-05-20T16:28:06.816150Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"914.014419ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/daemonsets/\" range_end:\"/registry/daemonsets0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2025-05-20T16:28:06.817052Z","caller":"traceutil/trace.go:171","msg":"trace[932264300] range","detail":"{range_begin:/registry/daemonsets/; range_end:/registry/daemonsets0; response_count:0; response_revision:1931; }","duration":"914.936632ms","start":"2025-05-20T16:28:05.902080Z","end":"2025-05-20T16:28:06.817017Z","steps":["trace[932264300] 'get authentication metadata'  (duration: 912.815419ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-20T16:28:06.817112Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-20T16:28:05.902064Z","time spent":"915.030932ms","remote":"127.0.0.1:58418","response type":"/etcdserverpb.KV/Range","request count":0,"request size":48,"response count":1,"response size":31,"request content":"key:\"/registry/daemonsets/\" range_end:\"/registry/daemonsets0\" count_only:true "}
{"level":"info","ts":"2025-05-20T16:28:14.358144Z","caller":"traceutil/trace.go:171","msg":"trace[85639674] linearizableReadLoop","detail":"{readStateIndex:2289; appliedIndex:2288; }","duration":"168.113872ms","start":"2025-05-20T16:28:14.189996Z","end":"2025-05-20T16:28:14.358109Z","steps":["trace[85639674] 'read index received'  (duration: 167.000677ms)","trace[85639674] 'applied index is now lower than readState.Index'  (duration: 1.111971ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-20T16:28:14.358464Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"168.43867ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-20T16:28:14.358490Z","caller":"traceutil/trace.go:171","msg":"trace[1499418109] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1936; }","duration":"168.511705ms","start":"2025-05-20T16:28:14.189970Z","end":"2025-05-20T16:28:14.358481Z","steps":["trace[1499418109] 'agreement among raft nodes before linearized reading'  (duration: 168.438223ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-20T16:28:14.409936Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"119.099787ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/configmaps/\" range_end:\"/registry/configmaps0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2025-05-20T16:28:14.410307Z","caller":"traceutil/trace.go:171","msg":"trace[1077658180] range","detail":"{range_begin:/registry/configmaps/; range_end:/registry/configmaps0; response_count:0; response_revision:1936; }","duration":"119.358465ms","start":"2025-05-20T16:28:14.290788Z","end":"2025-05-20T16:28:14.410146Z","steps":["trace[1077658180] 'agreement among raft nodes before linearized reading'  (duration: 73.592135ms)","trace[1077658180] 'count revisions from in-memory index tree'  (duration: 45.493918ms)"],"step_count":2}
{"level":"info","ts":"2025-05-20T16:28:34.713842Z","caller":"traceutil/trace.go:171","msg":"trace[1733947224] transaction","detail":"{read_only:false; response_revision:1953; number_of_response:1; }","duration":"119.357056ms","start":"2025-05-20T16:28:34.594452Z","end":"2025-05-20T16:28:34.713809Z","steps":["trace[1733947224] 'process raft request'  (duration: 87.978792ms)","trace[1733947224] 'compare'  (duration: 26.595367ms)"],"step_count":2}
{"level":"info","ts":"2025-05-20T16:28:47.294869Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1725}
{"level":"info","ts":"2025-05-20T16:28:47.333329Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":1725,"took":"36.840156ms","hash":352312285,"current-db-size-bytes":2711552,"current-db-size":"2.7 MB","current-db-size-in-use-bytes":1912832,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-05-20T16:28:47.333423Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":352312285,"revision":1725,"compact-revision":1450}
{"level":"warn","ts":"2025-05-20T16:29:25.015811Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"226.260637ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-20T16:29:25.015865Z","caller":"traceutil/trace.go:171","msg":"trace[103025921] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1993; }","duration":"226.335555ms","start":"2025-05-20T16:29:24.789520Z","end":"2025-05-20T16:29:25.015855Z","steps":["trace[103025921] 'range keys from in-memory index tree'  (duration: 226.249959ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-20T16:29:25.015920Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"182.588939ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-20T16:29:25.015928Z","caller":"traceutil/trace.go:171","msg":"trace[1823050389] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1993; }","duration":"182.599568ms","start":"2025-05-20T16:29:24.833325Z","end":"2025-05-20T16:29:25.015925Z","steps":["trace[1823050389] 'range keys from in-memory index tree'  (duration: 182.58136ms)"],"step_count":1}
{"level":"info","ts":"2025-05-20T16:29:25.016186Z","caller":"traceutil/trace.go:171","msg":"trace[784892597] transaction","detail":"{read_only:false; response_revision:1994; number_of_response:1; }","duration":"269.278481ms","start":"2025-05-20T16:29:24.746893Z","end":"2025-05-20T16:29:25.016171Z","steps":["trace[784892597] 'process raft request'  (duration: 190.487395ms)","trace[784892597] 'compare'  (duration: 77.999225ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-20T16:29:43.121983Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"219.261803ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-20T16:29:43.123839Z","caller":"traceutil/trace.go:171","msg":"trace[1205647758] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:2007; }","duration":"221.127041ms","start":"2025-05-20T16:29:42.902700Z","end":"2025-05-20T16:29:43.123827Z","steps":["trace[1205647758] 'agreement among raft nodes before linearized reading'  (duration: 219.238274ms)"],"step_count":1}
{"level":"info","ts":"2025-05-20T16:30:12.904770Z","caller":"traceutil/trace.go:171","msg":"trace[1679992630] transaction","detail":"{read_only:false; response_revision:2029; number_of_response:1; }","duration":"962.427993ms","start":"2025-05-20T16:30:11.937924Z","end":"2025-05-20T16:30:12.900352Z","steps":["trace[1679992630] 'process raft request'  (duration: 959.295132ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-20T16:30:12.905141Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-20T16:30:11.921295Z","time spent":"983.630213ms","remote":"127.0.0.1:58248","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":673,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:2021 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"info","ts":"2025-05-20T16:30:14.243454Z","caller":"traceutil/trace.go:171","msg":"trace[1897270824] transaction","detail":"{read_only:false; response_revision:2032; number_of_response:1; }","duration":"130.833028ms","start":"2025-05-20T16:30:14.112610Z","end":"2025-05-20T16:30:14.243443Z","steps":["trace[1897270824] 'process raft request'  (duration: 130.785365ms)"],"step_count":1}
{"level":"info","ts":"2025-05-20T16:30:14.244413Z","caller":"traceutil/trace.go:171","msg":"trace[1957227438] transaction","detail":"{read_only:false; response_revision:2031; number_of_response:1; }","duration":"182.134398ms","start":"2025-05-20T16:30:14.062250Z","end":"2025-05-20T16:30:14.244385Z","steps":["trace[1957227438] 'process raft request'  (duration: 181.058331ms)"],"step_count":1}
{"level":"info","ts":"2025-05-20T16:30:14.245223Z","caller":"traceutil/trace.go:171","msg":"trace[114212828] linearizableReadLoop","detail":"{readStateIndex:2409; appliedIndex:2408; }","duration":"182.746061ms","start":"2025-05-20T16:30:14.062179Z","end":"2025-05-20T16:30:14.244925Z","steps":["trace[114212828] 'read index received'  (duration: 45.522013ms)","trace[114212828] 'applied index is now lower than readState.Index'  (duration: 137.222032ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-20T16:30:14.245497Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"183.295137ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-20T16:30:14.245540Z","caller":"traceutil/trace.go:171","msg":"trace[28241671] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:2032; }","duration":"183.381146ms","start":"2025-05-20T16:30:14.062147Z","end":"2025-05-20T16:30:14.245528Z","steps":["trace[28241671] 'agreement among raft nodes before linearized reading'  (duration: 183.29031ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-20T16:30:14.248503Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"170.270413ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-20T16:30:14.248987Z","caller":"traceutil/trace.go:171","msg":"trace[1218909953] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:2032; }","duration":"173.50219ms","start":"2025-05-20T16:30:14.075464Z","end":"2025-05-20T16:30:14.248966Z","steps":["trace[1218909953] 'agreement among raft nodes before linearized reading'  (duration: 170.251446ms)"],"step_count":1}
{"level":"info","ts":"2025-05-20T16:30:14.243473Z","caller":"traceutil/trace.go:171","msg":"trace[778131697] transaction","detail":"{read_only:false; response_revision:2030; number_of_response:1; }","duration":"181.386725ms","start":"2025-05-20T16:30:14.062059Z","end":"2025-05-20T16:30:14.243446Z","steps":["trace[778131697] 'process raft request'  (duration: 123.625614ms)","trace[778131697] 'compare'  (duration: 57.43277ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-20T16:30:14.256901Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"136.575081ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kubernetes-dashboard/dashboard-metrics-scraper\" limit:1 ","response":"range_response_count:1 size:887"}
{"level":"info","ts":"2025-05-20T16:30:14.262247Z","caller":"traceutil/trace.go:171","msg":"trace[754165637] range","detail":"{range_begin:/registry/services/endpoints/kubernetes-dashboard/dashboard-metrics-scraper; range_end:; response_count:1; response_revision:2032; }","duration":"141.487686ms","start":"2025-05-20T16:30:14.120273Z","end":"2025-05-20T16:30:14.261761Z","steps":["trace[754165637] 'agreement among raft nodes before linearized reading'  (duration: 136.433334ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-20T16:31:35.133494Z","caller":"wal/wal.go:805","msg":"slow fdatasync","took":"1.153186473s","expected-duration":"1s"}
{"level":"warn","ts":"2025-05-20T16:31:35.170336Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-20T16:31:33.956954Z","time spent":"1.213377089s","remote":"127.0.0.1:58058","response type":"/etcdserverpb.Lease/LeaseGrant","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"warn","ts":"2025-05-20T16:31:35.203412Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"112.656477ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-20T16:31:35.203451Z","caller":"traceutil/trace.go:171","msg":"trace[1169598409] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:2092; }","duration":"112.704834ms","start":"2025-05-20T16:31:35.090738Z","end":"2025-05-20T16:31:35.203443Z","steps":["trace[1169598409] 'agreement among raft nodes before linearized reading'  (duration: 112.621497ms)"],"step_count":1}
{"level":"info","ts":"2025-05-20T16:32:39.212730Z","caller":"traceutil/trace.go:171","msg":"trace[124250457] transaction","detail":"{read_only:false; response_revision:2143; number_of_response:1; }","duration":"210.87348ms","start":"2025-05-20T16:32:39.001841Z","end":"2025-05-20T16:32:39.212715Z","steps":["trace[124250457] 'process raft request'  (duration: 170.971224ms)","trace[124250457] 'compare'  (duration: 39.131709ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-20T16:33:31.642125Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"146.888736ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-20T16:33:31.649777Z","caller":"traceutil/trace.go:171","msg":"trace[242802951] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:2180; }","duration":"153.344603ms","start":"2025-05-20T16:33:31.495204Z","end":"2025-05-20T16:33:31.648549Z","steps":["trace[242802951] 'range keys from in-memory index tree'  (duration: 146.873975ms)"],"step_count":1}
{"level":"info","ts":"2025-05-20T16:33:31.692253Z","caller":"traceutil/trace.go:171","msg":"trace[830994413] linearizableReadLoop","detail":"{readStateIndex:2600; appliedIndex:2599; }","duration":"182.148268ms","start":"2025-05-20T16:33:31.510093Z","end":"2025-05-20T16:33:31.692241Z","steps":["trace[830994413] 'read index received'  (duration: 140.405834ms)","trace[830994413] 'applied index is now lower than readState.Index'  (duration: 41.74161ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-20T16:33:31.692819Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"182.633032ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/namespaces/\" range_end:\"/registry/namespaces0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2025-05-20T16:33:31.693167Z","caller":"traceutil/trace.go:171","msg":"trace[1383460599] range","detail":"{range_begin:/registry/namespaces/; range_end:/registry/namespaces0; response_count:0; response_revision:2180; }","duration":"183.099418ms","start":"2025-05-20T16:33:31.510057Z","end":"2025-05-20T16:33:31.693156Z","steps":["trace[1383460599] 'agreement among raft nodes before linearized reading'  (duration: 182.638402ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-20T16:33:31.693908Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"183.75902ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-20T16:33:31.694732Z","caller":"traceutil/trace.go:171","msg":"trace[160657484] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:2181; }","duration":"183.835825ms","start":"2025-05-20T16:33:31.510134Z","end":"2025-05-20T16:33:31.693970Z","steps":["trace[160657484] 'agreement among raft nodes before linearized reading'  (duration: 183.74575ms)"],"step_count":1}
{"level":"info","ts":"2025-05-20T16:33:31.698037Z","caller":"traceutil/trace.go:171","msg":"trace[503952991] transaction","detail":"{read_only:false; response_revision:2181; number_of_response:1; }","duration":"184.141926ms","start":"2025-05-20T16:33:31.513883Z","end":"2025-05-20T16:33:31.698025Z","steps":["trace[503952991] 'process raft request'  (duration: 178.302808ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-20T16:33:41.787399Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"374.246454ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128037378318503120 > lease_revoke:<id:70cc96ee6fc58c94>","response":"size:29"}
{"level":"info","ts":"2025-05-20T16:33:47.373267Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1964}
{"level":"info","ts":"2025-05-20T16:33:47.413186Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":1964,"took":"39.348421ms","hash":2142755463,"current-db-size-bytes":2711552,"current-db-size":"2.7 MB","current-db-size-in-use-bytes":1761280,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-05-20T16:33:47.413301Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2142755463,"revision":1964,"compact-revision":1725}
{"level":"warn","ts":"2025-05-20T16:34:12.893907Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"956.441806ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128037378318503248 > lease_revoke:<id:70cc96ee6fc58d22>","response":"size:29"}
{"level":"info","ts":"2025-05-20T16:34:12.918167Z","caller":"traceutil/trace.go:171","msg":"trace[1954231142] transaction","detail":"{read_only:false; response_revision:2210; number_of_response:1; }","duration":"1.006422606s","start":"2025-05-20T16:34:11.911722Z","end":"2025-05-20T16:34:12.918145Z","steps":["trace[1954231142] 'process raft request'  (duration: 982.290063ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-20T16:34:12.921896Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-20T16:34:11.911705Z","time spent":"1.01010005s","remote":"127.0.0.1:58248","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":673,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:2204 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"warn","ts":"2025-05-20T16:34:13.020049Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"113.148382ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 ","response":"range_response_count:1 size:1109"}
{"level":"info","ts":"2025-05-20T16:34:13.122449Z","caller":"traceutil/trace.go:171","msg":"trace[1721764892] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:2210; }","duration":"163.75368ms","start":"2025-05-20T16:34:12.906828Z","end":"2025-05-20T16:34:13.070582Z","steps":["trace[1721764892] 'range keys from in-memory index tree'  (duration: 101.854438ms)"],"step_count":1}
{"level":"info","ts":"2025-05-20T16:34:13.283072Z","caller":"traceutil/trace.go:171","msg":"trace[932957628] transaction","detail":"{read_only:false; response_revision:2211; number_of_response:1; }","duration":"135.411849ms","start":"2025-05-20T16:34:13.147647Z","end":"2025-05-20T16:34:13.283059Z","steps":["trace[932957628] 'process raft request'  (duration: 135.072675ms)"],"step_count":1}


==> kernel <==
 15:38:42 up 26 min,  0 users,  load average: 5.85, 4.90, 3.22
Linux minikube 6.14.5-200.fc41.x86_64 #1 SMP PREEMPT_DYNAMIC Fri May  2 14:06:21 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [78f9be7622f1] <==
E0527 15:24:16.848588       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:24:16.848565410" prevR="5.73484173ss" incrR="184467440737.09542116ss" currentR="5.73474673ss"
E0527 15:24:16.849061       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:24:16.849040062" prevR="5.73609648ss" incrR="184467440737.09543196ss" currentR="5.73601228ss"
E0527 15:24:16.849351       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:24:16.849343250" prevR="5.73697142ss" incrR="184467440737.09546647ss" currentR="5.73692173ss"
E0527 15:24:26.888197       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:24:26.888178559" prevR="6.44292123ss" incrR="184467440737.09540848ss" currentR="6.44281355ss"
E0527 15:24:27.013479       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:24:27.013456522" prevR="6.53099181ss" incrR="184467440737.09549815ss" currentR="6.53097380ss"
E0527 15:24:41.946610       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:24:41.946596902" prevR="7.01344531ss" incrR="184467440737.09551546ss" currentR="7.01344461ss"
E0527 15:24:41.957372       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:24:41.957352928" prevR="7.02496164ss" incrR="184467440737.09550410ss" currentR="7.02494958ss"
E0527 15:24:41.962303       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:24:41.962288776" prevR="7.03080471ss" incrR="184467440737.09547036ss" currentR="7.03075891ss"
E0527 15:24:41.965648       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:24:41.965635252" prevR="7.03517283ss" incrR="184467440737.09550817ss" currentR="7.03516484ss"
E0527 15:24:46.854915       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:24:46.854899877" prevR="7.15923365ss" incrR="184467440737.09546983ss" currentR="7.15918732ss"
E0527 15:24:46.876768       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:24:46.876752333" prevR="7.19352953ss" incrR="184467440737.09549046ss" currentR="7.19350383ss"
E0527 15:24:51.889029       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:24:51.889010889" prevR="7.27798040ss" incrR="184467440737.09546802ss" currentR="7.27793226ss"
E0527 15:24:57.699050       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:24:57.699023327" prevR="7.51888316ss" incrR="184467440737.09547386ss" currentR="7.51884086ss"
E0527 15:25:01.979470       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:25:01.979448250" prevR="7.79427414ss" incrR="184467440737.09548942ss" currentR="7.79424740ss"
E0527 15:25:02.016371       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:25:02.016348711" prevR="7.80496199ss" incrR="184467440737.09546333ss" currentR="7.80490916ss"
E0527 15:25:06.966096       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:25:06.966082118" prevR="7.99020524ss" incrR="184467440737.09548957ss" currentR="7.99017865ss"
E0527 15:25:12.021547       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:25:12.021531534" prevR="8.13553590ss" incrR="184467440737.09546933ss" currentR="8.13548907ss"
E0527 15:25:16.892312       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:25:16.892293112" prevR="8.45440791ss" incrR="184467440737.09544368ss" currentR="8.45433543ss"
E0527 15:25:22.081705       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:25:22.081684022" prevR="8.63431987ss" incrR="184467440737.09545471ss" currentR="8.63425842ss"
E0527 15:25:32.094317       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:25:32.094302086" prevR="8.92014824ss" incrR="184467440737.09551193ss" currentR="8.92014401ss"
E0527 15:25:42.163239       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:25:42.163222282" prevR="9.51429101ss" incrR="184467440737.09551443ss" currentR="9.51428928ss"
E0527 15:25:56.864435       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:25:56.864419973" prevR="9.93502359ss" incrR="184467440737.09549018ss" currentR="9.93499761ss"
E0527 15:26:16.999328       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:26:16.999307983" prevR="12.62289905ss" incrR="184467440737.09547340ss" currentR="12.62285629ss"
E0527 15:26:26.934772       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:26:26.934754800" prevR="13.47703913ss" incrR="184467440737.09544648ss" currentR="13.47696945ss"
E0527 15:26:31.905252       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:26:31.905215675" prevR="13.63541335ss" incrR="184467440737.09550224ss" currentR="13.63539943ss"
E0527 15:26:31.934436       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:26:31.934417087" prevR="13.69091769ss" incrR="184467440737.09550411ss" currentR="13.69090564ss"
E0527 15:26:36.888579       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:26:36.888560280" prevR="13.80607870ss" incrR="184467440737.09547917ss" currentR="13.80604171ss"
E0527 15:26:36.947842       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:26:36.947818689" prevR="13.82750869ss" incrR="184467440737.09546321ss" currentR="13.82745574ss"
E0527 15:26:46.947425       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:26:46.947399211" prevR="14.01976368ss" incrR="184467440737.09547315ss" currentR="14.01972067ss"
E0527 15:26:47.020662       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:26:47.020643653" prevR="14.07223775ss" incrR="184467440737.09546375ss" currentR="14.07218534ss"
E0527 15:26:59.654045       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:26:59.654019767" prevR="14.86567689ss" incrR="184467440737.09551461ss" currentR="14.86567534ss"
E0527 15:26:59.657836       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:26:59.657819782" prevR="14.86754882ss" incrR="184467440737.09551026ss" currentR="14.86754292ss"
E0527 15:27:01.842362       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:27:01.842336727" prevR="14.95097725ss" incrR="184467440737.09549952ss" currentR="14.95096061ss"
E0527 15:27:01.844656       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:27:01.844641417" prevR="14.95475192ss" incrR="184467440737.09551428ss" currentR="14.95475004ss"
E0527 15:27:01.858779       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:27:01.858756751" prevR="14.98716541ss" incrR="184467440737.09548091ss" currentR="14.98713016ss"
E0527 15:27:06.857049       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:27:06.857030915" prevR="15.04630467ss" incrR="184467440737.09548240ss" currentR="15.04627091ss"
E0527 15:27:06.879727       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:27:06.879706410" prevR="15.09998750ss" incrR="184467440737.09545166ss" currentR="15.09992300ss"
E0527 15:27:11.913413       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:27:11.913393540" prevR="15.19973764ss" incrR="184467440737.09547795ss" currentR="15.19969943ss"
E0527 15:27:11.939414       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:27:11.939393267" prevR="15.24215842ss" incrR="184467440737.09546500ss" currentR="15.24210726ss"
E0527 15:27:11.956878       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:27:11.956853837" prevR="15.25807580ss" incrR="184467440737.09545511ss" currentR="15.25801475ss"
E0527 15:27:26.896947       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:27:26.896932676" prevR="15.82036781ss" incrR="184467440737.09547642ss" currentR="15.82032807ss"
E0527 15:27:36.870918       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:27:36.870899458" prevR="16.21122800ss" incrR="184467440737.09542240ss" currentR="16.21113424ss"
E0527 15:27:36.963695       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:27:36.963672445" prevR="16.25425435ss" incrR="184467440737.09551324ss" currentR="16.25425143ss"
E0527 15:27:42.026077       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:27:42.026062095" prevR="16.37642055ss" incrR="184467440737.09548039ss" currentR="16.37638478ss"
E0527 15:27:46.936186       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:27:46.936163471" prevR="16.44724940ss" incrR="184467440737.09548365ss" currentR="16.44721689ss"
E0527 15:28:22.029437       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:28:22.029419290" prevR="21.28248567ss" incrR="184467440737.09550978ss" currentR="21.28247929ss"
E0527 15:28:36.912157       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:28:36.912141318" prevR="21.83286861ss" incrR="184467440737.09547309ss" currentR="21.83282554ss"
E0527 15:28:41.923405       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:28:41.923383390" prevR="21.88006086ss" incrR="184467440737.09551100ss" currentR="21.88005570ss"
E0527 15:28:41.926806       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:28:41.926786229" prevR="21.88354022ss" incrR="184467440737.09545335ss" currentR="21.88347741ss"
E0527 15:28:42.027022       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:28:42.027006382" prevR="21.91968059ss" incrR="184467440737.09547656ss" currentR="21.91964099ss"
E0527 15:28:51.927523       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:28:51.927508145" prevR="22.24476619ss" incrR="184467440737.09546705ss" currentR="22.24471708ss"
E0527 15:29:02.075664       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:29:02.075646152" prevR="22.69533871ss" incrR="184467440737.09547627ss" currentR="22.69529882ss"
E0527 15:29:12.187428       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:29:12.187409155" prevR="24.24529803ss" incrR="184467440737.09551568ss" currentR="24.24529755ss"
E0527 15:29:31.921876       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:29:31.921851544" prevR="27.50145377ss" incrR="184467440737.09549099ss" currentR="27.50142860ss"
E0527 15:29:37.006996       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:29:37.006968096" prevR="27.83038998ss" incrR="184467440737.09550451ss" currentR="27.83037833ss"
E0527 15:30:12.000025       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:30:12.000000051" prevR="29.63148695ss" incrR="184467440737.09549247ss" currentR="29.63146326ss"
E0527 15:30:21.864122       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-low" when="2025-05-27 15:30:21.864103702" prevR="29.92159628ss" incrR="184467440737.09547874ss" currentR="29.92155886ss"
I0527 15:32:28.407711       1 alloc.go:330] "allocated clusterIPs" service="default/nginx-deploy" clusterIPs={"IPv4":"10.106.142.126"}
E0527 15:33:33.124107       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="system" when="2025-05-27 15:33:33.124079198" prevR="17.25157519ss" incrR="184467440737.09550078ss" currentR="17.25155981ss"
I0527 15:37:21.619938       1 alloc.go:330] "allocated clusterIPs" service="default/nginx-deploy" clusterIPs={"IPv4":"10.106.166.119"}


==> kube-apiserver [e5c99e887fbb] <==
E0520 16:22:42.512419       1 handler_proxy.go:143] error resolving kube-system/metrics-server: service "metrics-server" not found
I0520 16:22:43.355371       1 alloc.go:330] "allocated clusterIPs" service="kube-system/metrics-server" clusterIPs={"IPv4":"10.103.79.214"}
W0520 16:22:43.431226       1 handler_proxy.go:99] no RequestInfo found in the context
E0520 16:22:43.431263       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
W0520 16:22:43.480205       1 handler_proxy.go:99] no RequestInfo found in the context
E0520 16:22:43.483257       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
W0520 16:22:43.492760       1 handler_proxy.go:99] no RequestInfo found in the context
I0520 16:22:43.484732       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0520 16:22:43.495076       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
I0520 16:22:43.496570       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0520 16:22:43.656875       1 handler_proxy.go:99] no RequestInfo found in the context
E0520 16:22:43.663614       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
W0520 16:22:44.667210       1 handler_proxy.go:99] no RequestInfo found in the context
E0520 16:22:44.667316       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
W0520 16:22:44.667343       1 handler_proxy.go:99] no RequestInfo found in the context
E0520 16:22:44.667361       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I0520 16:22:44.668539       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0520 16:22:44.668567       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0520 16:23:44.668998       1 handler_proxy.go:99] no RequestInfo found in the context
E0520 16:23:44.669033       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
W0520 16:23:44.669055       1 handler_proxy.go:99] no RequestInfo found in the context
E0520 16:23:44.669075       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I0520 16:23:44.670417       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0520 16:23:44.670440       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0520 16:23:49.485484       1 handler_proxy.go:99] no RequestInfo found in the context
E0520 16:23:49.488307       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
W0520 16:23:50.491065       1 handler_proxy.go:99] no RequestInfo found in the context
E0520 16:23:50.491560       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
W0520 16:23:50.491224       1 handler_proxy.go:99] no RequestInfo found in the context
E0520 16:23:50.492802       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I0520 16:23:50.494232       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0520 16:23:50.494223       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0520 16:24:02.345830       1 remote_available_controller.go:448] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.103.79.214:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.103.79.214:443/apis/metrics.k8s.io/v1beta1\": dial tcp 10.103.79.214:443: connect: connection refused" logger="UnhandledError"
W0520 16:24:02.347846       1 handler_proxy.go:99] no RequestInfo found in the context
E0520 16:24:02.348937       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I0520 16:24:02.397879       1 handler.go:286] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager


==> kube-controller-manager [4333dd7570f7] <==
I0527 15:20:14.722942       1 shared_informer.go:320] Caches are synced for disruption
I0527 15:20:14.755837       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0527 15:20:14.875947       1 shared_informer.go:320] Caches are synced for taint
I0527 15:20:14.910034       1 node_lifecycle_controller.go:1234] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0527 15:20:14.915659       1 node_lifecycle_controller.go:886] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0527 15:20:14.915320       1 shared_informer.go:320] Caches are synced for daemon sets
I0527 15:20:14.930919       1 shared_informer.go:320] Caches are synced for resource quota
I0527 15:20:14.930727       1 node_lifecycle_controller.go:1080] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0527 15:20:14.942490       1 shared_informer.go:320] Caches are synced for resource quota
I0527 15:20:14.990450       1 shared_informer.go:313] Waiting for caches to sync for garbage collector
I0527 15:20:15.645162       1 shared_informer.go:320] Caches are synced for garbage collector
I0527 15:20:15.645531       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0527 15:20:15.645640       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0527 15:20:15.710532       1 shared_informer.go:320] Caches are synced for garbage collector
I0527 15:20:15.748943       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="1.017294385s"
I0527 15:20:15.749673       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="633.135¬µs"
I0527 15:20:15.756575       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-7fbb699795" duration="1.013886422s"
I0527 15:20:15.757421       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-7fbb699795" duration="756.315¬µs"
E0527 15:20:45.348044       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0527 15:20:45.795409       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
I0527 15:20:47.878807       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="47.689484ms"
I0527 15:20:47.879578       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="70.253¬µs"
I0527 15:20:49.243010       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="40.625449ms"
I0527 15:20:49.243286       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="45.026¬µs"
I0527 15:20:49.627707       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-7fbb699795" duration="79.633¬µs"
I0527 15:20:50.338612       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="35.62¬µs"
I0527 15:20:50.795345       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-7fbb699795" duration="106.792504ms"
I0527 15:20:50.798769       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-7fbb699795" duration="78.033¬µs"
I0527 15:21:02.969900       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="143.976522ms"
I0527 15:21:02.970428       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="66.921¬µs"
I0527 15:26:40.542733       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0527 15:31:41.546031       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0527 15:32:28.200981       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deploy-75cff64695" duration="339.367617ms"
I0527 15:32:28.230797       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deploy-75cff64695" duration="29.325663ms"
I0527 15:32:28.231369       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deploy-75cff64695" duration="58.465¬µs"
I0527 15:32:28.708633       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deploy-75cff64695" duration="63.29¬µs"
I0527 15:32:28.804003       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deploy-75cff64695" duration="30.834¬µs"
I0527 15:32:38.172994       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deploy-75cff64695" duration="144.454¬µs"
I0527 15:32:52.233752       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deploy-75cff64695" duration="42.713¬µs"
I0527 15:33:07.190373       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deploy-75cff64695" duration="42.022¬µs"
I0527 15:33:20.273132       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deploy-75cff64695" duration="73.243¬µs"
I0527 15:33:33.279794       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deploy-75cff64695" duration="30.664¬µs"
I0527 15:33:46.186002       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deploy-75cff64695" duration="338.234¬µs"
I0527 15:34:30.263407       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deploy-75cff64695" duration="57.676¬µs"
I0527 15:34:44.162052       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deploy-75cff64695" duration="58.432¬µs"
I0527 15:35:50.020226       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deploy-75cff64695" duration="34.162¬µs"
I0527 15:36:02.187457       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deploy-75cff64695" duration="48.467¬µs"
I0527 15:36:46.769592       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0527 15:37:19.115145       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deploy-75cff64695" duration="164.52¬µs"
I0527 15:37:21.265968       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deploy-75cff64695" duration="302.301489ms"
I0527 15:37:21.373472       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deploy-75cff64695" duration="107.096604ms"
I0527 15:37:21.373545       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deploy-75cff64695" duration="30.546¬µs"
I0527 15:37:22.115770       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deploy-75cff64695" duration="122.689¬µs"
I0527 15:37:27.719778       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deploy-75cff64695" duration="55.109¬µs"
I0527 15:37:29.683003       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0527 15:37:39.280532       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deploy-75cff64695" duration="41.489¬µs"
I0527 15:37:53.502590       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deploy-75cff64695" duration="43.287¬µs"
I0527 15:38:04.202141       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deploy-75cff64695" duration="48.246¬µs"
I0527 15:38:18.221625       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deploy-75cff64695" duration="180.705¬µs"
I0527 15:38:30.164740       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deploy-75cff64695" duration="51.782¬µs"


==> kube-controller-manager [ef9a8b4071ca] <==
I0527 15:19:31.811005       1 serving.go:386] Generated self-signed cert in-memory
I0527 15:19:33.091772       1 controllermanager.go:185] "Starting" version="v1.32.0"
I0527 15:19:33.093213       1 controllermanager.go:187] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0527 15:19:33.097530       1 secure_serving.go:213] Serving securely on 127.0.0.1:10257
I0527 15:19:33.098696       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0527 15:19:33.098810       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0527 15:19:33.098928       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
E0527 15:19:43.116105       1 controllermanager.go:230] "Error building controller context" err="failed to wait for apiserver being healthy: timed out waiting for the condition: failed to get apiserver /healthz status: an error on the server (\"[+]ping ok\\n[+]log ok\\n[+]etcd ok\\n[+]poststarthook/start-apiserver-admission-initializer ok\\n[+]poststarthook/generic-apiserver-start-informers ok\\n[+]poststarthook/priority-and-fairness-config-consumer ok\\n[+]poststarthook/priority-and-fairness-filter ok\\n[+]poststarthook/storage-object-count-tracker-hook ok\\n[+]poststarthook/start-apiextensions-informers ok\\n[+]poststarthook/start-apiextensions-controllers ok\\n[+]poststarthook/crd-informer-synced ok\\n[+]poststarthook/start-system-namespaces-controller ok\\n[+]poststarthook/start-cluster-authentication-info-controller ok\\n[+]poststarthook/start-kube-apiserver-identity-lease-controller ok\\n[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok\\n[+]poststarthook/start-legacy-token-tracking-controller ok\\n[+]poststarthook/start-service-ip-repair-controllers ok\\n[-]poststarthook/rbac/bootstrap-roles failed: reason withheld\\n[+]poststarthook/scheduling/bootstrap-system-priority-classes ok\\n[+]poststarthook/priority-and-fairness-config-producer ok\\n[+]poststarthook/bootstrap-controller ok\\n[+]poststarthook/aggregator-reload-proxy-client-cert ok\\n[+]poststarthook/start-kube-aggregator-informers ok\\n[+]poststarthook/apiservice-status-local-available-controller ok\\n[+]poststarthook/apiservice-status-remote-available-controller ok\\n[+]poststarthook/apiservice-registration-controller ok\\n[+]poststarthook/apiservice-discovery-controller ok\\n[+]poststarthook/kube-apiserver-autoregistration ok\\n[+]autoregister-completion ok\\n[+]poststarthook/apiservice-openapi-controller ok\\n[+]poststarthook/apiservice-openapiv3-controller ok\\nhealthz check failed\") has prevented the request from succeeding"


==> kube-proxy [1d0bec77a195] <==
I0527 15:20:02.866757       1 server_linux.go:66] "Using iptables proxy"
I0527 15:20:12.681332       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0527 15:20:12.683441       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0527 15:20:13.652038       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0527 15:20:13.652482       1 server_linux.go:170] "Using iptables Proxier"
I0527 15:20:13.730471       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0527 15:20:13.973520       1 server.go:497] "Version info" version="v1.32.0"
I0527 15:20:13.978055       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0527 15:20:14.072411       1 config.go:199] "Starting service config controller"
I0527 15:20:14.072571       1 shared_informer.go:313] Waiting for caches to sync for service config
I0527 15:20:14.072598       1 config.go:105] "Starting endpoint slice config controller"
I0527 15:20:14.072600       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0527 15:20:14.077410       1 config.go:329] "Starting node config controller"
I0527 15:20:14.078299       1 shared_informer.go:313] Waiting for caches to sync for node config
I0527 15:20:14.175186       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0527 15:20:14.175804       1 shared_informer.go:320] Caches are synced for service config
I0527 15:20:14.183069       1 shared_informer.go:320] Caches are synced for node config


==> kube-proxy [4ad63f452b38] <==
I0520 16:03:45.433061       1 server_linux.go:66] "Using iptables proxy"
I0520 16:03:49.754089       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0520 16:03:49.858646       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0520 16:03:50.345396       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0520 16:03:50.353882       1 server_linux.go:170] "Using iptables Proxier"
I0520 16:03:50.373466       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0520 16:03:50.375614       1 server.go:497] "Version info" version="v1.32.0"
I0520 16:03:50.376949       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0520 16:03:50.435169       1 config.go:199] "Starting service config controller"
I0520 16:03:50.436328       1 shared_informer.go:313] Waiting for caches to sync for service config
I0520 16:03:50.437018       1 config.go:329] "Starting node config controller"
I0520 16:03:50.437088       1 shared_informer.go:313] Waiting for caches to sync for node config
I0520 16:03:50.437094       1 config.go:105] "Starting endpoint slice config controller"
I0520 16:03:50.438488       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0520 16:03:50.537783       1 shared_informer.go:320] Caches are synced for node config
I0520 16:03:50.540043       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0520 16:03:50.540065       1 shared_informer.go:320] Caches are synced for service config


==> kube-scheduler [46ea0ea6d9b0] <==
I0520 16:03:43.641161       1 serving.go:386] Generated self-signed cert in-memory
W0520 16:03:49.070900       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0520 16:03:49.072885       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0520 16:03:49.073072       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0520 16:03:49.073079       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0520 16:03:49.311669       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0520 16:03:49.342056       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0520 16:03:49.349632       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0520 16:03:49.350056       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0520 16:03:49.350660       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0520 16:03:49.350130       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0520 16:03:49.681204       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [49253a43e31f] <==
I0527 15:19:36.833247       1 serving.go:386] Generated self-signed cert in-memory
W0527 15:19:41.546008       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0527 15:19:41.707995       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0527 15:19:41.708052       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0527 15:19:41.708063       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0527 15:19:41.940618       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0527 15:19:41.942520       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0527 15:19:41.964738       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0527 15:19:41.964788       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0527 15:19:41.965203       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0527 15:19:41.965270       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0527 15:19:42.576010       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
May 27 15:32:37 minikube kubelet[1285]: E0527 15:32:37.004189    1285 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-nginx\" with ErrImagePull: \"Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/nginx-deploy-75cff64695-btqq8" podUID="942c67ad-2287-4722-832d-083c03086e3c"
May 27 15:32:38 minikube kubelet[1285]: E0527 15:32:38.000533    1285 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-nginx\" with ImagePullBackOff: \"Back-off pulling image \\\"my-nginx\\\": ErrImagePull: Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/nginx-deploy-75cff64695-btqq8" podUID="942c67ad-2287-4722-832d-083c03086e3c"
May 27 15:32:53 minikube kubelet[1285]: E0527 15:32:53.755301    1285 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-nginx:latest"
May 27 15:32:53 minikube kubelet[1285]: E0527 15:32:53.755517    1285 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-nginx:latest"
May 27 15:32:53 minikube kubelet[1285]: E0527 15:32:53.755673    1285 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:my-nginx,Image:my-nginx,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dn4dc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nginx-deploy-75cff64695-btqq8_default(942c67ad-2287-4722-832d-083c03086e3c): ErrImagePull: Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 27 15:32:53 minikube kubelet[1285]: E0527 15:32:53.758015    1285 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-nginx\" with ErrImagePull: \"Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/nginx-deploy-75cff64695-btqq8" podUID="942c67ad-2287-4722-832d-083c03086e3c"
May 27 15:33:07 minikube kubelet[1285]: E0527 15:33:07.131888    1285 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-nginx\" with ImagePullBackOff: \"Back-off pulling image \\\"my-nginx\\\": ErrImagePull: Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/nginx-deploy-75cff64695-btqq8" podUID="942c67ad-2287-4722-832d-083c03086e3c"
May 27 15:33:22 minikube kubelet[1285]: E0527 15:33:22.129648    1285 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-nginx:latest"
May 27 15:33:22 minikube kubelet[1285]: E0527 15:33:22.129844    1285 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-nginx:latest"
May 27 15:33:22 minikube kubelet[1285]: E0527 15:33:22.129949    1285 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:my-nginx,Image:my-nginx,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dn4dc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nginx-deploy-75cff64695-btqq8_default(942c67ad-2287-4722-832d-083c03086e3c): ErrImagePull: Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 27 15:33:22 minikube kubelet[1285]: E0527 15:33:22.132138    1285 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-nginx\" with ErrImagePull: \"Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/nginx-deploy-75cff64695-btqq8" podUID="942c67ad-2287-4722-832d-083c03086e3c"
May 27 15:33:33 minikube kubelet[1285]: E0527 15:33:33.122958    1285 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-nginx\" with ImagePullBackOff: \"Back-off pulling image \\\"my-nginx\\\": ErrImagePull: Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/nginx-deploy-75cff64695-btqq8" podUID="942c67ad-2287-4722-832d-083c03086e3c"
May 27 15:33:46 minikube kubelet[1285]: E0527 15:33:46.122168    1285 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-nginx\" with ImagePullBackOff: \"Back-off pulling image \\\"my-nginx\\\": ErrImagePull: Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/nginx-deploy-75cff64695-btqq8" podUID="942c67ad-2287-4722-832d-083c03086e3c"
May 27 15:34:01 minikube kubelet[1285]: E0527 15:34:01.123007    1285 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-nginx\" with ImagePullBackOff: \"Back-off pulling image \\\"my-nginx\\\": ErrImagePull: Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/nginx-deploy-75cff64695-btqq8" podUID="942c67ad-2287-4722-832d-083c03086e3c"
May 27 15:34:15 minikube kubelet[1285]: E0527 15:34:15.877707    1285 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-nginx:latest"
May 27 15:34:15 minikube kubelet[1285]: E0527 15:34:15.877788    1285 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-nginx:latest"
May 27 15:34:15 minikube kubelet[1285]: E0527 15:34:15.878180    1285 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:my-nginx,Image:my-nginx,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dn4dc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nginx-deploy-75cff64695-btqq8_default(942c67ad-2287-4722-832d-083c03086e3c): ErrImagePull: Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 27 15:34:15 minikube kubelet[1285]: E0527 15:34:15.880544    1285 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-nginx\" with ErrImagePull: \"Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/nginx-deploy-75cff64695-btqq8" podUID="942c67ad-2287-4722-832d-083c03086e3c"
May 27 15:34:30 minikube kubelet[1285]: E0527 15:34:30.120906    1285 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-nginx\" with ImagePullBackOff: \"Back-off pulling image \\\"my-nginx\\\": ErrImagePull: Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/nginx-deploy-75cff64695-btqq8" podUID="942c67ad-2287-4722-832d-083c03086e3c"
May 27 15:34:44 minikube kubelet[1285]: E0527 15:34:44.120891    1285 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-nginx\" with ImagePullBackOff: \"Back-off pulling image \\\"my-nginx\\\": ErrImagePull: Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/nginx-deploy-75cff64695-btqq8" podUID="942c67ad-2287-4722-832d-083c03086e3c"
May 27 15:34:57 minikube kubelet[1285]: E0527 15:34:57.124893    1285 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-nginx\" with ImagePullBackOff: \"Back-off pulling image \\\"my-nginx\\\": ErrImagePull: Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/nginx-deploy-75cff64695-btqq8" podUID="942c67ad-2287-4722-832d-083c03086e3c"
May 27 15:35:09 minikube kubelet[1285]: E0527 15:35:09.122179    1285 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-nginx\" with ImagePullBackOff: \"Back-off pulling image \\\"my-nginx\\\": ErrImagePull: Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/nginx-deploy-75cff64695-btqq8" podUID="942c67ad-2287-4722-832d-083c03086e3c"
May 27 15:35:21 minikube kubelet[1285]: E0527 15:35:21.121621    1285 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-nginx\" with ImagePullBackOff: \"Back-off pulling image \\\"my-nginx\\\": ErrImagePull: Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/nginx-deploy-75cff64695-btqq8" podUID="942c67ad-2287-4722-832d-083c03086e3c"
May 27 15:35:38 minikube kubelet[1285]: E0527 15:35:38.007451    1285 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-nginx:latest"
May 27 15:35:38 minikube kubelet[1285]: E0527 15:35:38.008229    1285 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-nginx:latest"
May 27 15:35:38 minikube kubelet[1285]: E0527 15:35:38.008586    1285 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:my-nginx,Image:my-nginx,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dn4dc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nginx-deploy-75cff64695-btqq8_default(942c67ad-2287-4722-832d-083c03086e3c): ErrImagePull: Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 27 15:35:38 minikube kubelet[1285]: E0527 15:35:38.013185    1285 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-nginx\" with ErrImagePull: \"Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/nginx-deploy-75cff64695-btqq8" podUID="942c67ad-2287-4722-832d-083c03086e3c"
May 27 15:35:49 minikube kubelet[1285]: E0527 15:35:49.584533    1285 kubelet.go:2579] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.173s"
May 27 15:35:49 minikube kubelet[1285]: E0527 15:35:49.626722    1285 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-nginx\" with ImagePullBackOff: \"Back-off pulling image \\\"my-nginx\\\": ErrImagePull: Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/nginx-deploy-75cff64695-btqq8" podUID="942c67ad-2287-4722-832d-083c03086e3c"
May 27 15:36:02 minikube kubelet[1285]: E0527 15:36:02.121121    1285 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-nginx\" with ImagePullBackOff: \"Back-off pulling image \\\"my-nginx\\\": ErrImagePull: Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/nginx-deploy-75cff64695-btqq8" podUID="942c67ad-2287-4722-832d-083c03086e3c"
May 27 15:36:14 minikube kubelet[1285]: E0527 15:36:14.124452    1285 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-nginx\" with ImagePullBackOff: \"Back-off pulling image \\\"my-nginx\\\": ErrImagePull: Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/nginx-deploy-75cff64695-btqq8" podUID="942c67ad-2287-4722-832d-083c03086e3c"
May 27 15:36:28 minikube kubelet[1285]: E0527 15:36:28.121203    1285 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-nginx\" with ImagePullBackOff: \"Back-off pulling image \\\"my-nginx\\\": ErrImagePull: Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/nginx-deploy-75cff64695-btqq8" podUID="942c67ad-2287-4722-832d-083c03086e3c"
May 27 15:36:31 minikube kubelet[1285]: E0527 15:36:31.144286    1285 kubelet.go:2579] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.945s"
May 27 15:36:32 minikube kubelet[1285]: E0527 15:36:32.859008    1285 kubelet.go:2579] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.637s"
May 27 15:36:40 minikube kubelet[1285]: E0527 15:36:40.121206    1285 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-nginx\" with ImagePullBackOff: \"Back-off pulling image \\\"my-nginx\\\": ErrImagePull: Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/nginx-deploy-75cff64695-btqq8" podUID="942c67ad-2287-4722-832d-083c03086e3c"
May 27 15:36:52 minikube kubelet[1285]: E0527 15:36:52.360685    1285 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-nginx\" with ImagePullBackOff: \"Back-off pulling image \\\"my-nginx\\\": ErrImagePull: Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/nginx-deploy-75cff64695-btqq8" podUID="942c67ad-2287-4722-832d-083c03086e3c"
May 27 15:36:54 minikube kubelet[1285]: E0527 15:36:54.847311    1285 kubelet.go:2579] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.303s"
May 27 15:37:06 minikube kubelet[1285]: E0527 15:37:06.404165    1285 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-nginx\" with ImagePullBackOff: \"Back-off pulling image \\\"my-nginx\\\": ErrImagePull: Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/nginx-deploy-75cff64695-btqq8" podUID="942c67ad-2287-4722-832d-083c03086e3c"
May 27 15:37:21 minikube kubelet[1285]: I0527 15:37:21.603734    1285 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-p4wpv\" (UniqueName: \"kubernetes.io/projected/1cb10e21-7af5-474d-86b2-b0972b134852-kube-api-access-p4wpv\") pod \"nginx-deploy-75cff64695-gvdpt\" (UID: \"1cb10e21-7af5-474d-86b2-b0972b134852\") " pod="default/nginx-deploy-75cff64695-gvdpt"
May 27 15:37:23 minikube kubelet[1285]: I0527 15:37:23.489093    1285 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-dn4dc\" (UniqueName: \"kubernetes.io/projected/942c67ad-2287-4722-832d-083c03086e3c-kube-api-access-dn4dc\") pod \"942c67ad-2287-4722-832d-083c03086e3c\" (UID: \"942c67ad-2287-4722-832d-083c03086e3c\") "
May 27 15:37:23 minikube kubelet[1285]: I0527 15:37:23.670746    1285 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/942c67ad-2287-4722-832d-083c03086e3c-kube-api-access-dn4dc" (OuterVolumeSpecName: "kube-api-access-dn4dc") pod "942c67ad-2287-4722-832d-083c03086e3c" (UID: "942c67ad-2287-4722-832d-083c03086e3c"). InnerVolumeSpecName "kube-api-access-dn4dc". PluginName "kubernetes.io/projected", VolumeGIDValue ""
May 27 15:37:23 minikube kubelet[1285]: I0527 15:37:23.698412    1285 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-dn4dc\" (UniqueName: \"kubernetes.io/projected/942c67ad-2287-4722-832d-083c03086e3c-kube-api-access-dn4dc\") on node \"minikube\" DevicePath \"\""
May 27 15:37:25 minikube kubelet[1285]: I0527 15:37:25.150491    1285 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="942c67ad-2287-4722-832d-083c03086e3c" path="/var/lib/kubelet/pods/942c67ad-2287-4722-832d-083c03086e3c/volumes"
May 27 15:37:25 minikube kubelet[1285]: I0527 15:37:25.607275    1285 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="9d236fcbfe4204cca514263f4558f0f9073519d9226a86bc4d6120b1bdc74724"
May 27 15:37:27 minikube kubelet[1285]: E0527 15:37:27.373628    1285 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-nginx:latest"
May 27 15:37:27 minikube kubelet[1285]: E0527 15:37:27.374279    1285 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-nginx:latest"
May 27 15:37:27 minikube kubelet[1285]: E0527 15:37:27.374530    1285 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:my-nginx,Image:my-nginx,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p4wpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nginx-deploy-75cff64695-gvdpt_default(1cb10e21-7af5-474d-86b2-b0972b134852): ErrImagePull: Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 27 15:37:27 minikube kubelet[1285]: E0527 15:37:27.378725    1285 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-nginx\" with ErrImagePull: \"Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/nginx-deploy-75cff64695-gvdpt" podUID="1cb10e21-7af5-474d-86b2-b0972b134852"
May 27 15:37:27 minikube kubelet[1285]: E0527 15:37:27.672724    1285 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-nginx\" with ImagePullBackOff: \"Back-off pulling image \\\"my-nginx\\\": ErrImagePull: Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/nginx-deploy-75cff64695-gvdpt" podUID="1cb10e21-7af5-474d-86b2-b0972b134852"
May 27 15:37:40 minikube kubelet[1285]: E0527 15:37:40.741264    1285 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-nginx:latest"
May 27 15:37:40 minikube kubelet[1285]: E0527 15:37:40.741474    1285 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-nginx:latest"
May 27 15:37:40 minikube kubelet[1285]: E0527 15:37:40.741779    1285 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:my-nginx,Image:my-nginx,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p4wpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nginx-deploy-75cff64695-gvdpt_default(1cb10e21-7af5-474d-86b2-b0972b134852): ErrImagePull: Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 27 15:37:40 minikube kubelet[1285]: E0527 15:37:40.743602    1285 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-nginx\" with ErrImagePull: \"Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/nginx-deploy-75cff64695-gvdpt" podUID="1cb10e21-7af5-474d-86b2-b0972b134852"
May 27 15:37:53 minikube kubelet[1285]: E0527 15:37:53.346047    1285 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-nginx\" with ImagePullBackOff: \"Back-off pulling image \\\"my-nginx\\\": ErrImagePull: Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/nginx-deploy-75cff64695-gvdpt" podUID="1cb10e21-7af5-474d-86b2-b0972b134852"
May 27 15:38:05 minikube kubelet[1285]: E0527 15:38:05.694807    1285 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-nginx:latest"
May 27 15:38:05 minikube kubelet[1285]: E0527 15:38:05.694939    1285 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-nginx:latest"
May 27 15:38:05 minikube kubelet[1285]: E0527 15:38:05.695298    1285 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:my-nginx,Image:my-nginx,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p4wpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nginx-deploy-75cff64695-gvdpt_default(1cb10e21-7af5-474d-86b2-b0972b134852): ErrImagePull: Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 27 15:38:05 minikube kubelet[1285]: E0527 15:38:05.697310    1285 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-nginx\" with ErrImagePull: \"Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/nginx-deploy-75cff64695-gvdpt" podUID="1cb10e21-7af5-474d-86b2-b0972b134852"
May 27 15:38:18 minikube kubelet[1285]: E0527 15:38:18.122494    1285 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-nginx\" with ImagePullBackOff: \"Back-off pulling image \\\"my-nginx\\\": ErrImagePull: Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/nginx-deploy-75cff64695-gvdpt" podUID="1cb10e21-7af5-474d-86b2-b0972b134852"
May 27 15:38:30 minikube kubelet[1285]: E0527 15:38:30.123448    1285 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-nginx\" with ImagePullBackOff: \"Back-off pulling image \\\"my-nginx\\\": ErrImagePull: Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/nginx-deploy-75cff64695-gvdpt" podUID="1cb10e21-7af5-474d-86b2-b0972b134852"


==> kubernetes-dashboard [938ceae4fe1b] <==
2025/05/27 15:20:13 Starting overwatch
panic: Get "https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-csrf": dial tcp 10.96.0.1:443: i/o timeout

goroutine 1 [running]:
github.com/kubernetes/dashboard/src/app/backend/client/csrf.(*csrfTokenManager).init(0xc00061fae8)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:41 +0x30e
github.com/kubernetes/dashboard/src/app/backend/client/csrf.NewCsrfTokenManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:66
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).initCSRFKey(0xc000560f80)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:527 +0x94
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).init(0x19aba3a?)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:495 +0x32
github.com/kubernetes/dashboard/src/app/backend/client.NewClientManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:594
main.main()
	/home/runner/work/dashboard/dashboard/src/app/backend/dashboard.go:96 +0x1cf
2025/05/27 15:20:13 Using namespace: kubernetes-dashboard
2025/05/27 15:20:13 Using in-cluster config to connect to apiserver
2025/05/27 15:20:13 Using secret token for csrf signing
2025/05/27 15:20:13 Initializing csrf token from kubernetes-dashboard-csrf secret


==> kubernetes-dashboard [ae478048c46e] <==
2025/05/27 15:30:21 Getting list of all pet sets in the cluster
2025/05/27 15:30:22 [2025-05-27T15:30:22Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/27 15:30:22 [2025-05-27T15:30:22Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/27 15:30:22 [2025-05-27T15:30:22Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/27 15:30:22 [2025-05-27T15:30:22Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/27 15:30:22 [2025-05-27T15:30:22Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/27 15:30:26 [2025-05-27T15:30:26Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2025/05/27 15:30:26 Getting list of namespaces
2025/05/27 15:30:26 [2025-05-27T15:30:26Z] Incoming HTTP/1.1 GET /api/v1/daemonset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/27 15:30:26 [2025-05-27T15:30:26Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/27 15:30:26 Getting list of all deployments in the cluster
2025/05/27 15:30:26 [2025-05-27T15:30:26Z] Incoming HTTP/1.1 GET /api/v1/pod/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/27 15:30:26 Getting list of all pods in the cluster
2025/05/27 15:30:26 [2025-05-27T15:30:26Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/27 15:30:26 [2025-05-27T15:30:26Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/27 15:30:26 [2025-05-27T15:30:26Z] Incoming HTTP/1.1 GET /api/v1/cronjob/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/27 15:30:26 Getting list of all cron jobs in the cluster
2025/05/27 15:30:26 [2025-05-27T15:30:26Z] Incoming HTTP/1.1 GET /api/v1/job/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/27 15:30:26 Getting list of all jobs in the cluster
2025/05/27 15:30:26 [2025-05-27T15:30:26Z] Incoming HTTP/1.1 GET /api/v1/replicaset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/27 15:30:27 Getting list of all replica sets in the cluster
2025/05/27 15:30:27 [2025-05-27T15:30:27Z] Incoming HTTP/1.1 GET /api/v1/replicationcontroller/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/27 15:30:27 Getting list of all replication controllers in the cluster
2025/05/27 15:30:27 [2025-05-27T15:30:27Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/27 15:30:27 [2025-05-27T15:30:27Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/27 15:30:27 [2025-05-27T15:30:27Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/27 15:30:27 [2025-05-27T15:30:27Z] Incoming HTTP/1.1 GET /api/v1/statefulset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/27 15:30:27 Getting list of all pet sets in the cluster
2025/05/27 15:30:27 [2025-05-27T15:30:27Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/27 15:30:27 Getting pod metrics
2025/05/27 15:30:27 [2025-05-27T15:30:27Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/27 15:30:27 [2025-05-27T15:30:27Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/27 15:30:27 [2025-05-27T15:30:27Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/27 15:30:31 [2025-05-27T15:30:31Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2025/05/27 15:30:31 Getting list of namespaces
2025/05/27 15:30:31 [2025-05-27T15:30:31Z] Incoming HTTP/1.1 GET /api/v1/daemonset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/27 15:30:31 [2025-05-27T15:30:31Z] Incoming HTTP/1.1 GET /api/v1/cronjob/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/27 15:30:31 Getting list of all cron jobs in the cluster
2025/05/27 15:30:31 [2025-05-27T15:30:31Z] Incoming HTTP/1.1 GET /api/v1/job/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/27 15:30:31 Getting list of all jobs in the cluster
2025/05/27 15:30:31 [2025-05-27T15:30:31Z] Incoming HTTP/1.1 GET /api/v1/pod/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/27 15:30:31 Getting list of all pods in the cluster
2025/05/27 15:30:31 [2025-05-27T15:30:31Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/27 15:30:31 [2025-05-27T15:30:31Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/27 15:30:32 Getting list of all deployments in the cluster
2025/05/27 15:30:32 [2025-05-27T15:30:32Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/27 15:30:32 [2025-05-27T15:30:32Z] Incoming HTTP/1.1 GET /api/v1/replicaset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/27 15:30:32 Getting list of all replica sets in the cluster
2025/05/27 15:30:32 [2025-05-27T15:30:32Z] Incoming HTTP/1.1 GET /api/v1/replicationcontroller/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/27 15:30:32 Getting list of all replication controllers in the cluster
2025/05/27 15:30:32 [2025-05-27T15:30:32Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/27 15:30:32 [2025-05-27T15:30:32Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/27 15:30:32 [2025-05-27T15:30:32Z] Incoming HTTP/1.1 GET /api/v1/statefulset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/27 15:30:32 Getting list of all pet sets in the cluster
2025/05/27 15:30:32 [2025-05-27T15:30:32Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/27 15:30:32 [2025-05-27T15:30:32Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/27 15:30:32 [2025-05-27T15:30:32Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/27 15:30:32 Getting pod metrics
2025/05/27 15:30:32 [2025-05-27T15:30:32Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/27 15:30:32 [2025-05-27T15:30:32Z] Outcoming response to 127.0.0.1 with 200 status code


==> storage-provisioner [2d70f2ca015b] <==
I0527 15:19:59.553388       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0527 15:20:29.629087       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout


==> storage-provisioner [e41059225be2] <==
I0527 15:20:47.659138       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0527 15:20:47.761247       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0527 15:20:47.761557       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0527 15:21:03.534529       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0527 15:21:03.535690       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_dbf3bf44-9fe9-4e92-8b96-55bd3469b03a!
I0527 15:21:03.534895       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"a332c7b5-6b25-4cdf-8c81-9955b6f78b56", APIVersion:"v1", ResourceVersion:"2478", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_dbf3bf44-9fe9-4e92-8b96-55bd3469b03a became leader
I0527 15:21:04.324435       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_dbf3bf44-9fe9-4e92-8b96-55bd3469b03a!

