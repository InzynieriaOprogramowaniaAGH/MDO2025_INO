# Automatyzacja i zdalne wykonywanie poleceÅ„ za pomocÄ… Ansible (^o^)/ 

Do przeprowadzenia laboratoriÃ³w utworzono drugÄ… maszyne wirtualnÄ… ansible-target (na systemie Fedora) oraz przygotowano jÄ… do pracy aby umoÅ¼liwiÄ‡ komunikacjÄ™ pomiÄ™dzy maszynami wirtualnymi. Nadano obu maszynom hostame (ansible-target oraz ansible-controller). NastÄ™pnie wygenerowano nowy klucz SSH, aby umoÅ¼liwiÄ‡ bezhasÅ‚owe logowanie do maszyny ansible-target.

### Wygenerowanie nowego klucza SSH na maszynie gÅ‚Ã³wnej (ansible-controller)

```
ssh-keygen -t rsa -b 4096 -f ~/.ssh/nowy_klucz_ansible
```

### Skopiowanie klucza publicznego na maszynÄ™ docelowÄ… (ansible-target)

```
ssh-copy-id -i ~/.ssh/nowy_klucz_ansible.pub ansible@ansible-target
```

### Instalacja Ansible na ansible-controller

```
sudo apt update
sudo apt install ansible -y
```

NastÄ™pnie wprowadzono nazwy DNS w pliku /etc/hosts na obu maszynach wirtualnych aby uÅ‚atwiÄ‡ komunikacjÄ™ i aby moÅ¼liwe byÅ‚o uÅ¼ywanie nazw hostÃ³w zamiast adresÃ³w IP. Wystatczy w pliku /etc/hosts dodaÄ‡ nastÄ™pujÄ…cÄ… linijke :

```
10.0.2.4   ansible-target
```

### Utworzenie pliku inventory z definicjÄ… hosta

Utworzono plik inventory aby zdefiniowaÄ‡ target dla playbookÃ³w ansible.
TreÅ›Ä‡ pliku wyglÄ…da nastÄ™pujÄ…co:

```
[targets]
ansible-target ansible_host=10.0.2.4 ansible_user=ansible ansible_ssh_private_key_file=~/.ssh/nowy_klucz_ansible
```
### Utworzenie pliku ping.yaml czyli prostego playbooka

Utworzono prosty plik aby przetestowaÄ‡ czy host docelowy jest osiÄ…galny przez ansible

```
- name: Ping test on ansible-target
  hosts: targets
  gather_facts: no
  tasks:
    - name: Ping the machine
      ansible.builtin.ping:
```
A nastÄ™pnie uruchomiono playbook jak i sprawdzono dziaÅ‚anie.

```
ansible-playbook -i inventory ping.yaml
```

![alt text](screeny/ansible-ping.png)

### Skopiowanie pliku inventory na hosta docelowego 

Utworzono nowy playbook kopiujÄ…cy plik inventory na hosta docelowego 

```
- name: Copy inventory file to remote machine
  hosts: targets
  tasks:
    - name: Copy inventory file to /tmp on ansible-target
      ansible.builtin.copy:
        src: inventory
        dest: /tmp/inventory_copy
```

Po uruchomieniu playbooka komendÄ…:

```
ansible-playbook -i inventory copy_inventory.yml
```

Opercja zostaje wykonana pomyÅ›lnie a plik jest widoczony w katalogu docelowym na hoÅ›cie - powtÃ³rne uruchominie playbooka ping rÃ³wnieÅ¼ przechodzi pomyÅ›lnie.

![alt text](screeny/sc2.png)

NastÄ™pnym krokiem byÅ‚o utworznie playbooka aktualizujÄ…cego pakiety.  uÅ¼ytkownik ansible potrzebuje hasÅ‚a, Å¼eby uÅ¼yÄ‡ sudo, a Ansible nie moÅ¼e go podaÄ‡. Aby rozwiÄ…zaÄ‡ ten problem moÅ¼emy zezwoliÄ‡ uÅ¼ytkownikowi ansible na uÅ¼ycie sudo bez hasÅ‚a, logujemy siÄ™ na ansible-target: 

```
ssh ansible@ansible-target
```

Uruchamiamy plik visudo:

```
sudo visudo
```

oraz na koÅ„cu pliku dodajemy:

```
ansible ALL=(ALL) NOPASSWD: ALL
```

Oznacza to, Å¼e uÅ¼ytkownik ansible moÅ¼e uÅ¼ywaÄ‡ sudo bez hasÅ‚a â€“ dokÅ‚adnie tego potrzebuje Ansible.

Po aktualizacji playbook moÅ¼e zdalnie aktualizowaÄ‡ pakiety.

```
ansible-playbook -i inventory update.yml
```

![alt text](screeny/sc3.png)

NastÄ™pnie tworzymy playbook restartujÄ…cy usÅ‚ugi sshd oraz rngd. TreÅ›Ä‡ playbooka:

```
- name: Restart services on ansible-target
  hosts: targets
  become: true
  tasks:
    - name: Restart sshd
      ansible.builtin.service:
        name: sshd
        state: restarted

    - name: Restart rngd
      ansible.builtin.service:
        name: rngd
        state: restarted
        enabled: yes
```

![alt text](screeny/sc4.png)

### Przeprowadzenie operacji wzglÄ™dem maszyny z wyÅ‚Ä…czonym serwerem SSH

Sprawdzamy dziaÅ‚anie ansible, kiedy usÅ‚uga sshd jest nieaktywna na hoÅ›coe docelowym, w tym celu wyÅ‚Ä…czamy usÅ‚ugÄ™ sshd na ansible-target oraz przy pomocy playbooka prÃ³bujemy wykonaÄ‡ polecenie ping.

Otrzymujemy komunikat, Å¼e host jest nieosiÄ…galny wiÄ™c wszystko dziaÅ‚a jak powinno.

![alt text](screeny/sc5.png)

### ZarzÄ…dzanie stworzonym artefaktem przez ansible 

Utworzono nowe playbooki odpowiadajÄ…ce za:
 - Instalacja Dockera za pomocÄ… Ansible
 - Uruchomienie kontenera z obrazu z Docker Hub
 - Zweryfikowanie Å‚Ä…cznoÅ›ci z kontenerem
 - Zatrzymanie i usuniÄ™cie kontenera

ZawartoÅ›Ä‡ plikÃ³w prezentuje sie nastÄ™pujÄ…co :

```
- name: Install Docker on Fedora-based system
  hosts: targets
  become: true
  tasks:
    - name: Install required packages
      ansible.builtin.dnf:
        name:
          - dnf-plugins-core
          - device-mapper
          - device-mapper-persistent-data
          - lvm2
        state: present

    - name: Add Docker repo
      ansible.builtin.get_url:
        url: https://download.docker.com/linux/fedora/docker-ce.repo
        dest: /etc/yum.repos.d/docker-ce.repo

    - name: Install Docker
      ansible.builtin.dnf:
        name:
          - docker-ce
          - docker-ce-cli
          - containerd.io
        state: latest

    - name: Enable and start Docker
      ansible.builtin.service:
        name: docker
        state: started
        enabled: true

```

```
- name: Pull and run Docker container
  hosts: targets
  become: true
  tasks:
    - name: Pull Docker image
      community.docker.docker_image:
        name: bambusscooby/irssi-runtime
        tag: "11"
        source: pull

    - name: Run container
      community.docker.docker_container:
        name: irssi-container
        image: bambusscooby/irssi-runtime:11
        state: started
        detach: true

```
```
- name: Verify Docker container is running
  hosts: targets
  become: true
  tasks:
    - name: Get container info
      community.docker.docker_container_info:
        name: irssi-container
      register: container_info

    - name: Debug container state
      ansible.builtin.debug:
        var: container_info.container.State

```


```
- name: Stop and remove Docker container
  hosts: targets
  become: true
  tasks:
    - name: Stop container
      community.docker.docker_container:
        name: irssi-container
        state: stopped

    - name: Remove container
      community.docker.docker_container:
        name: irssi-container
        state: absent

```

Utworzono takÅ¼e plik Makefile ktÃ³ry automatyzuje proces, przy pomocy komendy 'make full' uruchamia playbooki w kolejnosci:
Install_docker -> run_docker -> verify_container

Natomiast przy uÅ¼yciu 'make clean' uruchamia playbook usuwajÄ…cy kontener.


![alt text](screeny/make1.png)

![alt text](screeny/make2.png)

![alt text](screeny/make3.png)

![alt text](screeny/make4.png)

(ã¥ï½¡â—•â€¿â€¿â—•ï½¡)ã¥pozdrawiam(ã¥ï½¡â—•â€¿â€¿â—•ï½¡)ã¥

# Pliki odpowiedzi dla wdroÅ¼eÅ„ nienadzorowanych
(â•¯Â°â–¡Â°ï¼‰â•¯ï¸µ â”»â”â”»
## Cel zadania
UtworzyÄ‡ ÅºrÃ³dÅ‚o instalacji nienadzorowanej dla systemu operacyjnego hostujÄ…cego nasze oprogramowanie
PrzeprowadziÄ‡ instalacjÄ™ systemu, ktÃ³ry po uruchomieniu rozpocznie hostowanie naszego programu

W pierwszym kroku zainstalowano system Fedora Server, a nastÄ™pnie po 'czystej' instalacji pobrano plik odpowiedzi z lokalizacji '/root/anaconda-ks.cfg`, ktÃ³ry nastÄ™pnie zmodyfikowano o :

### WzmiankÄ™ o repo oraz skÄ…d je pobraÄ‡:
```
url --mirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=fedora-38&arch=x86_64

repo --name=update --mirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=updates-released-f38&arch=x86_64
```

### Zapewnienie Å¼e plik odpowiedzi formatuje dysk 

```
clearpart --all
```

### Ustawienie hostaname na inny niÅ¼ domyÅ›lny

CaÅ‚oÅ›ciowy plik odpowiedzi po modyfikacji wyglÄ…da nastÄ™pujÄ…co:

```
# Keyboard layouts
keyboard --vckeymap=pl --xlayouts='pl'

# System language
lang pl_PL.UTF-8

# Instalation source (Fedora 38 mirrors)
url --mirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=fedora-38&arch=x86_64
# Updates repository
repo --name=updates --mirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=updates-released-f38&arch=x86_64

# Network information
network --bootproto=dhcp --ipv6=auto --activate --hostname=harambe.localdomain


# Partitioning
ignoredisk --only-use=sda
clearpart --all --initlabel        
autopart                           

# System timezone
timezone Europe/Warsaw --utc

# Run the Setup Agent on first boot
firstboot --enable

# Root password (zaszyfrowany)
rootpw --iscrypted --allow-ssh $y$j9T$k3bQKxtopLwWyUKwD0BDo46H$QhGDBD...
# UÅ¼ytkownik zwykÅ‚y
user --groups=wheel --name=user --gecos="user"

%packages
@^server-product-environment
%end

```

NastÄ™pnie na podstawie utworzonego pliku odpowiedzi utworzono pomocniczy obraz .iso za pomocÄ… komedy:
```
genisoimage -output ks.iso -volid KS -joliet -rock ks.cfg
```

Oraz w VirtualBoxie dodano ten pomocniczy plik .iso

![alt text](screeny/dodanie2Iso.png)

NastÄ™pnie uruchomiono maszynÄ™ wirtualnÄ…, w menu instalacyjnym naleÅ¼y wybraÄ‡ â€Install Fedora Server 38â€¦â€ i nacisnÄ…Ä‡ klawisz "e" to przeniesie nas do edycji wpisu GRUB-a. NaleÅ¼y zmodyfikowaÄ‡ ten wpis - w linijce zaczynajÄ…cej siÄ™  od linux albo linuxefi na koÅ„cu lini naleÅ¼y dodaÄ‡:

```
inst.ks=hd:sr1:/ks.cfg
```
lub podobny wpis zaleÅ¼nie od tego jak zamotowaliÅ›my nasze pomocnicze .iso.

![alt text](screeny/install1.png)

Po wykonaniu tych krokÃ³w i rozpoczÄ™ciu instlacji przez Ctrl + X, instalacja przebiega automatycznie bez koniecznoÅ›ci dalszej ingerencji ze strony administratora - bardzo przyjemne i wygodne swojÄ… drogÄ….

### Dodanie do pliku odpowiedzi repozytoria i oprogramowanie potrzebne do uruchomienia programu, zbudowanego w ramach projektu - naszego pipeline'u.

(à²¥ï¹à²¥)(à²¥ï¹à²¥)
Rozbudowujemy plik odpowiedzi, aby mieÄ‡ w peÅ‚ni nienadzorowanÄ… instalacjÄ™ Fedory z Dockerem i automatycznym startem utworzonego w ramach pipeline'u obrazu. Dokonujemy modyfikacji pliku odpowiedzi, plik wyglÄ…da nastÄ™pujÄ…co :

```
# ---------------------------------------------------------------------------
# Fedora 38 Server â€“ Kickstart z Docker CE i autostartem kontenera
# ---------------------------------------------------------------------------
keyboard --vckeymap=pl --xlayouts='pl'
lang pl_PL.UTF-8


url  --mirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=fedora-38&arch=x86_64
repo --name=updates --mirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=updates-released-f38&arch=x86_64


repo --name=docker-ce --baseurl=https://download.docker.com/linux/fedora/38/x86_64/stable

network --bootproto=dhcp --ipv6=auto --activate --hostname=harambe.localdomain


ignoredisk --only-use=sda
clearpart --all --initlabel
autopart

timezone Europe/Warsaw --utc


rootpw  --iscrypted --allow-ssh $y$j9T$k3bQKxtopLwWyUKwD0BDo46H$QhGDBDB0PK1l...
user    --groups=wheel --name=user --gecos="user"

# 1. Pakiety (Dockera + zaleÅ¼noÅ›ci dorzucamy tutaj)
%packages
@^server-product-environment
dnf-plugins-core
device-mapper
device-mapper-persistent-data
lvm2
docker-ce
docker-ce-cli
containerd.io
%end

# 2. Konfiguracje po zainstalowaniu plikÃ³w 
%post --log=/root/ks-post.log --interpreter=/usr/bin/bash --erroronfail

cat >/etc/yum.repos.d/docker-ce.repo <<'EOF'
[docker-ce-stable]
name=Docker CE Stable - $basearch
baseurl=https://download.docker.com/linux/fedora/$releasever/$basearch/stable
enabled=1
gpgcheck=1
gpgkey=https://download.docker.com/linux/fedora/gpg
EOF

# ---  Instalacja / aktualizacja Dockera ---------------
dnf -y install docker-ce docker-ce-cli containerd.io

# ---  WÅ‚Ä…czenie usÅ‚ugi ----------------------
systemctl enable --now docker.service

# ---  Skrypt first-boot --------------------------------
cat >/usr/local/bin/setup-container.sh <<'EOSH'
#!/usr/bin/env bash
set -euo pipefail

/usr/bin/docker pull bambusscooby/irssi-runtime:11
/usr/bin/docker run -d --name irssi-container --restart=unless-stopped bambusscooby/irssi-runtime:11

systemctl disable firstboot-container.service
EOSH
chmod +x /usr/local/bin/setup-container.sh
restorecon -v /usr/local/bin/setup-container.sh  # SELinux

# ---  Jednostka systemd one-shot -------------------
cat >/etc/systemd/system/firstboot-container.service <<'EOUNIT'
[Unit]
Requires=docker.service
After=docker.service network-online.target
Wants=network-online.target

[Service]
Type=oneshot
ExecStart=/usr/local/bin/setup-container.sh
RemainAfterExit=yes

[Install]
WantedBy=multi-user.target
EOUNIT

systemctl enable firstboot-container.service

%end

reboot
```

W drugiej wersji pliku kickstartowego w pierwszej kolejnoÅ›ci ustawiamy klawiature, jÄ™zyk, czas, sieÄ‡, partycje i tworzymy uÅ¼ytkownikÃ³w. NastÄ™pnie instalujemy paczki na podstawie podanego wczeÅ›niej ÅºrÃ³dla pakietÃ³w bazowach (url, repo) oraz z repozytorium Dockera - konieczne do uruchomienia aplikacja z pipeline'u. 

Sekcja %post zawiera polecenia powÅ‚oki (bash), ktÃ³re zostanÄ… wykonane zaraz po zakoÅ„czeniu instalacji pakietÃ³w, ale jeszcze przed pierwszym uruchomieniem systemu.

MoÅ¼na tam: kopiowaÄ‡ pliki, pisaÄ‡ wÅ‚asne skrypty, instalowaÄ‡ dodatkowe rzeczy, przygotowaÄ‡ konfiguracjÄ™ systemu. W tym przypadku :
dodajemy plik repozytorium Docker CE - tworzymy plik .repo, ktÃ³ry mÃ³wi systemowi, gdzie znaleÅºÄ‡ i jak pobieraÄ‡ pakiety Dockera z internetu. DziÄ™ki temu dnf wie, skÄ…d Å›ciÄ…gaÄ‡ Dockera. Po tym upewnimy siÄ™, Å¼e Docker jest aktualny:

```
dnf -y update docker-ce docker-ce-cli containerd.io 
```
Przez systemctl zapewniamy wÅ‚Ä…czenia Dockera przy kaÅ¼dym starcie systemu:
```
systemctl enable docker
```
Po uruchomieniu systemu Docker uruchomi siÄ™ automatycznie. Tworzymy skrypt ktÃ³ry uruchomi siÄ™ tylko przy pierwszym starcie systemu, skrypt poczeka aÅ¼ Docker siÄ™ uruchomi => pobierze kontener z DockerHub => uruchamia go z odpowiednimi parametrami => dezaktywuje siÄ™ aby wiÄ™cej siÄ™ nie uruchamiaÄ‡.
```
cat >/usr/local/bin/setup-container.sh <<'EOSH'
...
EOSH
chmod +x /usr/local/bin/setup-container.sh
```

Skrypt tworzy jednostkÄ™ systemowÄ… (service) 
```
cat >/etc/systemd/system/firstboot-container.service <<'EOUNIT'
...
EOUNIT
```
w systemd (czyli systemie, ktÃ³ry zarzÄ…dza uruchamianiem wszystkiego po starcie systemu). Ta jednostka uruchomi skrypt raz, przy pierwszym starcie systemu. Skrypt wÅ‚Ä…cza tÄ™ jednostkÄ™:
```
systemctl enable firstboot-container.service
```
tak aby wystartowaÅ‚a automatycznie przy pierwszym starcie systemu.

Czyli idÄ…c krok po kroku:

1. System startuje
2. Docker siÄ™ uruchamia
3. firstboot-container.service widzi, Å¼e Docker juÅ¼ dziaÅ‚a
4. firstboot-container.service uruchamia skrypt ktÃ³ry:
  1. Pobiera obraz kontenera z DockerHub
  2. uruchamia go w tle
  3. dezaktywuje jednostkÄ™ systemd, Å¼eby wiÄ™cej nie uruchamiaÅ‚a siÄ™ przy starcie.

Sekcja %post to kluczowy element dla pliku kickstartowego przy automatyzacji instalacji aplikacji z pipeline'u.

â”¬â”€â”¬ ãƒ( ã‚œ-ã‚œãƒ)

# WdraÅ¼anie na zarzÄ…dzalne kontenery: Kubernetes (1)
(^o^)/ 
## Instalacje minikube

![alt text](screeny/curl_minikube.png)
## Uruchomienie klastra
Uruchamiamy klaster Kubernetes za pomocÄ… polecenia minikube start, co powoduje uruchomienie Å›rodowiska gotowego do wdraÅ¼ania kontenerÃ³w.
![alt text](<screeny/minikube start.png>)
## Zaopatrzenie siÄ™ w polecenie kubectl oraz dodanie aliasu
Instalujemy narzÄ™dzie kubectl sÅ‚uÅ¼Ä…ce do zarzÄ…dzania klastrem Kubernetes oraz definiujemy alias, aby wygodniej uÅ¼ywaÄ‡ tego polecenia.
![alt text](screeny/alias.png)
![alt text](screeny/daszboard1.png)
![alt text](screeny/daszboard2.png)

## Uruchamianie oprogramowania
Polecenie uruchamia kontener z obrazem nginx w Kubernetes jako pojedynczy pod o nazwie nginx-pod, wystawiajÄ…c go na porcie 80 i przypisujÄ…c mu etykietÄ™ app=nginx-deployment.
```
minikube kubectl -- run nginx-pod --image=nginx --port=80 --labels app=nginx-deployment
```

Utworzony zostaje pod oraz nastÄ™pnie jest skalowany do 12 instancji, przez zakÅ‚adke deployments w dashboard minikube.
![alt text](screeny/pods1.png)
![alt text](screeny/scale.png)
![alt text](screeny/podsyskalowane.png)

![alt text](screeny/workloads.png)

## Wyprowadzenie portu

![alt text](screeny/portFoward.png)

![alt text](screeny/ngixDZIAÅÄ„AAAAAA.png)

## Utworzenie pliku yaml oraz kubectl apply

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:latest
          ports:
            - containerPort: 80

```

![alt text](screeny/yaml1.png)

## Rozszerzenie pliku yaml o 4 repliki 

Zmieniamy czÄ™Å›Ä‡ pliku replicas nastÄ™pujÄ…co :

```
spec:
  replicas: 4
```

Oraz wykonujemy nastÄ™pujÄ…cÄ… konfiguracje 

![alt text](screeny/yaml2konfiguracja.png)

![alt text](screeny/yaml2testdziaÅ‚ania.png)

Jak widaÄ‡ konfiguracja zostaÅ‚a przeprowadzona poprawnie.

# WdraÅ¼anie na zarzÄ…dzalne kontenery: Kubernetes (2)
(Â¬â€¿Â¬)(âŒ’Ï‰âŒ’)(Â¬â€¿Â¬)
Stworzenie obrazÃ³w na podstawie httpd oraz jednego obrazu ktÃ³ry koÅ„czy siÄ™ bÅ‚Ä™dem oraz wypchniÄ™cie na DockerHub.

```
<html>
    <body>
        <h1>Sprawdzenie dziaÅ‚ania</h1>
    </body>
</html>
```

```
FROM httpd:2.4
COPY ./index.html /usr/local/apache2/htdocs/index.html

```

```
FROM httpd:2.4
COPY ./broken.sh /start.sh
RUN chmod +x /start.sh
CMD ["/start.sh"]
```

![alt text](screeny/dbuild1.png)

![alt text](screeny/dbuild2.png)

![alt text](screeny/dockerPush.png)

Po wypchniÄ™ciu utworzono pliki .yaml dla obu wersji httpd:v1 oraz httpd:v2

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: httpd-deployment          # nowa nazwa
  labels:
    app: httpd
spec:
  replicas: 4                     # zostawiasz lub zmieniasz wg potrzeb
  selector:
    matchLabels:
      app: httpd                  # musi siÄ™ zgadzaÄ‡ z template.metadata.labels
  template:
    metadata:
      labels:
        app: httpd
    spec:
      containers:
        - name: httpd             # dowolna przyjazna nazwa kontenera
          image: bambusscooby/httpd-custom:v1   # â† tu wskazujesz swÃ³j obraz
          ports:
            - containerPort: 80   # Apache domyÅ›lnie sÅ‚ucha na 80



```

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: httpd-deployment          # nowa nazwa
  labels:
    app: httpd
spec:
  replicas: 4                     # zostawiasz lub zmieniasz wg potrzeb
  selector:
    matchLabels:
      app: httpd                  # musi siÄ™ zgadzaÄ‡ z template.metadata.labels
  template:
    metadata:
      labels:
        app: httpd
    spec:
      containers:
        - name: httpd             # dowolna przyjazna nazwa kontenera
          image: bambusscooby/httpd-custom:v2   # â† tu wskazujesz swÃ³j obraz
          ports:
            - containerPort: 80   # Apache domyÅ›lnie sÅ‚ucha na 80

```

## ğŸŒµ Aktualizacja pliku YAML z wdroÅ¼eniem i przeprowadzenie je ponownie po zastosowaniu nastÄ™pujÄ…cych zmian

#### Stworzenie pierwszej wersji
Tworzona jest poczÄ…tkowa wersja deploymentu, ktÃ³ra uruchamia 4 instancje aplikacji HTTPD.

![alt text](screeny/httpd4.png)

#### ZwiÄ™kszenie do 8
NastÄ™puje skalowanie liczby instancji aplikacji z 4 do 8 replik, aby zwiÄ™kszyÄ‡ dostÄ™pnoÅ›Ä‡ i obsÅ‚uÅ¼yÄ‡ wiÄ™kszy ruch.
![alt text](screeny/httpd8.png)
#### Zmniejszenie do 1
Liczba instancji jest redukowana z 8 do 1 repliki, minimalizujÄ…c zuÅ¼ycie zasobÃ³w klastra Kubernetes.
![alt text](screeny/httpd1.png)
#### Zmniejszenie do 0
Wszystkie instancje aplikacji zostajÄ… zatrzymane poprzez skalowanie deploymentu do 0, co powoduje caÅ‚kowity brak dostÄ™pnoÅ›ci aplikacji.
![alt text](screeny/httpd0.png)
#### ZwiÄ™kszenie do 4
Ponownie uruchamiane sÄ… 4 instancje aplikacji, przywracajÄ…c deployment do pierwotnej konfiguracji.
![alt text](screeny/httpd4.png)
![alt text](screeny/httpdv1-komendy.png)

## Uruchomienie wadliwego obrazu
WdroÅ¼ony zostaje wadliwy obraz aplikacji, co prowadzi do niepowodzenia uruchomienia podÃ³w i przejÅ›cia deploymentu w stan bÅ‚Ä™du.
![alt text](screeny/wadliwyHttpd.png)

![alt text](screeny/wadliwyKubernetes.png)

## Sprawdzenie history rollout'Ã³w oraz przywrÃ³cenie do dziaÅ‚ajÄ…cej wersji 
Za pomocÄ… historii rollout sprawdzane sÄ… poprzednie wersje wdroÅ¼enia, a nastÄ™pnie deployment jest cofany (rollback) do ostatniej poprawnie dziaÅ‚ajÄ…cej wersji aplikacji.
![alt text](screeny/rolloutHistory.png)

![alt text](screeny/rolloutUndo.png)

# Strategie wdroÅ¼enia 

### Skrypt weryfikujÄ…cy wdroÅ¼enia 

```
#!/bin/bash

DEPLOYMENT_NAME="httpd-custom"
NAMESPACE="default"
TIMEOUT=60

echo "Sprawdzam rollout '$DEPLOYMENT_NAME' (maks. $TIMEOUT sek)..."

start_time=$(date +%s)

while true; do
    status=$(kubectl rollout status deployment/$DEPLOYMENT_NAME --namespace $NAMESPACE --timeout=5s 2>&1)

    if echo "$status" | grep -q "successfully rolled out"; then
        echo "WdroÅ¼enie zakoÅ„czone sukcesem!"
        exit 0
    fi

    if echo "$status" | grep -q "error"; then
        echo "BÅ‚Ä…d rolloutu:"
        echo "$status"
        exit 1
    fi

    now=$(date +%s)
    elapsed=$((now - start_time))

    if [ $elapsed -ge $TIMEOUT ]; then
        echo "Timeout: rollout nie zakoÅ„czyÅ‚ siÄ™ w ciÄ…gu $TIMEOUT sekund"
        kubectl get pods -l app=$DEPLOYMENT_NAME
        exit 2
    fi

    sleep 3
done
```

## Wersje WdroÅ¼eÅ„

### WdroÅ¼enie typu recreate

1. Strategia Recreate (odtworzenie):
Na czym polega?

Stara wersja aplikacji jest najpierw caÅ‚kowicie zatrzymywana.

Dopiero gdy poprzednia wersja jest caÅ‚kowicie usuniÄ™ta, uruchamiana jest nowa wersja.
Powoduje krÃ³tkÄ… przerwÄ™ w dostÄ™pnoÅ›ci aplikacji.

Idealna dla aplikacji, ktÃ³re nie mogÄ… dziaÅ‚aÄ‡ jednoczeÅ›nie w dwÃ³ch rÃ³Å¼nych wersjach (np. baza danych z niekompatybilnymi wersjami).

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-recreate
spec:
  replicas: 3
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: demo
      version: v1
  template:
    metadata:
      labels:
        app: demo
        version: v1
    spec:
      containers:
      - name: httpd
        image: bambusscooby/httpd-custom:v1
        ports:
        - containerPort: 80
```

2. Strategia Rolling Update:
Stopniowo zastÄ™puje stare instancje aplikacji nowymi.

Pozwala uniknÄ…Ä‡ caÅ‚kowitej niedostÄ™pnoÅ›ci aplikacji. Kluczowe parametry obejmujÄ…:

available: liczba podÃ³w, ktÃ³re mogÄ… byÄ‡ jednoczeÅ›nie niedostÄ™pne podczas aktualizacji. JeÅ›li ustawimy > 1, zezwalamy na jednoczesne zatrzymanie kilku podÃ³w.

maxSurge: dodatkowe pody uruchamiane ponad liczbÄ™ docelowÄ… podczas aktualizacji. WartoÅ›Ä‡ > 20% pozwala przyspieszyÄ‡ wdroÅ¼enie, uruchamiajÄ…c wiÄ™cej podÃ³w niÅ¼ standardowa liczba.

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-rolling
spec:
  replicas: 4
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 2
      maxSurge: 50%
  selector:
    matchLabels:
      app: demo
      version: v1
  template:
    metadata:
      labels:
```

![alt text](screeny/apply&&rollout.png)

![](screeny/strategieWdroÅ¼enie_kubernetes.png)

![alt text](screeny/kubectlpods.png)

| Cecha                  | Recreate                               | Rolling Update               | Canary Deployment                                    |
| ---------------------- | -------------------------------------- | ---------------------------- | ---------------------------------------------------- |
| PrzestÃ³j aplikacji     | Tak                                    | Nie                          | Nie                                                  |
| Kontrola ryzyka        | Niska                                  | Åšrednia                      | Wysoka (peÅ‚na kontrola)                              |
| ZÅ‚oÅ¼onoÅ›Ä‡ konfiguracji | Niska                                  | Åšrednia                      | Wysoka (wymaga dodatkowych narzÄ™dzi)                 |
| Zastosowanie           | Bazy danych, aplikacje niekompatybilne | WiÄ™kszoÅ›Ä‡ aplikacji webowych | Aplikacje krytyczne, wymagajÄ…ce szczegÃ³lnej kontroli |
